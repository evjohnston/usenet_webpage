"0","import pandas as pd"
"0","import numpy as np"
"0","import re"
"0","from nltk.tokenize import word_tokenize"
"0","from collections import Counter"
"0","import matplotlib.pyplot as plt"
"0","import os"
"0",""
"0",""
"0","def preprocess_text_for_matching(text):"
"0","    """""""
"0","    Preprocess text for keyword matching: tokenize, lowercase, and clean."
"0","    """""""
"0","    if pd.isna(text) or not isinstance(text, str):"
"0","        return []"
"0","    # Tokenize, lowercase, and remove non-alphanumeric characters"
"0","    text = re.sub(r""\W+"", "" "", text)"
"0","    tokens = word_tokenize(text.lower())"
"0","    return tokens"
"0","    "
"0","    "
"0","def extract_lda_keywords_with_betas(lda_model, vectorizer, n_top_words=20):"
"0","    """""""
"0","    Extract top keywords and their beta values from LDA model topics."
"0","    """""""
"0","    # Define custom stop words"
"0","    custom_stop_words = {""iii""}"
"0","    "
"0","    keyword_betas = {}"
"0","    for topic_idx, topic in enumerate(lda_model.components_):"
"0","        top_indices = topic.argsort()[:-n_top_words - 1:-1]"
"0","        top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]"
"0","        top_betas = [topic[i] for i in top_indices]"
"0","        for word, beta in zip(top_words, top_betas):"
"0","            if word not in custom_stop_words:"
"0","                if word in keyword_betas:"
"0","                    keyword_betas[word] += beta"
"0","                else:"
"0","                    keyword_betas[word] = beta"
"0","    return keyword_betas  # Return dictionary of keywords and their combined betas"
"0","    "
"0","    "
"0","def calculate_keyword_prevalence(df, keywords, text_column='Full.Text'):"
"0","    """""""
"0","    Calculate monthly prevalence of shared keywords."
"0","    """""""
"0","    # Ensure 'Date' column is datetime"
"0","    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')"
"0","    df = df.dropna(subset=['Date'])  # Drop rows with invalid dates"
"0","    "
"0","    monthly_counts = []"
"0","    for _, row in df.iterrows():"
"0","        tokens = preprocess_text_for_matching(row[text_column])"
"0","        keyword_counts = Counter(word for word in tokens if word in keywords)"
"0","        for word, count in keyword_counts.items():"
"0","            monthly_counts.append({"
"0","                'Date': row['Date'],"
"0","                'word': word,"
"0","                'count': count"
"0","            })"
"0","            "
"0","    counts_df = pd.DataFrame(monthly_counts)"
"0","    if counts_df.empty:"
"0","        raise ValueError(""No shared keywords found in the dataset after processing."")"
"0","        "
"0","    # Group by month and calculate prevalence"
"0","    monthly_prevalence = counts_df.groupby(["
"0","        pd.Grouper(key='Date', freq='M'),"
"0","        'word'"
"0","    ])['count'].sum().reset_index()"
"0","    "
"0","    total_counts = monthly_prevalence.groupby('Date')['count'].sum().reset_index()"
"0","    monthly_prevalence = monthly_prevalence.merge(total_counts, on='Date', suffixes=('', '_total'))"
"0","    monthly_prevalence['prevalence'] = monthly_prevalence['count'] / monthly_prevalence['count_total']"
"0","    "
"0","    return monthly_prevalence"
"0","    "
"0","    "
"0","def plot_keyword_adoption(influential_prevalence, overall_prevalence, save_path):"
"0","    """""""
"0","    Plot keyword adoption patterns for influential authors and general discussions."
"0","    """""""
"0","    influential_prevalence['Group'] = 'Influential Authors'"
"0","    overall_prevalence['Group'] = 'Overall Discussion'"
"0","    combined_data = pd.concat([influential_prevalence, overall_prevalence])"
"0","    "
"0","    keywords = combined_data['word'].unique()"
"0","    n_keywords = len(keywords)"
"0","    "
"0","    n_cols = 4"
"0","    n_rows = int(np.ceil(n_keywords / n_cols))"
"0","    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3))"
"0","    axes = axes.flatten()"
"0","    "
"0","    for idx, keyword in enumerate(sorted(keywords)):"
"0","        keyword_data = combined_data[combined_data['word'] == keyword]"
"0","        for group in ['Influential Authors', 'Overall Discussion']:"
"0","            group_data = keyword_data[keyword_data['Group'] == group]"
"0","            color = '#ff7f0e' if group == 'Influential Authors' else '#1f77b4'"
"0","            axes[idx].plot(group_data['Date'], group_data['prevalence'], label=group, color=color)"
"0","        axes[idx].set_title(keyword)"
"0","        axes[idx].tick_params(axis='x', rotation=45)"
"0","        axes[idx].grid(True, alpha=0.3)"
"0","        "
"0","    for idx in range(len(keywords), len(axes)):"
"0","        fig.delaxes(axes[idx])"
"0","        "
"0","    handles, labels = axes[0].get_legend_handles_labels()"
"0","    fig.legend(handles, labels, loc='upper center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 0.96))"
"0","    "
"0","    plt.tight_layout()"
"0","    plt.subplots_adjust(top=0.9)"
"0","    plt.savefig(save_path, dpi=300, bbox_inches='tight')"
"0","    plt.close()"
"0","    "
"0","    "
"0","# Execution for Top 20 Shared Keywords"
"0","try:"
"0","    print(""\nAnalyzing adoption patterns for top 20 shared LDA keywords..."")"
"0","    "
"0","    # Extract LDA keywords with betas from both models"
"0","    lda_keywords_betas_all_comments = extract_lda_keywords_with_betas(lda_dataset2, dataset2_vectorizer)"
"0","    lda_keywords_betas_influential_authors = extract_lda_keywords_with_betas(lda_dataset3, dataset3_vectorizer)"
"0","    "
"0","    # Find shared keywords between general and influential topics"
"0","    shared_keywords = set(lda_keywords_betas_all_comments.keys()).intersection("
"0","        set(lda_keywords_betas_influential_authors.keys())"
"0","    )"
"0","    "
"0","    # Combine beta values for shared keywords"
"0","    combined_keyword_betas = {"
"0","        word: lda_keywords_betas_all_comments[word] + lda_keywords_betas_influential_authors[word]"
"0","        for word in shared_keywords"
"0","    }"
"0","    "
"0","    # Select top 20 shared keywords based on combined beta values"
"0","    top_20_keywords = sorted(combined_keyword_betas.items(), key=lambda x: x[1], reverse=True)[:20]"
"0","    top_20_keywords = {word for word, beta in top_20_keywords}"
"0","    print(f""\nTop 20 Shared Keywords: {top_20_keywords}"")"
"0","    "
"0","    # Calculate keyword prevalence for top 20 shared keywords"
"0","    influential_prevalence = calculate_keyword_prevalence("
"0","        dataset3_comments_onlyinfluential, top_20_keywords"
"0","    )"
"0","    overall_prevalence = calculate_keyword_prevalence("
"0","        dataset3_comments_all, top_20_keywords"
"0","    )"
"0","    "
"0","    # Plot and save results"
"0","    output_path = os.path.join(images_dir, ""lda_keyword_adoption_over_time.png"")"
"0","    plot_keyword_adoption(influential_prevalence, overall_prevalence, output_path)"
"0","    "
"0","    print(""\nTop 20 shared keyword adoption analysis completed successfully!"")"
"0","    print(f""Visualization saved to: {output_path}"")"
"0","    "
"0","except Exception as e:"
"0","    print(f""An error occurred: {str(e)}"")"
"0",""
"1","
Analyzing adoption patterns for top 20 shared LDA keywords...

Top 20 Shared Keywords: {'disease', 'year', 'virus', 'positive', 'case', 'cell', 'blood', 'risk', 'htlv', 'immune', 'net', 'people', 'insurance', 'medical', 'article', 'cost', 'gay', 'test', 'aid', 'patient'}
"
"1","
Top 20 shared keyword adoption analysis completed successfully!
Visualization saved to: /Users/emerson/Github/usenet_webpage/Images and Tables/Images/lda_keyword_adoption_over_time.png
"

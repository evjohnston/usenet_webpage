"0","from sklearn.feature_extraction.text import CountVectorizer"
"0","from sklearn.decomposition import LatentDirichletAllocation"
"0","from nltk.corpus import stopwords"
"0","from nltk.tokenize import word_tokenize"
"0","from nltk.stem import WordNetLemmatizer"
"0","import numpy as np"
"0","import pandas as pd"
"0","import re"
"0",""
"0","# Preprocessing function"
"0","def preprocess_text(text):"
"0","    stop_words = set(stopwords.words(""english""))"
"0","    lemmatizer = WordNetLemmatizer()"
"0","    text = re.sub(r""\W+"", "" "", text)  # Remove non-alphanumeric characters"
"0","    tokens = word_tokenize(text.lower())"
"0","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]"
"0","    return "" "".join(tokens)"
"0","    "
"0","# Preprocess datasets"
"0","dataset2_text = dataset2_comments[""Full.Text""].dropna().astype(str).apply(preprocess_text)"
"0","dataset3_text = dataset3_comments_all[""Full.Text""].dropna().astype(str).apply(preprocess_text)"
"0",""
"0","# Vectorize text"
"0","def vectorize_text(data):"
"0","    vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words=""english"")"
"0","    doc_term_matrix = vectorizer.fit_transform(data)"
"0","    return doc_term_matrix, vectorizer"
"0","    "
"0","# Determine optimal number of topics"
"0","def find_optimal_topics(doc_term_matrix, topic_range=(1, 10)):"
"0","    coherence_values = []"
"0","    for n_topics in range(topic_range[0], topic_range[1] + 1):"
"0","        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)"
"0","        lda.fit(doc_term_matrix)"
"0","        coherence_values.append(lda.perplexity(doc_term_matrix))"
"0","    optimal_n_topics = coherence_values.index(min(coherence_values)) + topic_range[0]"
"0","    return optimal_n_topics, coherence_values"
"0","    "
"0","# Extract topics and top words"
"0","def extract_topics(lda_model, vectorizer, n_top_words=20):"
"0","    topics = []"
"0","    for topic_idx, topic in enumerate(lda_model.components_):"
"0","        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]"
"0","        topics.append((f""Topic {topic_idx + 1}"", top_words))"
"0","    return topics"
"0","    "
"0","# Process Dataset 2"
"0","dataset2_matrix, dataset2_vectorizer = vectorize_text(dataset2_text)"
"0","optimal_topics_2, coherence_2 = find_optimal_topics(dataset2_matrix)"
"0","lda_dataset2 = LatentDirichletAllocation(n_components=optimal_topics_2, random_state=42)"
"0","lda_dataset2.fit(dataset2_matrix)"
"0","topics_2 = extract_topics(lda_dataset2, dataset2_vectorizer)"
"1","LatentDirichletAllocation(random_state=42)
"
"0",""
"0","# Process Dataset 3"
"0","dataset3_matrix, dataset3_vectorizer = vectorize_text(dataset3_text)"
"0","optimal_topics_3, coherence_3 = find_optimal_topics(dataset3_matrix)"
"0","lda_dataset3 = LatentDirichletAllocation(n_components=optimal_topics_3, random_state=42)"
"0","lda_dataset3.fit(dataset3_matrix)"
"1","LatentDirichletAllocation(n_components=7, random_state=42)
"
"0","topics_3 = extract_topics(lda_dataset3, dataset3_vectorizer)"
"0",""
"0","# Output results"
"0","print(f""Optimal number of topics for Dataset 2: {optimal_topics_2}"")"
"1","Optimal number of topics for Dataset 2: 10
"
"0","print(f""\nOptimal number of topics for Dataset 3: {optimal_topics_3}"")"
"1","
Optimal number of topics for Dataset 3: 7
"
"0","def save_topics_to_html(lda_model, vectorizer, output_path, n_top_words=20):"
"0","    topics_data = []"
"0","    for topic_idx, topic in enumerate(lda_model.components_):"
"0","        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]"
"0","        top_betas = [topic[i] for i in topic.argsort()[:-n_top_words - 1:-1]]"
"0","        topic_dict = {"
"0","            f""Topic_{topic_idx + 1}_terms"": top_words,"
"0","            f""Topic_{topic_idx + 1}_betas"": top_betas"
"0","        }"
"0","        topics_data.append(pd.DataFrame(topic_dict))"
"0","    "
"0","    # Combine all topics into a single DataFrame"
"0","    topics_df = pd.concat(topics_data, axis=1)"
"0","    topics_df.to_html(output_path, index=False)"
"0","    print(f""Topics saved to: {output_path}"")"
"0","    "
"0","# Save topics for Dataset 2 (All Comments)"
"0","save_topics_to_html("
"0","    lda_model=lda_dataset2,"
"0","    vectorizer=dataset2_vectorizer,"
"0","    output_path=""/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_all_comments.html"","
"0","    n_top_words=20"
"0",")"
"1","Topics saved to: /Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_all_comments.html
"
"0","# Save topics for Dataset 3 (Influential Authors)"
"0","save_topics_to_html("
"0","    lda_model=lda_dataset3,"
"0","    vectorizer=dataset3_vectorizer,"
"0","    output_path=""/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_influential_authors.html"","
"0","    n_top_words=20"
"0",")"
"1","Topics saved to: /Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_influential_authors.html
"

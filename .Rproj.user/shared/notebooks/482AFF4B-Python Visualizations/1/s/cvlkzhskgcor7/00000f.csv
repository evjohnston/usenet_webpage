"0","import os"
"0",""
"0","# Directories"
"0","output_directory = ""/Users/emerson/Github/usenet_webpage"""
"0","threads_directory = os.path.join(output_directory, ""CSV Files/Threads"")"
"0","comments_directory = os.path.join(output_directory, ""CSV Files/Comments"")"
"0","images_dir = os.path.join(output_directory, ""Images and Tables/Images"")"
"0","tables_dir = os.path.join(output_directory, ""Images and Tables/Tables"")"
"0",""
"0","# Ensure the images directory exists"
"0","os.makedirs(images_dir, exist_ok=True)"
"0",""
"0","# Full Influence Propagation Network Code"
"0","def create_influence_network(dataset1_comments, influential_authors, output_path):"
"0","    """"""Create and visualize influence network based on thread interactions."""""""
"0","    "
"0","    # Convert 'Date' column to datetime if not already done"
"0","    dataset1_comments['Date'] = pd.to_datetime(dataset1_comments['Date'], errors='coerce')"
"0","    dataset1_comments = dataset1_comments.dropna(subset=['Date'])  # Drop rows with invalid dates"
"0","    "
"0","    # Filter threads involving influential authors"
"0","    influential_authors_set = set(influential_authors['Author'])"
"0","    dataset1_comments = dataset1_comments[dataset1_comments['Author'].isin(influential_authors_set)]"
"0","    "
"0","    # Group by thread ID"
"0","    interactions = []"
"0","    for thread_id, group in dataset1_comments.groupby('TH_ID'):"
"0","        group = group.sort_values('Date')  # Sort comments in the thread by date"
"0","        authors = group['Author'].tolist()"
"0","        dates = group['Date'].tolist()"
"0","        "
"0","        # Create interactions between consecutive authors in the thread"
"0","        for i in range(1, len(authors)):"
"0","            source = authors[i - 1]"
"0","            target = authors[i]"
"0","            response_time = (dates[i] - dates[i - 1]).total_seconds()  # Calculate response time in seconds"
"0","            interactions.append((source, target, response_time))"
"0","    "
"0","    # Build the directed graph"
"0","    G = nx.DiGraph()"
"0","    "
"0","    # Add nodes (authors)"
"0","    for author in influential_authors_set:"
"0","        comment_count = len(dataset1_comments[dataset1_comments['Author'] == author])"
"0","        G.add_node(author, size=np.log2(comment_count + 1) * 100)"
"0","        "
"0","    # Add edges (interactions between authors)"
"0","    edge_weights = {}"
"0","    response_times = {}"
"0","    for source, target, response_time in interactions:"
"0","        if (source, target) in edge_weights:"
"0","            edge_weights[(source, target)] += 1"
"0","            response_times[(source, target)].append(response_time)"
"0","        else:"
"0","            edge_weights[(source, target)] = 1"
"0","            response_times[(source, target)] = [response_time]"
"0","            "
"0","    for (source, target), weight in edge_weights.items():"
"0","        avg_response_time = np.mean(response_times[(source, target)]) / 3600  # Average response time in hours"
"0","        G.add_edge(source, target, weight=weight, response_time=avg_response_time)"
"0","        "
"0","    # Create visualization"
"0","    fig, ax = plt.subplots(figsize=(20, 16))"
"0","    pos = nx.spring_layout(G, seed=42)  # Use spring layout for positioning nodes"
"0","    "
"0","    # Get edge properties for visualization"
"0","    edges = G.edges()"
"0","    weights = [G[u][v]['weight'] for u, v in edges]"
"0","    response_times = [G[u][v]['response_time'] for u, v in edges]"
"0","    "
"0","    # Normalize edge colors and widths"
"0","    max_weight = max(weights) if weights else 1"
"0","    max_response_time = max(response_times) if response_times else 1"
"0","    edge_colors = [plt.cm.viridis(rt / max_response_time) for rt in response_times]"
"0","    edge_widths = [3 * w / max_weight for w in weights]"
"0","    "
"0","    # Draw nodes"
"0","    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]"
"0","    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', edgecolors='black', alpha=0.9)"
"0","    "
"0","    # Draw edges"
"0","    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, alpha=0.7, edge_cmap=plt.cm.viridis)"
"0","    "
"0","    # Draw labels"
"0","    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')"
"0","    "
"0","    # Add legend for node sizes"
"0","    size_labels = [min(node_sizes), np.median(node_sizes), max(node_sizes)]"
"0","    legend_labels = [f'{int(2 ** (size / 100) - 1)} comments' for size in size_labels]"
"0","    for size, label in zip(size_labels, legend_labels):"
"0","        plt.scatter([], [], s=size, c='lightblue', label=label, edgecolors='black', alpha=0.9)"
"0","    plt.legend(title='Author Activity', loc='upper right')"
"0","    "
"0","    # Add colorbar for edge response times"
"0","    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(0, max_response_time))"
"0","    sm.set_array([])"
"0","    plt.colorbar(sm, ax=ax, label='Average Response Time (hours)')"
"0","    "
"0","    # Set title and remove axes"
"0","    plt.title('Influence Network of Key Authors\nEdge thickness = interaction frequency, Edge color = response time', fontsize=16)"
"0","    plt.axis('off')"
"0","    "
"0","    # Save the visualization"
"0","    plt.savefig(output_path, dpi=300, bbox_inches='tight')"
"0","    plt.close()"
"0","    "
"0","    # Return graph statistics"
"0","    stats = {"
"0","        'num_nodes': G.number_of_nodes(),"
"0","        'num_edges': G.number_of_edges(),"
"0","        'avg_degree': np.mean([d for n, d in G.degree()]),"
"0","        'most_connected': sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:5]"
"0","    }"
"0","    return stats"
"0","    "
"0","    "
"0","# Example usage"
"0","output_path = os.path.join(images_dir, ""influence_network.png"")"
"0","stats = create_influence_network(dataset1_comments, influential_authors, output_path=output_path)"
"0",""
"0","# Print summary statistics"
"0","print(""\nNetwork Summary:"")"
"0","print(f""Number of authors: {stats['num_nodes']}"")"
"1","
Network Summary:
"
"1","Number of authors: 20
"
"0","print(f""Number of interactions: {stats['num_edges']}"")"
"1","Number of interactions: 114
"
"0","print(f""Average degree: {stats['avg_degree']:.2f}"")"
"1","Average degree: 11.40
"
"0","print(""\nMost Connected Authors (by total interactions):"")"
"1","
Most Connected Authors (by total interactions):
"
"0","for author, degree in stats['most_connected']:"
"0","    print(f""{author}: {degree} interactions"")"
"0",""
"1","Steve Dyer: 320 interactions
Ron Rizzo: 236 interactions
Craig Werner: 204 interactions
Rob Bernardo: 91 interactions
Bill Stoll: 44 interactions
"

---
title: "Author Name Standardization"
author: "Emerson Johnston"
lastmodifeddate: "2024-11-17"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{python}
import os
import pandas as pd
import re

# Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory = os.path.join(output_directory, "CSV Files/Threads")
comments_directory = os.path.join(output_directory, "CSV Files/Comments")

# Load cleaned datasets
all_threads = pd.read_csv(os.path.join(threads_directory, "combined_threads.csv"))
all_comments = pd.read_csv(os.path.join(comments_directory, "combined_comments.csv"))

# Step 1: Filter Authors that are not proper names
# Criteria: Contains non-alphabetic characters excluding hyphen "-" and dot "."
non_name_pattern = r"[^A-Za-z\s\-.]"  # Matches any non-alphabetic character except spaces, hyphens, and dots
filtered_authors = all_comments[all_comments['Author'].str.contains(non_name_pattern, na=False, regex=True)]

# Step 2: Extract potential full names from the Full.Text
def extract_real_name(full_text):
    if not isinstance(full_text, str):
        return None
    name_pattern = r"[-–—]? *(by|from)? *([A-Z][a-z]+(?: [A-Z][a-z]+)+)$"
    match = re.search(name_pattern, full_text)
    if match:
        return match.group(2)
    return None

filtered_authors.loc[:, 'Extracted_Real_Name'] = filtered_authors['Full.Text'].apply(extract_real_name)

# Filter rows where a real name was successfully extracted
filtered_with_real_names = filtered_authors[filtered_authors['Extracted_Real_Name'].notnull()]

# Step 3: Create a mapping of "Author" to "Extracted_Real_Name"
author_name_mapping = filtered_with_real_names.set_index('Author')['Extracted_Real_Name'].to_dict()

# Add manual mappings for stragglers
manual_mappings = {
    "wer...@aecom.uucp": "Craig Werner",
    "mi...@tekecs.uucp": "Mike Sellers",
    "#D.ANDERSON": "Dave Anderson",
    "SEVENER": "Tim Sevener",
    "The Polymath": "Jerry Hollombe",
    "fau...@ucbcad.uucp": "Wayne A. Christopher",
    "bi...@persci.uucp": "Bill Swan",
    "pam pincha": "Pam Pincha",
    "stephanie da silva": "Stephanie Da Silva",
    "JB": "Beth Christy"
    # Add additional mappings here
}
author_name_mapping.update(manual_mappings)

# Step 4: Create the new DataFrame all_comments_AS
all_comments_AS = all_comments.copy()

# Rename the current 'Author' column to 'Original_Username'
all_comments_AS.rename(columns={'Author': 'Original_Username'}, inplace=True)

# Replace 'Author' with the standardized author names
all_comments_AS['Author'] = all_comments_AS['Original_Username'].map(author_name_mapping).fillna(all_comments_AS['Original_Username'])

# Define the desired column order
column_order = [
    "Thread.ID",
    "Comment.ID",
    "Unique.Comment.ID",
    "Author",
    "Date.and.Time",
    "Full.Text",
    "URL.String",
    "newsgroup",
    "Original_Username"
]

# Reorder the DataFrame columns
all_comments_AS = all_comments_AS[column_order]

# Display the first few rows to verify the column order
print(all_comments_AS.head())

# Verify that the mapping was applied
print(f"Number of authors standardized: {len(all_comments_AS[all_comments_AS['Author'] != all_comments_AS['Original_Username']])}")

# Step 5: Identify entries with authors still containing weird characters

# Define a regex pattern for valid names:
# - Firstname
# - Firstname Lastname
# - Firstname Middlename Lastname
# - Firstname MiddleInitial Lastname
valid_name_pattern = r"^[A-Z][a-z]+(?: [A-Z][a-z]+| [A-Z]\.)?(?: [A-Z][a-z]+)?$"

# Identify entries where Author contains weird characters but exclude valid names
still_weird = all_comments_AS[
    all_comments_AS['Author'].str.contains(non_name_pattern, na=False, regex=True) &
    ~all_comments_AS['Author'].str.match(valid_name_pattern, na=False)
]

# Display the number of entries and a sample of the still_weird dataset
print(f"Number of entries with still weird authors: {len(still_weird)}")
print(still_weird.head())

# Save still_weird to CSV (optional)
still_weird_output_path = os.path.join(comments_directory, "still_weird.csv")
still_weird.to_csv(still_weird_output_path, index=False)
print(f"Still weird authors saved to: {still_weird_output_path}")
```

```{python}
import re

# Step 1: Define a function to extract valid names from the Author column
def extract_clean_name(author):
    """
    Extracts a name from the author string if it matches a valid name pattern.
    Handles cases with weird characters, such as 'Firstname Lastname' or 'Firstname M. Lastname'.
    """
    if not isinstance(author, str):
        return None
    # Enhanced regex to handle names with surrounding noise
    name_pattern = r"([A-Z][a-z]+(?: [A-Z]\.?)?(?: [A-Z][a-z]+)?)"
    match = re.search(name_pattern, author)  # Use search to find names within noisy data
    if match:
        return match.group(1)  # Return the matched name
    return None

# Step 2: Apply the function to extract valid names for the still_weird dataset
still_weird['Extracted_Name'] = still_weird['Author'].apply(extract_clean_name)

# Step 3: Create a mapping of original author to extracted name for resolved entries
resolved_names = still_weird[still_weird['Extracted_Name'].notnull()][['Author', 'Extracted_Name']].drop_duplicates()

# Step 4: Merge the resolved names back into all_comments_AS
# Map resolved names back to the Author column in all_comments_AS
all_comments_AS['Author'] = all_comments_AS['Original_Username'].map(
    resolved_names.set_index('Author')['Extracted_Name']
).fillna(all_comments_AS['Author'])

# Step 5: Update still_weird to include only entries with unresolved authors
still_weird = still_weird[still_weird['Extracted_Name'].isnull()].drop(columns=['Extracted_Name'], inplace=False)

# Display the updated still_weird and all_comments_AS datasets
print(f"Number of entries still unresolved: {len(still_weird)}")
print(still_weird.head())

print(f"Number of authors resolved and updated in all_comments_AS: {len(resolved_names)}")
print(all_comments_AS.head())

# Step 6: Save the updated still_weird dataset to CSV
still_weird_output_path = os.path.join(comments_directory, "still_weird.csv")
still_weird.to_csv(still_weird_output_path, index=False)
print(f"Updated still_weird saved to: {still_weird_output_path}")
```

```{python}
import re

# Step 1: Define a function to extract a full name from Full.Text if the Author has only one name
def find_full_name(author, full_text):
    """
    Searches for the last name in Full.Text when the Author column contains only one name.
    """
    if not isinstance(author, str) or not isinstance(full_text, str):
        return None

    # Escape special characters in the Author name
    escaped_author = re.escape(author)

    # Check if the Author has only one word (a single name)
    if len(author.split()) == 1:
        # Regex to find potential full names (e.g., "Firstname Lastname" or "Firstname M. Lastname")
        name_pattern = rf"\b{escaped_author} [A-Z](?:[a-z]+|\.)(?: [A-Z][a-z]+)?\b"  # Matches 'Firstname Lastname' or 'Firstname M. Lastname'
        match = re.search(name_pattern, full_text)
        if match:
            return match.group(0)  # Return the full name

    # Return None if no full name is found
    return None

# Step 2: Apply the function to the still_weird dataset
still_weird['Extracted_Full_Name'] = still_weird.apply(
    lambda row: find_full_name(row['Author'], row['Full.Text']), axis=1
)

# Step 3: Merge resolved names back into all_comments_AS
# Create a mapping of resolved names
resolved_full_names = still_weird[still_weird['Extracted_Full_Name'].notnull()][['Author', 'Extracted_Full_Name']].drop_duplicates()

# Map resolved full names back to the Author column in all_comments_AS
all_comments_AS['Author'] = all_comments_AS['Original_Username'].map(
    resolved_full_names.set_index('Author')['Extracted_Full_Name']
).fillna(all_comments_AS['Author'])

# Step 4: Update the still_weird dataset to include only unresolved authors
still_weird = still_weird[still_weird['Extracted_Full_Name'].isnull()].drop(columns=['Extracted_Full_Name'], inplace=False)

# Step 5: Drop the temporary Possible_Full_Name column in all_comments_AS (if it exists)
if 'Possible_Full_Name' in all_comments_AS.columns:
    all_comments_AS.drop(columns=['Possible_Full_Name'], inplace=True)

# Display the updated datasets
print(f"Number of entries still unresolved: {len(still_weird)}")
print(still_weird.head())

print(f"Number of authors updated with full names in all_comments_AS: {len(resolved_full_names)}")
print(all_comments_AS.head())

# Save the updated still_weird dataset to CSV
still_weird_output_path = os.path.join(comments_directory, "still_weird.csv")
still_weird.to_csv(still_weird_output_path, index=False)
print(f"Updated still_weird saved to: {still_weird_output_path}")

# Count how many authors were updated in all_comments_AS
updated_authors_count = (all_comments_AS['Author'] != all_comments_AS['Original_Username']).sum()
print(f"Number of authors updated with full names: {updated_authors_count}")
```

```{python}
import re

# Step 1: Define a function to extract names appearing after "--" or " --" in Full.Text
def extract_name_after_dash(full_text):
    """
    Extracts a name from the Full.Text column if it appears after "--" or " --".
    Matches names like "Firstname Lastname", "Firstname Middlename Lastname", or "Firstname M. Lastname".
    """
    if not isinstance(full_text, str):
        return None

    # Regex to match names after "--" or " --"
    pattern = r"--\s*([A-Z][a-z]+(?: [A-Z][a-z]+| [A-Z]\.)?(?: [A-Z][a-z]+)?)"
    match = re.search(pattern, full_text)
    if match:
        return match.group(1)  # Return the matched name
    return None

# Step 2: Apply the function to the still_weird dataset
still_weird.loc[:, 'Extracted_Name_After_Dash'] = still_weird['Full.Text'].apply(extract_name_after_dash)

# Step 3: Merge resolved names from this step back into all_comments_AS
# Create a mapping of resolved names
resolved_dash_names = still_weird[still_weird['Extracted_Name_After_Dash'].notnull()][['Author', 'Extracted_Name_After_Dash']].drop_duplicates()

# Ensure resolved_dash_names has unique indices
resolved_dash_names = resolved_dash_names.drop_duplicates(subset=['Author']).set_index('Author')

# Map resolved names back to the Author column in all_comments_AS
all_comments_AS['Author'] = all_comments_AS['Original_Username'].map(
    resolved_dash_names['Extracted_Name_After_Dash']
).fillna(all_comments_AS['Author'])

# Step 4: Update still_weird to include only unresolved authors
still_weird = still_weird[still_weird['Extracted_Name_After_Dash'].isnull()].drop(columns=['Extracted_Name_After_Dash'], inplace=False)

# Display the updated datasets
print(f"Number of entries still unresolved: {len(still_weird)}")
print(still_weird.head())

print(f"Number of authors resolved with names after '--': {len(resolved_dash_names)}")
print(all_comments_AS.head())

# Save the updated still_weird dataset to CSV
still_weird_output_path = os.path.join(comments_directory, "still_weird.csv")
still_weird.to_csv(still_weird_output_path, index=False)
print(f"Updated still_weird saved to: {still_weird_output_path}")

# Count how many authors were updated in all_comments_AS
updated_authors_count = (all_comments_AS['Author'] != all_comments_AS['Original_Username']).sum()
print(f"Number of authors updated with names after '--': {updated_authors_count}")
```

```{python}
# Define the output file path
output_file_path = os.path.join(comments_directory, "combined_comments_AS.csv")

# Save the DataFrame to the specified CSV file
all_comments_AS.to_csv(output_file_path, index=False)

# Confirm the save location
print(f"DataFrame saved to: {output_file_path}")
```
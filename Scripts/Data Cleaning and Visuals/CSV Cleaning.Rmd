---
title: "Usenet Project - CSV Cleaning"
author: "Emerson Johnston"
lastmodifeddate: "2024-08-07"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r Old Read and Merge, eval=FALSE, include=FALSE}
netmed_threads <- read.csv(file.path(threads_directory, "netmed_threads.csv"))
netmotss_threads <- read.csv(file.path(threads_directory, "netmotss_threads.csv"))
netnews_threads <- read.csv(file.path(threads_directory, "netnews_threads.csv"))
netpolitics_threads <- read.csv(file.path(threads_directory, "netpolitics_threads.csv"))
netreligion_threads <- read.csv(file.path(threads_directory, "netreligion_threads.csv"))
netsingles_threads <- read.csv(file.path(threads_directory, "netsingles_threads.csv"))

netmed_comments <- read.csv(file.path(comments_directory, "netmed_comments.csv"))
netmotss_comments <- read.csv(file.path(comments_directory, "netmotss_comments.csv"))
netnews_comments <- read.csv(file.path(comments_directory, "netnews_comments.csv"))
netpolitics_comments <- read.csv(file.path(comments_directory, "netpolitics_comments.csv"))
netreligion_comments <- read.csv(file.path(comments_directory, "netreligion_comments.csv"))
netsingles_comments <- read.csv(file.path(comments_directory, "netsingles_comments.csv"))

netmed_threads$newsgroup <- "netmed"
netmotss_threads$newsgroup <- "netmotss"
netnews_threads$newsgroup <- "netnews"
netpolitics_threads$newsgroup <- "netpolitics"
netreligion_threads$newsgroup <- "netreligion"
netsingles_threads$newsgroup <- "netsingles"

all_threads <- rbind(netmed_threads, netmotss_threads, netnews_threads, netpolitics_threads, netreligion_threads, netsingles_threads)

netmed_comments$newsgroup <- "netmed"
netmotss_comments$newsgroup <- "netmotss"
netnews_comments$newsgroup <- "netnews"
netpolitics_comments$newsgroup <- "netpolitics"
netreligion_comments$newsgroup <- "netreligion"
netsingles_comments$newsgroup <- "netsingles"

all_comments <- rbind(netmed_comments, netmotss_comments, netnews_comments, netpolitics_comments, netreligion_comments, netsingles_comments)

threads_directory <- "CSV Files/Threads"
comments_directory <- "CSV Files/Comments"

write.csv(all_threads, file.path(threads_directory, "combined_threads.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "combined_comments.csv"), row.names = FALSE)
```

# Maintainence
```{r Reset and Set Working Directory}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_webpage')
```

```{r Load Libraries, Directories, and Datasets}
# Load Libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(syuzhet)

# Directories
output_directory <- "/Users/emerson/Github/usenet_webpage"
threads_directory <- file.path(output_directory, "CSV Files/Threads")
comments_directory <- file.path(output_directory, "CSV Files/Comments")

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "combined_threads.csv"))
all_comments <- read.csv(file.path(comments_directory, "combined_comments.csv"))
```

## Dataset One - All Comments Cleaned
```{r Dataset Cleaning}
# Map newsgroups to IDs
newsgroup_ids <- c("netmed" = "NG01", "netmotss" = "NG02", "netnews" = "NG03",
                   "netpolitics" = "NG04", "netreligion" = "NG05", "netsingles" = "NG06")

# Threads cleaning
all_threads <- all_threads %>%
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         Unique_ThreadID = paste(newsgroup_ID, ThreadID, sep = "_")) %>%
  rename(NG_Relative_ThreadID = ThreadID) %>%
  select(Unique_ThreadID, newsgroup, newsgroup_ID, everything()) %>%
  mutate(Date = as.Date(Date, format = "%m/%d/%y"))

# Comments cleaning
all_comments <- all_comments %>%
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         Unique_CommentID = paste(newsgroup_ID, Unique.Comment.ID, sep = "_"),
         NG_Relative_CommentID = Unique.Comment.ID,
         NG_Relative_ThreadID = Thread.ID,
         Thread.ID = paste(newsgroup_ID, Thread.ID, sep = "_")) %>%
  select(Unique_CommentID, newsgroup, newsgroup_ID, Thread.ID, NG_Relative_CommentID, NG_Relative_ThreadID, everything()) %>%
  mutate(Date.and.Time = as.POSIXct(gsub("[^[:alnum:] [:punct:]]", "", Date.and.Time), format = "%b %d, %Y, %I:%M:%S%p"),
         Hour = as.numeric(format(Date.and.Time, "%H")),
         Date = as.Date(Date.and.Time))

# Add sentiment scores
all_comments <- all_comments %>%
  mutate(SentimentScore = get_sentiment(Full.Text, method = "afinn"))

# Remove Duplicates
all_comments <- all_comments %>%
  distinct(Full.Text, .keep_all = TRUE)

write.csv(all_threads, file.path(threads_directory, "dataset1_threads.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "dataset1_comments.csv"), row.names = FALSE)
```

## Dataset Two - AIDS-Related Comments (1982–1986)
```{r Filtering by Keywords with Weighted Groupsand to between 1982-1986}
# Step 1: Filter threads and comments for the desired period
relevant_threads <- all_threads %>%
  filter(Date >= as.Date("1982-01-01") & Date < as.Date("1987-01-01"))

# Step 2: Define grouped keywords with their respective weights
keyword_groups <- list(
  `10` = c("aids", "acquired immune deficiency syndrome", "human immunodeficiency virus", 
           "grid", "gay-related immune deficiency", "gay plague"),
  `9` = c("hiv", "htlv", "human t-lymphotropic virus", "gay cancer", "kaposi's sarcoma"),
  `8` = c("slim disease", "pneumocystis pneumonia", "gay disease", "homosexual disease", 
          "immune disease"),
  `7` = c("fag cancer", "homosexual cancer", "gay compromise syndrome", "aids hysteria"),
  `6` = c("gay fear", "fear of gay", "fear of homosexual", "gay panic", "queer disease", 
          "pink disease"),
  `5` = c("sexual orientation disease", "sexual deviant disease", "patients", 
          "victims", "carriers"),
  `4` = c("bisexual", "gay/bisexual", "gay men", "homosexuals", "gay sex", "homophobia"),
  `3` = c("virus", "syndrome", "outbreak", "pandemic", "blood disease", "blood test", 
          "sexual transmission"),
  `2` = c("sodomy", "bathhouses", "promiscuity", "gay community", "homosexual acts", 
          "infection", "epidemic"),
  `1` = c("sex", "queer", "gays", "lesbians", "lifestyle", "contagion", 
          "unsafe practices")
)

# Combine all keywords into a single named vector with weights
keyword_weights <- unlist(lapply(names(keyword_groups), function(weight) {
  setNames(rep(as.numeric(weight), length(keyword_groups[[weight]])), keyword_groups[[weight]])
}))

# Define title keywords with weights (subset of keyword_weights)
title_keyword_weights <- keyword_weights[names(keyword_weights) %in% c(
  "aids", "htlv", "hiv", "acquired immune deficiency syndrome",
  "human immunodeficiency virus", "gay plague", "gay cancer",
  "kaposi's sarcoma", "pneumocystis pneumonia", "homosexual disease",
  "gay disease"
)]

# Step 3: Add weighted relevancy score to comments
comments_with_relevancy <- all_comments %>%
  filter(Date >= as.Date("1982-01-01") & Date < as.Date("1987-01-01")) %>%
  mutate(Relevancy = rowSums(sapply(names(keyword_weights), function(kw) {
    grepl(kw, tolower(Full.Text), fixed = TRUE) * keyword_weights[kw]
  }))) %>%
  filter(Relevancy > 0)

# Step 4: Identify relevant threads
relevant_threads <- relevant_threads %>%
  mutate(
    TitleHasKeyword = rowSums(sapply(names(title_keyword_weights), function(kw) {
      grepl(kw, tolower(Thread.Title), fixed = TRUE) * title_keyword_weights[kw]
    })) > 0
  ) %>%
  filter(Unique_ThreadID %in% comments_with_relevancy$Thread.ID | TitleHasKeyword)

# Step 5: Capture all comments in relevant threads
relevant_comments <- all_comments %>%
  filter(Thread.ID %in% relevant_threads$Unique_ThreadID) %>%
  mutate(
    Relevancy = rowSums(sapply(names(keyword_weights), function(kw) {
      grepl(kw, tolower(Full.Text), fixed = TRUE) * keyword_weights[kw]
    }))
  )

# Step 6: Calculate thread-level relevancy
thread_relevancy <- comments_with_relevancy %>%
  group_by(Thread.ID) %>%
  summarise(
    AvgCommentRelevancy = mean(Relevancy, na.rm = TRUE)  # Average score per relevant comment
  ) %>%
  mutate(
    TitleRelevancy = relevant_threads$TitleHasKeyword[match(Thread.ID, relevant_threads$Unique_ThreadID)] * 10,  # High relevancy for title match
    ThreadRelevancyScore = ifelse(TitleRelevancy == 10, 10, AvgCommentRelevancy + TitleRelevancy)  # Prioritize threads with relevant titles
  )

# Step 7: Merge back thread relevancy into relevant_threads
relevant_threads <- relevant_threads %>%
  left_join(thread_relevancy, by = c("Unique_ThreadID" = "Thread.ID"))

# Step 8: Save the final datasets
write.csv(relevant_threads, file.path(threads_directory, "dataset2_threads.csv"), row.names = FALSE)
write.csv(relevant_comments, file.path(comments_directory, "dataset2_comments.csv"), row.names = FALSE)
```

## Dataset Three - AIDS-Related Comments (1982–1986) - Filtered to Exclude Bottom Quartile of Relevancy Scores
```{r Calculate Quartiles and Plots}
# Step 1: Separate threads with TitleRelevancy == 10
high_title_relevance_threads <- relevant_threads %>%
  filter(TitleRelevancy == 10)

remaining_threads <- relevant_threads %>%
  filter(TitleRelevancy != 10)

# Step 2: Calculate quartiles based on value ranges
min_value <- min(remaining_threads$ThreadRelevancyScore, na.rm = TRUE)
max_value <- max(remaining_threads$ThreadRelevancyScore, na.rm = TRUE)
range <- max_value - min_value
quartile_size <- range / 4

# Create breaks based on value ranges
breaks <- c(
  min_value,
  min_value + quartile_size,
  min_value + (2 * quartile_size),
  min_value + (3 * quartile_size),
  max_value
)

# Assign quartiles based on these breaks
remaining_threads <- remaining_threads %>%
  mutate(Quartile = cut(ThreadRelevancyScore, 
                       breaks = breaks, 
                       labels = c("Q1", "Q2", "Q3", "Q4"),
                       include.lowest = TRUE))

# Create quartile plot
quartile_plot <- ggplot(remaining_threads, aes(x = Quartile)) +
  geom_bar(fill = "skyblue", color = "black") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  labs(title = "Frequency of Thread Relevancy Quartiles",
       x = "Quartile",
       y = "Frequency") +
  theme_bw()

print(quartile_plot)
```

```{r Create Dataset Three}
# First, remove the Quartile column from remaining_threads after filtering
filtered_relevant_threads <- rbind(
  remaining_threads %>% 
    filter(Quartile != "Q1") %>%
    select(-Quartile),
  high_title_relevance_threads
)

filtered_irrelevant_threads <- remaining_threads %>% 
    filter(Quartile == "Q1")

# Create corresponding comments dataset
filtered_relevant_comments <- relevant_comments %>%
  filter(Thread.ID %in% filtered_relevant_threads$Unique_ThreadID)

# Save the final datasets
write.csv(filtered_relevant_threads, 
          file.path(threads_directory, "dataset3_threads.csv"), 
          row.names = FALSE)
write.csv(filtered_relevant_comments, 
          file.path(comments_directory, "dataset3_comments.csv"), 
          row.names = FALSE)

# Print summary statistics
cat("Number of threads in Dataset Three:", nrow(filtered_relevant_threads), "\n")
cat("Number of comments in Dataset Three:", nrow(filtered_relevant_comments), "\n")

# Print quartile boundaries for verification
cat("\nQuartile value ranges:\n")
for(i in 1:4) {
  cat(sprintf("Q%d: %.2f to %.2f\n", 
              i, 
              breaks[i], 
              breaks[i+1]))
}
````

```{r}
debugging1 <- relevant_comments %>%
  filter(newsgroup == "netreligion")

debugging2 <- filtered_relevant_comments %>%
  filter(newsgroup == "netreligion")

debugging3 <- filtered_relevant_threads %>%
  filter(newsgroup == "netreligion")
```

## Dataset Four - AIDS-Related Comments (1982–1986) - Filtered to Exclude Bottom Quartiles of Relevancy Scores and for Only Influential Authors Comments and Threads
```{r Identify Influential Authors}
library(dplyr)
library(igraph)

# Step 1: Remove authors with only one comment
filter_authors_with_multiple_comments <- function(filtered_relevant_comments) {
  print("Filtering out authors with only one comment...")
  
  authors_with_multiple_comments <- filtered_relevant_comments %>%
    group_by(Author) %>%
    filter(n() > 1) %>%
    ungroup()
  
  print(paste("Number of authors after filtering:", n_distinct(authors_with_multiple_comments$Author)))
  return(authors_with_multiple_comments)
}

filtered_relevant_comments_authors_with_multiple_comments <- filter_authors_with_multiple_comments(filtered_relevant_comments)

# Step 2: Create an author network based on thread co-participation
create_author_network <- function(filtered_relevant_comments_authors_with_multiple_comments) {
  print("Creating author network...")

  # Generate author pairs for each thread
  author_pairs <- filtered_relevant_comments_authors_with_multiple_comments %>%
    filter(!is.na(Thread.ID)) %>%
    group_by(Thread.ID) %>%
    summarise(
      Pairs = list(if (length(unique(Author)) > 1) {
        as.data.frame(t(combn(unique(Author), 2)))
      } else {
        NULL
      })
    ) %>%
    unnest(Pairs, keep_empty = TRUE) %>%  # Allow empty results for threads with only one author
    rename(Authors1 = V1, Authors2 = V2) %>%
    filter(!is.na(Authors1) & !is.na(Authors2)) %>%  # Remove any NA pairs
    count(Authors1, Authors2, name = "weight", sort = TRUE)

  # Rename columns for clarity
  edge_df <- author_pairs %>%
    rename(source = Authors1, target = Authors2)
  
  print(paste("Created network with", nrow(edge_df), "edges"))
  return(edge_df)
}

edge_df <- create_author_network(filtered_relevant_comments_authors_with_multiple_comments)

# Step 3: Identify influential authors
identify_influential_authors <- function(edge_df, n_authors = 15) {
  print("Identifying influential authors...")
  
  if (nrow(edge_df) == 0) {
    warning("Empty edge dataframe. Cannot identify influential authors.")
    return(data.frame(Author = character(0), Influence = numeric(0)))
  }
  
  # Create graph from edge dataframe
  graph <- graph_from_data_frame(edge_df, directed = FALSE)
  
  # Check if weights are present and calculate degree centrality
  if ("weight" %in% edge_attr_names(graph)) {
    degree_centrality <- strength(graph, mode = "all", weights = E(graph)$weight)  # Use strength for weighted degree
  } else {
    degree_centrality <- degree(graph, mode = "all")  # Unweighted degree
  }
  
  # Create dataframe of authors and their influence scores
  author_influence <- data.frame(
    Author = names(degree_centrality),
    Influence = degree_centrality
  )
  
  # Select top N influential authors
  top_influential_authors <- author_influence %>%
    arrange(desc(Influence)) %>%
    slice_head(n = n_authors)
  
  print("Top influential authors identified:")
  print(top_influential_authors)
  
  return(top_influential_authors)
}

top_influential_authors <- identify_influential_authors(edge_df, n_authors = 20)
```

```{r Influential Authors Comments and Threads}
# Step 4: Generate data for influential authors and capture all comments in their threads
generate_influential_author_data <- function(filtered_relevant_comments, filtered_relevant_threads, top_authors) {
  # Step 1: Get comments authored by influential authors
  influential_author_comments <- filtered_relevant_comments %>%
    filter(Author %in% top_authors$Author)
  
  # Step 2: Get the thread IDs from these comments
  threads_with_influential_authors <- influential_author_comments %>%
    distinct(Thread.ID) %>%
    pull(Thread.ID)  # Extract as a vector
  
  # Step 3: Retrieve all comments in those threads
  all_comments_in_influential_threads <- filtered_relevant_comments %>%
    filter(Thread.ID %in% threads_with_influential_authors)
  
  # Step 4: Comments specifically authored by influential authors (from Step 1)
  top_authors_comments <- influential_author_comments
  
  # Step 5: Get the corresponding threads
  threads_involving_influential_authors <- filtered_relevant_threads %>%
    filter(Unique_ThreadID %in% threads_with_influential_authors)
  
  # Return datasets
  return(list(
    threads = threads_involving_influential_authors,
    comments_all = all_comments_in_influential_threads,
    comments_only_influential = top_authors_comments
  ))
}

influential_data <- generate_influential_author_data(
  filtered_relevant_comments,
  filtered_relevant_threads,
  top_influential_authors
)

# Save datasets
write.csv(influential_data$threads, file.path(threads_directory, "dataset4_threads.csv"), row.names = FALSE)
write.csv(influential_data$comments_all, file.path(comments_directory, "dataset4_comments_all.csv"), row.names = FALSE)
write.csv(influential_data$comments_only_influential, file.path(comments_directory, "dataset4_comments_onlyinfluential.csv"), row.names = FALSE)

# Print summary
cat("Number of threads involving influential authors:", nrow(influential_data$threads), "\n")
cat("Number of comments in these threads (all comments):", nrow(influential_data$comments_all), "\n")
cat("Number of comments by influential authors only:", nrow(influential_data$comments_only_influential), "\n")
```

# Descriptive Statistics Tables
## Dataset 1 Descriptive Statistics
```{r Dataset 1 Descriptive Statistics}
library(sjPlot)

# Dataset 1: All Comments
dataset1_summary <- all_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Threads = n_distinct(Thread.ID),
    Comments = n(),
    Authors = n_distinct(Author),
    Avg_Comments_Per_Thread = Comments / Threads,
    Avg_Sentiment_Score = mean(SentimentScore, na.rm = TRUE)
  )

# Add a totals row
dataset1_totals <- dataset1_summary %>%
  summarize(
    newsgroup = "Total",
    Threads = sum(Threads),
    Comments = sum(Comments),
    Authors = n_distinct(all_comments$Author),
    Avg_Comments_Per_Thread = sum(Comments) / sum(Threads),
    Avg_Sentiment_Score = mean(all_comments$SentimentScore, na.rm = TRUE)
  )

# Combine the summary with the totals row
dataset1_summary <- bind_rows(dataset1_summary, dataset1_totals)

# Save or print the summary
tab_df(dataset1_summary, file = paste0(output_directory, "/Images and Tables/Tables/dataset1_statistics.html"))
```

## Dataset 2 Descriptive Statistics
```{r Dataset 2 Descriptive Statistics}
dataset2_stats <- relevant_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Related_Comments = n(),
    Related_Threads = n_distinct(Thread.ID),
    Total_Comments_in_Related_Threads = sum(n()),
    Unique_Authors_in_Related_Threads = n_distinct(Author),
    Percent_Comments_with_Keyword = mean(Relevancy > 0) * 100,
    Avg_Comment_Relevancy = mean(Relevancy, na.rm = TRUE),
    Avg_Thread_Relevancy = mean(relevant_threads$ThreadRelevancyScore[relevant_threads$newsgroup == first(newsgroup)], na.rm = TRUE)
  )

# Add a totals row
dataset2_totals <- dataset2_stats %>%
  summarize(
    newsgroup = "Total",
    Related_Comments = sum(Related_Comments),
    Related_Threads = sum(Related_Threads),
    Total_Comments_in_Related_Threads = sum(Total_Comments_in_Related_Threads),
    Unique_Authors_in_Related_Threads = n_distinct(relevant_comments$Author),
    Percent_Comments_with_Keyword = mean(Percent_Comments_with_Keyword),
    Avg_Comment_Relevancy = mean(Avg_Comment_Relevancy),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy)
  )

dataset2_stats <- bind_rows(dataset2_stats, dataset2_totals)

# Save as HTML or CSV
tab_df(dataset2_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset2_statistics.html"))
```

## Dataset 3 Descriptive Statistics
```{r Dataset 3 Descriptive Statistics}
dataset3_stats <- filtered_relevant_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Related_Comments = n(),
    Related_Threads = n_distinct(Thread.ID),
    Total_Comments_in_Related_Threads = sum(n()),
    Unique_Authors_in_Related_Threads = n_distinct(Author),
    Percent_Comments_with_Keyword = mean(Relevancy > 0) * 100,
    Avg_Comment_Relevancy = mean(Relevancy, na.rm = TRUE),
    Avg_Thread_Relevancy = mean(filtered_relevant_threads$ThreadRelevancyScore[filtered_relevant_threads$newsgroup == first(newsgroup)], na.rm = TRUE)
  )

# Add a totals row
dataset3_totals <- dataset3_stats %>%
  summarize(
    newsgroup = "Total",
    Related_Comments = sum(Related_Comments),
    Related_Threads = sum(Related_Threads),
    Total_Comments_in_Related_Threads = sum(Total_Comments_in_Related_Threads),
    Unique_Authors_in_Related_Threads = n_distinct(filtered_relevant_comments$Author),
    Percent_Comments_with_Keyword = mean(Percent_Comments_with_Keyword),
    Avg_Comment_Relevancy = mean(Avg_Comment_Relevancy),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy)
  )

dataset3_stats <- bind_rows(dataset3_stats, dataset3_totals)

# Save as HTML or CSV
tab_df(dataset3_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset3_statistics.html"))
```

## Dataset 4 Descriptive Statistics
```{r Dataset 4 Descriptive Statistics}
library(dplyr)

# Generate descriptive statistics for Dataset 4
dataset4_stats <- influential_data$comments_all %>%
  group_by(newsgroup) %>%
  summarize(
    Unique_Threads = n_distinct(Thread.ID),  # Unique threads with influential authors
    Total_Comments_All = n(),  # Total comments in relevant threads
    Influential_Comments = sum(Author %in% top_influential_authors$Author),  # Comments by influential authors
    Non_Influential_Comments = Total_Comments_All - Influential_Comments,  # Comments by non-influential authors
    Avg_Comment_Relevancy_All = mean(Relevancy, na.rm = TRUE),  # Avg relevancy for all comments
    Avg_Comment_Relevancy_Influential = mean(Relevancy[Author %in% top_influential_authors$Author], na.rm = TRUE),  # Avg relevancy for influential authors
    Avg_Comment_Relevancy_Non_Influential = mean(Relevancy[!(Author %in% top_influential_authors$Author)], na.rm = TRUE),  # Avg relevancy for non-influential authors
    Percent_Comments_With_Keyword_All = mean(Relevancy > 0) * 100,  # Percent of all comments with keyword
    Percent_Comments_With_Keyword_Influential = mean(Relevancy[Author %in% top_influential_authors$Author] > 0) * 100,  # Percent of influential comments with keyword
    Percent_Comments_With_Keyword_Non_Influential = mean(Relevancy[!(Author %in% top_influential_authors$Author)] > 0) * 100,  # Percent of non-influential comments with keyword
    Unique_Authors_in_Threads = n_distinct(Author),  # Unique authors in threads
    Avg_Thread_Relevancy = mean(influential_data$threads$ThreadRelevancyScore[influential_data$threads$newsgroup == first(newsgroup)], na.rm = TRUE)  # Average thread relevancy
  )

# Add a totals row for all newsgroups combined
dataset4_totals <- dataset4_stats %>%
  summarize(
    newsgroup = "Total",
    Unique_Threads = sum(Unique_Threads),
    Total_Comments_All = sum(Total_Comments_All),
    Influential_Comments = sum(Influential_Comments),
    Non_Influential_Comments = sum(Non_Influential_Comments),
    Avg_Comment_Relevancy_All = mean(Avg_Comment_Relevancy_All, na.rm = TRUE),
    Avg_Comment_Relevancy_Influential = mean(Avg_Comment_Relevancy_Influential, na.rm = TRUE),
    Avg_Comment_Relevancy_Non_Influential = mean(Avg_Comment_Relevancy_Non_Influential, na.rm = TRUE),
    Percent_Comments_With_Keyword_All = mean(Percent_Comments_With_Keyword_All, na.rm = TRUE),
    Percent_Comments_With_Keyword_Influential = mean(Percent_Comments_With_Keyword_Influential, na.rm = TRUE),
    Percent_Comments_With_Keyword_Non_Influential = mean(Percent_Comments_With_Keyword_Non_Influential, na.rm = TRUE),
    Unique_Authors_in_Threads = sum(Unique_Authors_in_Threads),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy, na.rm = TRUE)
  )

# Combine statistics and totals
dataset4_stats <- bind_rows(dataset4_stats, dataset4_totals)

# Save or display the table
tab_df(dataset4_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset4_statistics.html"))

# Print the table
print(dataset4_stats)
```

## Dataset 4 Author Statistics
```{r Dataset 4 Author Statistics}
# Filter influential authors' participation in Dataset Three
dataset3_influential_comments <- filtered_relevant_comments %>%
  filter(Author %in% top_influential_authors$Author)

dataset3_influential_threads <- filtered_relevant_threads %>%
  filter(Unique_ThreadID %in% dataset3_influential_comments$Thread.ID)

# Add descriptive statistics for each influential author
author_stats <- dataset3_influential_comments %>%
  group_by(Author) %>%
  summarise(
    Influence = unique(top_influential_authors$Influence[top_influential_authors$Author == Author]),
    Num_Comments = n_distinct(NG_Relative_CommentID),  # Number of comments authored
    Num_Threads = n_distinct(Thread.ID),  # Number of threads participated in
    Threads_Started = sum(Comment.ID == "CM00001", na.rm = TRUE),  # Threads started
    Avg_Sentiment = mean(SentimentScore, na.rm = TRUE),  # Average sentiment score
    Avg_Relevancy = mean(Relevancy, na.rm = TRUE)  # Average relevancy score
  )

# Filter out authors with missing Influence scores and sort by Influence
author_stats <- author_stats %>%
  filter(!is.na(Influence)) %>%
  arrange(desc(Influence))

# Save the table as an HTML file
tab_df(author_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset4_author_statistics.html"))

# Display the table
print(author_stats)
```

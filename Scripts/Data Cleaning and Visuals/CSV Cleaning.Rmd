---
title: "Usenet Project - CSV Cleaning"
author: "Emerson Johnston"
lastmodifeddate: "2024-08-07"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r Old Read and Merge, eval=FALSE, include=FALSE}
netmed_threads <- read.csv(file.path(threads_directory, "netmed_threads.csv"))
netmotss_threads <- read.csv(file.path(threads_directory, "netmotss_threads.csv"))
netnews_threads <- read.csv(file.path(threads_directory, "netnews_threads.csv"))
netpolitics_threads <- read.csv(file.path(threads_directory, "netpolitics_threads.csv"))
netreligion_threads <- read.csv(file.path(threads_directory, "netreligion_threads.csv"))
netsingles_threads <- read.csv(file.path(threads_directory, "netsingles_threads.csv"))

netmed_comments <- read.csv(file.path(comments_directory, "netmed_comments.csv"))
netmotss_comments <- read.csv(file.path(comments_directory, "netmotss_comments.csv"))
netnews_comments <- read.csv(file.path(comments_directory, "netnews_comments.csv"))
netpolitics_comments <- read.csv(file.path(comments_directory, "netpolitics_comments.csv"))
netreligion_comments <- read.csv(file.path(comments_directory, "netreligion_comments.csv"))
netsingles_comments <- read.csv(file.path(comments_directory, "netsingles_comments.csv"))

netmed_threads$newsgroup <- "netmed"
netmotss_threads$newsgroup <- "netmotss"
netnews_threads$newsgroup <- "netnews"
netpolitics_threads$newsgroup <- "netpolitics"
netreligion_threads$newsgroup <- "netreligion"
netsingles_threads$newsgroup <- "netsingles"

all_threads <- rbind(netmed_threads, netmotss_threads, netnews_threads, netpolitics_threads, netreligion_threads, netsingles_threads)

netmed_comments$newsgroup <- "netmed"
netmotss_comments$newsgroup <- "netmotss"
netnews_comments$newsgroup <- "netnews"
netpolitics_comments$newsgroup <- "netpolitics"
netreligion_comments$newsgroup <- "netreligion"
netsingles_comments$newsgroup <- "netsingles"

all_comments <- rbind(netmed_comments, netmotss_comments, netnews_comments, netpolitics_comments, netreligion_comments, netsingles_comments)

threads_directory <- "CSV Files/Threads"
comments_directory <- "CSV Files/Comments"

write.csv(all_threads, file.path(threads_directory, "combined_threads.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "combined_comments.csv"), row.names = FALSE)
```

# Maintainence
```{r Reset and Set Working Directory}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_webpage')
```

```{r Load Libraries, Directories, and Datasets}
# Load Libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(syuzhet)

# Directories
output_directory <- "/Users/emerson/Github/usenet_webpage"
threads_directory <- file.path(output_directory, "CSV Files/Threads")
comments_directory <- file.path(output_directory, "CSV Files/Comments")

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "combined_threads.csv"))
all_comments <- read.csv(file.path(comments_directory, "combined_comments.csv"))
```

## Dataset One - All Comments Cleaned
```{r Dataset Cleaning}
# Map newsgroups to IDs
newsgroup_ids <- c("netmed" = "NG01", "netmotss" = "NG02", "netnews" = "NG03",
                   "netpolitics" = "NG04", "netreligion" = "NG05", "netsingles" = "NG06")

# Threads cleaning
all_threads <- all_threads %>%
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         Unique_ThreadID = paste(newsgroup_ID, ThreadID, sep = "_")) %>%
  rename(NG_Relative_ThreadID = ThreadID) %>%
  select(Unique_ThreadID, newsgroup, newsgroup_ID, everything()) %>%
  mutate(Date = as.Date(Date, format = "%m/%d/%y"))

# Comments cleaning
all_comments <- all_comments %>%
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         Unique_CommentID = paste(newsgroup_ID, Unique.Comment.ID, sep = "_"),
         NG_Relative_CommentID = Unique.Comment.ID,
         NG_Relative_ThreadID = Thread.ID,
         Thread.ID = paste(newsgroup_ID, Thread.ID, sep = "_")) %>%
  select(Unique_CommentID, newsgroup, newsgroup_ID, Thread.ID, NG_Relative_CommentID, NG_Relative_ThreadID, everything()) %>%
  mutate(Date.and.Time = as.POSIXct(gsub("[^[:alnum:] [:punct:]]", "", Date.and.Time), format = "%b %d, %Y, %I:%M:%S%p"),
         Hour = as.numeric(format(Date.and.Time, "%H")),
         Date = as.Date(Date.and.Time))

# Add sentiment scores
all_comments <- all_comments %>%
  mutate(SentimentScore = get_sentiment(Full.Text, method = "afinn"))

write.csv(all_threads, file.path(threads_directory, "dataset1_threads.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "dataset1_comments.csv"), row.names = FALSE)
```

## Dataset Two - AIDS-Related Comments (1982–1986)
```{r Filtering by Keywords and to between 1982-1986}
# Filter threads and comments for the desired period
relevant_threads <- all_threads %>%
  filter(Date >= as.Date("1982-01-01") & Date < as.Date("1987-01-01"))
relevant_comments <- all_comments %>%
  filter(Date >= as.Date("1982-01-01") & Date < as.Date("1987-01-01"))

# Keywords for relevancy
keywords <- c("aids", "htlv", "hiv", "acquired immune deficiency syndrome", "human immunodeficiency virus",
              "gay plague", "gay cancer", "kaposi's sarcoma", "pneumocystis pneumonia",
              "homosexual disease", "gay disease", "gay fear", "fear of gay", "fear of homosexual",
              "sexual orientation disease", "virus", "sex", "promiscuity", "patients")

# Identify comments with keywords
relevant_comments <- relevant_comments %>%
  mutate(Relevancy = rowSums(sapply(keywords, function(kw) grepl(kw, tolower(Full.Text), fixed = TRUE)))) %>%
  filter(Relevancy > 0)

# Filter relevant threads to retain all original variables but only include threads from relevant comments
relevant_threads <- relevant_threads %>%
  filter(Unique_ThreadID %in% relevant_comments$Thread.ID)

# Save relevant threads and comments
write.csv(relevant_threads, file.path(threads_directory, "dataset2_threads.csv"), row.names = FALSE)
write.csv(relevant_comments, file.path(comments_directory, "dataset2_comments.csv"), row.names = FALSE)
```

## Dataset Three - AIDS-Related Comments (1982–1986) - Filtered to Exclude Bottom Two Quintiles of Relevancy Scores
```{r Calculate Thread Level Relevancy Score and Do Some Plots}
# Step 1: Calculate thread-level relevancy score by summing comment scores
thread_relevancy <- relevant_comments %>%
  group_by(Thread.ID) %>%
  summarise(ThreadRelevancyScore = sum(Relevancy))

# Step 2: Merge thread relevancy scores into relevant_threads
relevant_threads <- relevant_threads %>%
  left_join(thread_relevancy, by = c("Unique_ThreadID" = "Thread.ID"))

# Step 3: Replace NA relevancy scores with zero (for threads without matching comments)
relevant_threads$ThreadRelevancyScore[is.na(relevant_threads$ThreadRelevancyScore)] <- 0
```
```{r}
# Step 4: Assign Quintiles to thread relevancy scores
min_value <- min(relevant_threads$ThreadRelevancyScore, na.rm = TRUE)
max_value <- max(relevant_threads$ThreadRelevancyScore, na.rm = TRUE)

# Create four equal-sized bins between the min and max values
breaks <- seq(min_value, max_value, length.out = 6) 

# Create a factor for Quintiles ranges based on these breaks
relevant_threads$Quintile <- cut(
  relevant_threads$ThreadRelevancyScore,
  breaks = breaks,
  labels = c("Q1", "Q2", "Q3", "Q4", "Q5"),
  include.lowest = TRUE
)

# Plot the bar graph showing the frequency of each quartile
ggplot(relevant_threads, aes(x = Quintile)) +
  geom_bar(fill = "skyblue", color = "black") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +  # Add labels above bars
  labs(title = "Frequency of Thread Relevancy Quintiles",
       x = "Quintile",
       y = "Frequency") +
  theme_bw()
````

```{r}
# Step 5: Filter threads to exclude lowest quintile
filtered_relevant_threads <- relevant_threads %>%
  filter(as.numeric(Quintile) > 1)

# Step 6: Include all comments from threads in the top 3 Quintiles
filtered_relevant_comments <- relevant_comments %>%
  filter(Thread.ID %in% filtered_relevant_threads$Unique_ThreadID)

# Step 7: Save Dataset 3
write.csv(filtered_relevant_threads, file.path(threads_directory, "dataset3_threads.csv"), row.names = FALSE)
write.csv(filtered_relevant_comments, file.path(comments_directory, "dataset3_comments.csv"), row.names = FALSE)
```

## Dataset Four - AIDS-Related Comments (1982–1986) - Filtered to Exclude Bottom Two Quintiles of Relevancy Scores and for Only Influential Authors Comments and Threads
```{r}
library(dplyr)
library(igraph)

# Step 1: Create an author network based on thread co-participation
create_author_network <- function(filtered_relevant_comments) {
  print("Creating author network...")

  # Generate author pairs for each thread
  author_pairs <- filtered_relevant_comments %>%
    filter(!is.na(Thread.ID)) %>%
    group_by(Thread.ID) %>%
    summarise(
      Pairs = list(if (length(unique(Author)) > 1) {
        as.data.frame(t(combn(unique(Author), 2)))
      } else {
        NULL
      })
    ) %>%
    unnest(Pairs, keep_empty = TRUE) %>%  # Allow empty results for threads with only one author
    rename(Authors1 = V1, Authors2 = V2) %>%
    filter(!is.na(Authors1) & !is.na(Authors2)) %>%  # Remove any NA pairs
    count(Authors1, Authors2, name = "weight", sort = TRUE)

  # Rename columns for clarity
  edge_df <- author_pairs %>%
    rename(source = Authors1, target = Authors2)
  
  print(paste("Created network with", nrow(edge_df), "edges"))
  return(edge_df)
}


# Step 2: Identify influential authors
identify_influential_authors <- function(edge_df, n_authors = 15) {
  print("Identifying influential authors...")
  
  if (nrow(edge_df) == 0) {
    warning("Empty edge dataframe. Cannot identify influential authors.")
    return(data.frame(Author = character(0), Influence = numeric(0)))
  }
  
  # Create graph from edge dataframe
  graph <- graph_from_data_frame(edge_df, directed = FALSE)
  
  # Check if weights are present and calculate degree centrality
  if ("weight" %in% edge_attr_names(graph)) {
    degree_centrality <- strength(graph, mode = "all", weights = E(graph)$weight)  # Use strength for weighted degree
  } else {
    degree_centrality <- degree(graph, mode = "all")  # Unweighted degree
  }
  
  # Create dataframe of authors and their influence scores
  author_influence <- data.frame(
    Author = names(degree_centrality),
    Influence = degree_centrality
  )
  
  # Select top N influential authors
  top_influential_authors <- author_influence %>%
    arrange(desc(Influence)) %>%
    slice_head(n = n_authors)
  
  print("Top influential authors identified:")
  print(top_influential_authors)
  
  return(top_influential_authors)
}

# Step 3: Filter comments and threads based on top authors
generate_influential_author_data <- function(filtered_relevant_comments, filtered_relevant_threads, top_authors) {
  # Filter comments and threads for the top authors
  top_authors_comments <- filtered_relevant_comments %>%
    filter(Author %in% top_authors$Author)
  
  top_authors_threads <- filtered_relevant_threads %>%
    filter(Unique_ThreadID %in% top_authors_comments$Thread.ID)
  
  return(list(comments = top_authors_comments, threads = top_authors_threads))
}

# Full Workflow
# Generate the author network using filtered_relevant_comments
edge_df <- create_author_network(filtered_relevant_comments)

# Identify the top 20 influential authors
top_influential_authors <- identify_influential_authors(edge_df, n_authors = 20)

# Filter comments and threads for top authors
influential_data <- generate_influential_author_data(
  filtered_relevant_comments,
  filtered_relevant_threads,
  top_influential_authors
)

# Save datasets
write.csv(influential_data$threads, file.path(threads_directory, "dataset4_threads.csv"), row.names = FALSE)
write.csv(influential_data$comments, file.path(comments_directory, "dataset4_comments.csv"), row.names = FALSE)
```

# Descriptive Statistics Tables
```{r Descriptive Statistics}
# Calculate statistics using dplyr
summary_df <- all_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Threads = n_distinct(Thread.ID),
    Comments = n(),
    Authors = n_distinct(Author),
    Avg_Comments_Per_Thread = Comments / Threads,
    Avg_Sentiment_Score = mean(SentimentScore, na.rm = TRUE)
  )

# Add a totals row
totals <- summary_df %>%
  summarize(
    newsgroup = "Total",
    Threads = sum(Threads),
    Comments = sum(Comments),
    Authors = n_distinct(all_comments$Author),
    Avg_Comments_Per_Thread = sum(Comments) / sum(Threads),
    Avg_Sentiment_Score = mean(all_comments$SentimentScore, na.rm = TRUE)
  )

# Combine the summary with the totals row
summary_df <- bind_rows(summary_df, totals)

# Print the summary data frame with totals
tab_df(summary_df, file = paste0(output_directory, "images and tables/table_descriptive_statistics_full.html"))
```

## Dataset 1 Descriptive Statistics
```{r Dataset 1 Descriptive Statistics}
library(sjPlot)

# Dataset 1: All Comments
dataset1_summary <- all_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Threads = n_distinct(Thread.ID),
    Comments = n(),
    Authors = n_distinct(Author),
    Avg_Comments_Per_Thread = Comments / Threads,
    Avg_Sentiment_Score = mean(SentimentScore, na.rm = TRUE)
  )

# Add a totals row
dataset1_totals <- dataset1_summary %>%
  summarize(
    newsgroup = "Total",
    Threads = sum(Threads),
    Comments = sum(Comments),
    Authors = n_distinct(all_comments$Author),
    Avg_Comments_Per_Thread = sum(Comments) / sum(Threads),
    Avg_Sentiment_Score = mean(all_comments$SentimentScore, na.rm = TRUE)
  )

# Combine the summary with the totals row
dataset1_summary <- bind_rows(dataset1_summary, dataset1_totals)

# Save or print the summary
tab_df(dataset1_summary, file = paste0(output_directory, "/Images and Tables/Tables/dataset1_descriptive_statistics.html"))
```

## Dataset 2 Descriptive Statistics
```{r Dataset 2 Descriptive Statistics}
dataset2_stats <- relevant_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Related_Comments = n(),
    Related_Threads = n_distinct(Thread.ID),
    Total_Comments_in_Related_Threads = sum(n()),
    Unique_Authors_in_Related_Threads = n_distinct(Author),
    Percent_Comments_with_Keyword = mean(Relevancy > 0) * 100,
    Avg_Comment_Relevancy = mean(Relevancy, na.rm = TRUE),
    Avg_Thread_Relevancy = mean(relevant_threads$ThreadRelevancyScore[relevant_threads$newsgroup == first(newsgroup)], na.rm = TRUE)
  )

# Add a totals row
dataset2_totals <- dataset2_stats %>%
  summarize(
    newsgroup = "Total",
    Related_Comments = sum(Related_Comments),
    Related_Threads = sum(Related_Threads),
    Total_Comments_in_Related_Threads = sum(Total_Comments_in_Related_Threads),
    Unique_Authors_in_Related_Threads = n_distinct(relevant_comments$Author),
    Percent_Comments_with_Keyword = mean(Percent_Comments_with_Keyword),
    Avg_Comment_Relevancy = mean(Avg_Comment_Relevancy),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy)
  )

dataset2_stats <- bind_rows(dataset2_stats, dataset2_totals)

# Save as HTML or CSV
tab_df(dataset2_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset2_statistics.html"))
```

## Dataset 3 Descriptive Statistics
```{r Dataset 3 Descriptive Statistics}
dataset3_stats <- filtered_relevant_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Related_Comments = n(),
    Related_Threads = n_distinct(Thread.ID),
    Total_Comments_in_Related_Threads = sum(n()),
    Unique_Authors_in_Related_Threads = n_distinct(Author),
    Avg_Comment_Relevancy = mean(Relevancy, na.rm = TRUE),
    Avg_Thread_Relevancy = mean(filtered_relevant_threads$ThreadRelevancyScore[filtered_relevant_threads$newsgroup == first(newsgroup)], na.rm = TRUE)
  )

# Add a totals row
dataset3_totals <- dataset3_stats %>%
  summarize(
    newsgroup = "Total",
    Related_Comments = sum(Related_Comments),
    Related_Threads = sum(Related_Threads),
    Total_Comments_in_Related_Threads = sum(Total_Comments_in_Related_Threads),
    Unique_Authors_in_Related_Threads = n_distinct(filtered_relevant_comments$Author),
    Avg_Comment_Relevancy = mean(Avg_Comment_Relevancy),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy)
  )

dataset3_stats <- bind_rows(dataset3_stats, dataset3_totals)

# Save as HTML or CSV
tab_df(dataset3_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset3_statistics.html"))
```

## Dataset 4 Descriptive Statistics
```{r Dataset 4 Descriptive Statistics}
dataset4_stats <- influential_data$comments %>%
  group_by(newsgroup) %>%
  summarize(
    Related_Comments = n(),
    Related_Threads = n_distinct(Thread.ID),
    Total_Comments_in_Related_Threads = sum(n()),
    Unique_Authors_in_Related_Threads = n_distinct(Author),
    Avg_Comment_Relevancy = mean(Relevancy, na.rm = TRUE),
    Avg_Thread_Relevancy = mean(influential_data$threads$ThreadRelevancyScore[influential_data$threads$newsgroup == first(newsgroup)], na.rm = TRUE)
  )

# Add a totals row
dataset4_totals <- dataset4_stats %>%
  summarize(
    newsgroup = "Total",
    Related_Comments = sum(Related_Comments),
    Related_Threads = sum(Related_Threads),
    Total_Comments_in_Related_Threads = sum(Total_Comments_in_Related_Threads),
    Unique_Authors_in_Related_Threads = n_distinct(influential_data$comments$Author),
    Avg_Comment_Relevancy = mean(Avg_Comment_Relevancy),
    Avg_Thread_Relevancy = mean(Avg_Thread_Relevancy)
  )

dataset4_stats <- bind_rows(dataset4_stats, dataset4_totals)

# Save as HTML or CSV
tab_df(dataset4_stats, file = paste0(output_directory, "/Images and Tables/Tables/dataset4_statistics.html"))
```

## Dataset 4 Author Statistics
```{r Dataset 4 Author Statistics}
# Merge this summary with the original `top_influential_authors` dataframe if needed
influential_authors_metadata <- top_influential_authors %>%
  left_join(top_authors_comments_summary, by = "Author")

# Calculate totals for each numeric column
totals_row <- influential_authors_metadata %>%
  summarize(
    Author = "Total",
    Influence = sum(Influence, na.rm = TRUE),
    Num_Comments = sum(Num_Comments, na.rm = TRUE),
    Num_Threads = sum(Num_Threads, na.rm = TRUE),
    Threads_Started = sum(Threads_Started, na.rm = TRUE),
    Avg_Sentiment = mean(Avg_Sentiment, na.rm = TRUE),  # Average sentiment
    Avg_Relevancy = mean(Avg_Relevancy, na.rm = TRUE)   # Average relevancy
  )

# Bind the totals row to the bottom of the dataframe
influential_authors_metadata_with_totals <- bind_rows(influential_authors_metadata, totals_row)

# Display the updated table with totals
print(influential_authors_metadata_with_totals)
```

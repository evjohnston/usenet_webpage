---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Maintenance for R and Python.
```{r Reset and Setup}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_webpage')
```

```{r R Load Libraries, Directories, and Datasets}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(igraph)
library(ggraph)
library(tidytext)
library(topicmodels)
library(visNetwork)
library(RColorBrewer) 
library(sjPlot)
library(ldatuning)
library(tm)
library(SnowballC)

# Load Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory <- "/Users/emerson/Github/usenet_webpage/CSV Files/Threads"
comments_directory <- "/Users/emerson/Github/usenet_webpage/CSV Files/Comments"

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments <- read.csv(file.path(comments_directory, "aids_related_comments_82TO86.csv"))
```

```{r R CSV Cleaning}
all_threads <- all_threads %>% drop_na()
all_comments <- all_comments %>% drop_na()
all_comments$Date <- as.Date(all_comments$Date, format = "%Y-%m-%d")
all_threads$Date <- as.Date(all_threads$Date, format = "%Y-%m-%d")
```

```{python Python Load Libraries, Directories, and Datasets, eval=FALSE, include=FALSE}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
import scipy

# Define directories
output_directory = "/Users/emerson/Github/usenet_webpage/"
IT_directory = "/Users/emerson/Github/usenet_webpage/Images and Tables/"
threads_directory = "/Users/emerson/Github/usenet_webpage/CSV Files/Threads/"
comments_directory = "/Users/emerson/Github/usenet_webpage/CSV Files/Comments/"
images_dir = os.path.join(IT_directory, "Images")
tables_dir = os.path.join(IT_directory, "Tables")

# Load the datasets with the correct file paths
all_threads = pd.read_csv(os.path.join(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments = pd.read_csv(os.path.join(comments_directory, "aids_related_comments_82TO86.csv"))
```

```{python Relevancy Filtering}
def create_relevancy_quartiles(all_threads):
    # Calculate min and max values
    min_value = all_threads['Thread_Relevancy'].min()
    max_value = all_threads['Thread_Relevancy'].max()
    
    # Create breaks for quartiles
    breaks = np.linspace(min_value, max_value, 5)
    
    # Create quartile labels
    all_threads['Quartile'] = pd.cut(
        all_threads['Thread_Relevancy'],
        bins=breaks,
        labels=['Q1', 'Q2', 'Q3', 'Q4'],
        include_lowest=True
    )
    
    return all_threads

def plot_quartile_frequencies(df, title="Frequency of Thread Relevancy Quartiles"):
    plt.figure(figsize=(10, 6))
    counts = df['Quartile'].value_counts().sort_index()
    
    bars = plt.bar(counts.index, counts.values, color='skyblue', edgecolor='black')
    
    # Add value labels above bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}',
                ha='center', va='bottom')
    
    plt.title(title)
    plt.xlabel("Quartile")
    plt.ylabel("Frequency")
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_relevancy_histogram(df, bin_width=0.5):
    plt.figure(figsize=(10, 6))
    
    # Create histogram
    plt.hist(df['Thread_Relevancy'], bins=np.arange(min(df['Thread_Relevancy']), 
            max(df['Thread_Relevancy']) + bin_width, bin_width),
            color='skyblue', edgecolor='black', alpha=0.7)
    
    # Add density curve
    density = sns.kdeplot(data=df['Thread_Relevancy'], 
                         color='orange', 
                         alpha=0.4,
                         fill=True)
    
    plt.title("Histogram and Density of Thread Relevancy Scores")
    plt.xlabel("Relevancy")
    plt.ylabel("Number of Threads")
    plt.grid(True, alpha=0.3)
    plt.show()

# Load your data (uncomment and modify paths as needed)
# all_threads = pd.read_csv('path_to_your_threads_file.csv')
# all_comments = pd.read_csv('path_to_your_comments_file.csv')

# Ensure column names match exactly
all_threads_renamed = all_threads.rename(columns={
    'Thread.Relevancy': 'Thread_Relevancy'
})

# Create filtered_threads by removing Q1 and requiring more than 1 message
all_threads_renamed = create_relevancy_quartiles(all_threads_renamed)

filtered_threads = all_threads_renamed[
    (all_threads_renamed['Quartile'] != 'Q1') & 
    (all_threads_renamed['Number.of.Messages'] > 1)
].copy()

# Create filtered_comments by keeping only comments from the filtered threads
filtered_comments = all_comments[
    all_comments['Thread.ID'].isin(filtered_threads['Unique_ThreadID'])
].copy()

# Optional: Save the filtered datasets to CSV files
filtered_threads.to_csv('filtered_threads.csv', index=False)
filtered_comments.to_csv('filtered_comments.csv', index=False)

# Display the counts of filtered data
print(f"\nNumber of threads after filtering: {len(filtered_threads)}")
print(f"Number of comments after filtering: {len(filtered_comments)}")

# Display information about the datasets
print("\nFiltered Threads Info:")
print(filtered_threads.info())
print("\nFirst few rows of filtered_threads:")
print(filtered_threads.head())

print("\nFiltered Comments Info:")
print(filtered_comments.info())
print("\nFirst few rows of filtered_comments:")
print(filtered_comments.head())

# Display the distribution of messages in filtered threads
print("\nSummary of Number.of.Messages in filtered threads:")
print(filtered_threads['Number.of.Messages'].describe())
```
## Cool Initial Visualizations
```{python}

```

## Theme Prevelance Hypothesis
```{python LDA Topic Model + Betas, 6 Topics}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx

# Define the Images and Tables directory
IT_directory = "/Users/emerson/Github/usenet_webpage/Images and Tables/"
images_dir = os.path.join(IT_directory, "Images")
tables_dir = os.path.join(IT_directory, "Tables")

# Create directories if they don't exist
os.makedirs(images_dir, exist_ok=True)
os.makedirs(tables_dir, exist_ok=True)

# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

def preprocess_text(text):
    """Preprocess text data"""
    # Convert to lowercase
    text = str(text).lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

def create_styled_html(results_df):
    """Create a styled HTML table from the results DataFrame"""
    # Create a copy of the DataFrame for styling
    styled_df = results_df.copy()
    
    # Define CSS styles
    styles = [
        dict(selector="caption", 
             props=[("caption-side", "top"),
                   ("font-size", "16px"),
                   ("font-weight", "bold"),
                   ("text-align", "center"),
                   ("padding", "10px")]),
        dict(selector="th", 
             props=[("font-size", "14px"),
                   ("text-align", "center"),
                   ("background-color", "#f0f0f0"),
                   ("padding", "8px")]),
        dict(selector="td", 
             props=[("padding", "8px"),
                   ("text-align", "left")]),
        dict(selector="tr:nth-child(even)",
             props=[("background-color", "#f9f9f9")]),
        dict(selector="table",
             props=[("border-collapse", "collapse"),
                   ("width", "100%"),
                   ("margin", "20px 0"),
                   ("font-family", "Arial, sans-serif")]),
        dict(selector="", 
             props=[("border", "1px solid #ddd")])
    ]
    
    # Apply styling
    styled_table = (styled_df.style
                   .set_table_styles(styles)
                   .set_caption("LDA Topic Analysis Results")
                   .format(precision=4)
                   .background_gradient(subset=[col for col in styled_df.columns if 'betas' in col],
                                     cmap='Blues')
                   .hide(axis='index'))
    
    # Add custom CSS for alternating topic columns
    for i in range(1, 7):
        styled_table = styled_table.set_properties(**{
            f'Topic_{i}_terms': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            },
            f'Topic_{i}_betas': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            }
        })
    
    return styled_table

def analyze_topics(filtered_comments, n_topics=6, n_top_words=20):
    """Perform topic analysis on the comments"""
    print("Starting topic analysis...")
    
    # Preprocess texts
    texts = [preprocess_text(text) for text in filtered_comments['Full.Text']]
    
    # Create document-term matrix
    print("Creating document-term matrix...")
    vectorizer = CountVectorizer(max_df=0.95, min_df=2)
    doc_term_matrix = vectorizer.fit_transform(texts)
    
    # Train LDA model
    print("Training LDA model...")
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=123,
        max_iter=20
    )
    lda.fit(doc_term_matrix)
    
    # Get feature names
    feature_names = vectorizer.get_feature_names_out()
    
    # Create top terms dataframe
    print("Extracting top terms...")
    top_terms_dict = {}
    
    for topic_idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words-1:-1]
        top_terms = [feature_names[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        
        top_terms_dict[f'Topic_{topic_idx+1}_terms'] = top_terms
        top_terms_dict[f'Topic_{topic_idx+1}_betas'] = [round(beta, 4) for beta in top_betas]
    
    # Create DataFrame
    results_df = pd.DataFrame(top_terms_dict)
    
    # Save table to Tables directory
    tables_path = os.path.join(tables_dir, "general_topics_lda_analysis.html")
    
    # Create HTML content
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>General LDA Topic Analysis Results</caption>
            {results_df.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(tables_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Create visualization
    print("Creating visualization...")
    plt.figure(figsize=(15, 10))
    for topic_idx in range(n_topics):
        plt.subplot(2, 3, topic_idx + 1)
        top_terms = top_terms_dict[f'Topic_{topic_idx+1}_terms'][:10]
        top_betas = top_terms_dict[f'Topic_{topic_idx+1}_betas'][:10]
        plt.barh(range(len(top_terms)), top_betas)
        plt.yticks(range(len(top_terms)), top_terms)
        plt.title(f'Topic {topic_idx + 1}')
    
    plt.tight_layout()
    
    # Save plot to Images directory
    plot_path = os.path.join(images_dir, "general_topics_visualization.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print("Analysis complete! Results saved to:")
    print(f"- Table: {tables_path}")
    print(f"- Plot: {plot_path}")
    
    return results_df, lda, vectorizer, doc_term_matrix

# Run the analysis
try:
    print("\nStarting topic analysis...")
    results_df, lda_model, vectorizer, doc_term_matrix = analyze_topics(filtered_comments)
    print("\nAnalysis completed successfully!")
    
    # Display top terms for each topic
    print("\nTop terms for each topic:")
    print(results_df.head())
    
except Exception as e:
    print(f"An error occurred: {str(e)}")
    raise
```

```{python co-occurance network}
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Polygon
from scipy.spatial import ConvexHull

def create_static_network(doc_term_matrix, lda_model, vectorizer, output_path='topic_network_visualization.png'):
    """Create static co-occurrence network visualization with topic regions"""
    print("Creating static co-occurrence network...")
    
    # Get the document-term matrix as array
    dtm_array = doc_term_matrix.toarray()
    co_occurrence = np.dot(dtm_array.T, dtm_array)
    
    # Create binary DTM
    binary_dtm = (dtm_array > 0).astype(int)
    term_frequency = np.sum(binary_dtm, axis=0)
    
    # Filter for terms that appear in the top terms from LDA results
    top_terms = []
    for i in range(6):
        col_name = f'Topic_{i+1}_terms'
        top_terms.extend(results_df[col_name].head(15).tolist())  # Limit to top 15 terms per topic
    top_terms = list(set(top_terms))
    
    # Get indices of these terms
    feature_names = vectorizer.get_feature_names_out()
    filtered_terms = [i for i, term in enumerate(feature_names) if term in top_terms]
    filtered_co_occurrence = co_occurrence[filtered_terms][:, filtered_terms]
    terms = feature_names[filtered_terms]
    
    # Create networkx graph
    G = nx.Graph()
    
    # Add edges with weights
    for i in range(len(terms)):
        for j in range(i + 1, len(terms)):
            if filtered_co_occurrence[i, j] > 0:
                G.add_edge(terms[i], terms[j], weight=filtered_co_occurrence[i, j])
    
    # Get topic assignments for terms
    term_topic_assignment = []
    for term in terms:
        term_topics = []
        for i in range(6):
            terms_col = f'Topic_{i+1}_terms'
            betas_col = f'Topic_{i+1}_betas'
            if term in results_df[terms_col].values:
                idx = results_df[terms_col][results_df[terms_col] == term].index[0]
                term_topics.append((i, results_df[betas_col].iloc[idx]))
        if term_topics:
            term_topic_assignment.append(max(term_topics, key=lambda x: x[1])[0])
        else:
            term_topic_assignment.append(0)
    
    # Set up the plot with white background
    plt.figure(figsize=(20, 20), facecolor='white')
    
    # Create layout with more spacing
    pos = nx.spring_layout(G, k=4, iterations=100, seed=42)
    
    # Convert pos dict to numpy arrays for each topic
    topic_positions = {i: [] for i in range(6)}
    for node, position in pos.items():
        idx = list(terms).index(node)
        topic = term_topic_assignment[idx]
        topic_positions[topic].append(position)
    
    # Draw topic regions
    colors = sns.color_palette("Set2", n_colors=6)
    alpha_fill = 0.2
    alpha_edge = 0.5
    
    # Draw convex hulls for each topic
    for topic in range(6):
        if len(topic_positions[topic]) > 2:  # Need at least 3 points for convex hull
            points = np.array(topic_positions[topic])
            hull = ConvexHull(points)
            hull_points = points[hull.vertices]
            plt.fill(hull_points[:, 0], hull_points[:, 1], 
                    alpha=alpha_fill, color=colors[topic])
            plt.plot(hull_points[:, 0], hull_points[:, 1], 
                    color=colors[topic], alpha=alpha_edge)
    
    # Draw edges
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    max_edge_weight = max(edge_weights) if edge_weights else 1
    edge_widths = [0.3 + (w / max_edge_weight) for w in edge_weights]
    nx.draw_networkx_edges(G, pos, alpha=0.1, width=edge_widths, edge_color='gray')
    
    # Draw nodes
    node_sizes = [np.log1p(term_frequency[filtered_terms][i]) * 500 for i in range(len(terms))]
    for topic in range(6):
        # Get nodes for this topic
        topic_nodes = [node for node, idx in enumerate(term_topic_assignment) if idx == topic]
        if topic_nodes:
            nx.draw_networkx_nodes(G, pos, 
                                 nodelist=[terms[i] for i in topic_nodes],
                                 node_color=[colors[topic]],
                                 node_size=[node_sizes[i] for i in topic_nodes],
                                 alpha=0.7)
    
    # Add labels with better spacing and formatting
    labels = {node: node for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels,
                          font_size=10,
                          font_weight='bold',
                          bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.5))
    
    # Add legend
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                label=f'Topic {i+1}',
                                markerfacecolor=colors[i], markersize=15)
                      for i in range(6)]
    plt.legend(handles=legend_elements, loc='upper right',
              title='Topics', title_fontsize=12, fontsize=10)
    
    # Remove axes
    plt.axis('on')
    
    # Save with high quality
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return G

# Create the visualization using existing data
try:
    print("\nCreating static network visualization...")
    G = create_static_network(
        doc_term_matrix=doc_term_matrix,
        lda_model=lda_model,
        vectorizer=vectorizer,
        output_path='topic_network_visualization.png'
    )
    
    print("\nNetwork visualization completed successfully!")
    print("Check 'topic_network_visualization.png' for the static network visualization.")
    
except Exception as e:
    print(f"An error occurred while creating the network: {str(e)}")
```

## Emotional Tone Hypothesis
```{python}
usenet_threads_all = pd.read_csv(os.path.join(threads_directory, "combined_threads_cleaned_82TO86.csv"))
usenet_comments_all = pd.read_csv(os.path.join(comments_directory, "combined_comments_cleaned_82TO86.csv"))
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
from itertools import combinations

def create_author_network(filtered_comments):
    """Create a network of authors based on thread co-participation"""
    print("Creating author network...")
    
    edges = []
    weights = {}
    
    # Group comments by Thread ID
    thread_groups = filtered_comments.groupby('Thread.ID')
    
    # Create edges for each thread
    for thread_id, group in thread_groups:
        if thread_id and len(group) > 1:
            authors = group['Author'].unique()
            
            for source, target in combinations(authors, 2):
                if source != target:
                    edge = tuple(sorted([source, target]))
                    
                    if edge in weights:
                        weights[edge] += 1
                    else:
                        weights[edge] = 1
                        edges.append({'source': edge[0], 'target': edge[1]})
    
    edge_df = pd.DataFrame(edges)
    if not edges:  # Check if edges list is empty
        print("Warning: No edges created. Check if there are multiple authors per thread.")
        return pd.DataFrame(columns=['source', 'target', 'weight'])
        
    edge_df['weight'] = edge_df.apply(lambda row: weights[tuple(sorted([row['source'], row['target']]))], axis=1)
    
    print(f"Created network with {len(edge_df)} edges")
    return edge_df

def identify_influential_authors(edge_df, n_authors=15):
    """Identify the most influential authors based on network metrics"""
    print("Identifying influential authors...")
    
    if edge_df.empty:
        print("Warning: Empty edge dataframe. Cannot identify influential authors.")
        return pd.DataFrame(columns=['Author', 'Influence'])
        
    G = nx.from_pandas_edgelist(edge_df, 'source', 'target', 'weight')
    degree_dict = dict(G.degree(weight='weight'))
    
    author_influence = pd.DataFrame({
        'Author': list(degree_dict.keys()),
        'Influence': list(degree_dict.values())
    })
    
    top_influential_authors = author_influence.sort_values(
        'Influence', ascending=False
    ).head(n_authors).reset_index(drop=True)
    
    print("\nTop influential authors identified:")
    print(top_influential_authors)
    return top_influential_authors

def create_sentiment_visualization():
    """Create time series visualization of sentiment scores using existing dataframes"""
    print("Creating sentiment visualization...")
    
    # Create a figure
    plt.figure(figsize=(15, 8))
    
    # Calculate monthly averages for each dataset
    def calculate_monthly_sentiment(df):
        df['Date'] = pd.to_datetime(df['Date'])
        return df.groupby(pd.Grouper(key='Date', freq='M'))['SentimentScore'].mean().reset_index()
    
    # Get monthly averages for each dataset
    monthly_sentiment_usenet = calculate_monthly_sentiment(usenet_comments_all)
    monthly_sentiment_aids = calculate_monthly_sentiment(filtered_comments)
    
    # Get influential authors
    edge_df = create_author_network(filtered_comments)
    top_authors = identify_influential_authors(edge_df)
    
    # Calculate sentiment for influential authors
    influential_comments = filtered_comments[filtered_comments['Author'].isin(top_authors['Author'])]
    monthly_sentiment_influential = calculate_monthly_sentiment(influential_comments)
    
    # Plot the lines
    plt.plot(monthly_sentiment_usenet['Date'], 
             monthly_sentiment_usenet['SentimentScore'],
             label='Dataset One', linewidth=2, color='#1f77b4')
    plt.plot(monthly_sentiment_aids['Date'],
             monthly_sentiment_aids['SentimentScore'],
             label='Dataset Three', linewidth=2, color='#ff7f0e')
    plt.plot(monthly_sentiment_influential['Date'],
             monthly_sentiment_influential['SentimentScore'],
             label='Influential Authors', linewidth=2, color='#2ca02c')
    
    # Add key events annotations
    key_events = {
        '1982-05-11': "Term 'AIDS' Introduced",
        '1983-09-30': "CDC AIDS Guidelines",
        '1984-04-23': "HHS HIV/AIDS",
        '1984-10-01': "First HIV Blood Test",
        '1985-03-02': "First HIV Test Approved",
        '1985-07-25': "Rock Hudson's Diagnosis",
        '1985-10-02': "HIV Transmission Routes",
        '1986-02-01': "'HIV' Renamed",
        '1986-08-14': "AZT Approved"
    }
    
    # Add event annotations
    y_min = plt.ylim()[0]
    for date, event in key_events.items():
        date_obj = pd.to_datetime(date)
        plt.axvline(x=date_obj, color='gray', linestyle='--', alpha=0.5)
        plt.text(date_obj, y_min, event,
                rotation=90, verticalalignment='bottom',
                horizontalalignment='right', fontsize=8)
    
    # Customize plot
    plt.title('Time Series of Average Sentiment Scores Over Time', fontsize=14, pad=20)
    plt.xlabel('Month', fontsize=12)
    plt.ylabel('Average Sentiment Score', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),
              ncol=3, frameon=False)
    
    plt.gca().set_facecolor('white')
    plt.gcf().set_facecolor('white')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Save the plot
    output_path = os.path.join(output_directory, "Images and Tables/Images/sentiment_time_series.png")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"Visualization saved to: {output_path}")
    return monthly_sentiment_usenet, monthly_sentiment_aids, monthly_sentiment_influential

# Run the analysis
try:
    print("\nStarting sentiment analysis...")
    results = create_sentiment_visualization()
    print("\nAnalysis completed successfully!")
    
    # Print summary statistics
    print("\nSentiment Statistics:")
    for name, data in zip(['Dataset One', 'Dataset Three', 'Influential Authors'], results):
        print(f"\n{name}:")
        print(f"Average sentiment: {data['SentimentScore'].mean():.3f}")
        print(f"Number of months: {len(data)}")
        
except Exception as e:
    print(f"An error occurred: {str(e)}")
```

## Author Impact Hypothesis
```{python}
# Define the Images and Tables directory
IT_directory = "/Users/emerson/Github/usenet_webpage/Images and Tables/"
images_dir = os.path.join(IT_directory, "Images")
tables_dir = os.path.join(IT_directory, "Tables")

# Create directories if they don't exist
os.makedirs(images_dir, exist_ok=True)
os.makedirs(tables_dir, exist_ok=True)

# First create the author network and get influential authors
edge_df = create_author_network(filtered_comments)
top_influential_authors = identify_influential_authors(edge_df)

# Create top_authors_comments
top_authors_comments = filtered_comments[
    filtered_comments['Author'].isin(top_influential_authors['Author'])
]

def analyze_influential_topics(top_authors_comments):
    """Perform LDA analysis for influential authors' comments"""
    print("\nStarting influential authors' topic analysis...")
    
    # Preprocess texts using existing preprocess_text function
    texts = [preprocess_text(text) for text in top_authors_comments['Full.Text']]
    
    # Create document-term matrix
    print("Creating document-term matrix for influential authors...")
    influential_vectorizer = CountVectorizer(max_df=0.95, min_df=2)
    influential_dtm = influential_vectorizer.fit_transform(texts)
    
    # Train LDA model
    print("Training LDA model for influential authors...")
    influential_lda = LatentDirichletAllocation(
        n_components=6,
        random_state=123,
        max_iter=20
    )
    influential_lda.fit(influential_dtm)
    
    # Get feature names
    influential_features = influential_vectorizer.get_feature_names_out()
    
    # Create top terms dictionary
    influential_terms_dict = {}
    
    for topic_idx, topic in enumerate(influential_lda.components_):
        top_indices = topic.argsort()[:-20-1:-1]
        top_terms = [influential_features[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        
        influential_terms_dict[f'Topic_{topic_idx+1}_terms'] = top_terms
        influential_terms_dict[f'Topic_{topic_idx+1}_betas'] = [round(beta, 4) for beta in top_betas]
    
    # Create DataFrame
    influential_results_df = pd.DataFrame(influential_terms_dict)
    
    # Save results to Tables directory only
    tables_path = os.path.join(tables_dir, "influential_topics_lda_analysis.html")
    
    # Create a clean HTML table with basic styling
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>LDA Topic Analysis Results for Influential Authors</caption>
            {influential_results_df.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(tables_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"Influential authors' topic analysis saved to: {tables_path}")
    
    return influential_results_df, influential_lda, influential_vectorizer, influential_dtm

def compare_topics(main_results_df, influential_results_df):
    """Compare topics between main analysis and influential authors"""
    print("\nComparing topics between main and influential authors...")
    
    similarities = []
    
    def calculate_jaccard_similarity(set1, set2):
        """Calculate Jaccard similarity between two sets of terms"""
        intersection = len(set(set1) & set(set2))
        union = len(set(set1) | set(set2))
        return intersection / union if union > 0 else 0
    
    # Calculate similarities between all topic pairs
    for i in range(1, 7):
        main_terms = main_results_df[f'Topic_{i}_terms'].tolist()
        for j in range(1, 7):
            infl_terms = influential_results_df[f'Topic_{j}_terms'].tolist()
            similarity = calculate_jaccard_similarity(main_terms, infl_terms)
            similarities.append({
                'Main_Topic': i,
                'Influential_Topic': j,
                'Similarity': similarity
            })
    
    similarities_df = pd.DataFrame(similarities)
    
    # Find best matches
    best_matches = similarities_df.sort_values('Similarity', ascending=False).groupby(
        'Influential_Topic').first().reset_index()
    
    # Create comparison visualization
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=best_matches,
        x='Influential_Topic',
        y='Similarity',
        hue='Main_Topic',
        palette='Set2'
    )
    
    # Add value labels on bars
    for i, row in best_matches.iterrows():
        plt.text(
            i, row['Similarity']/2,
            f"{row['Similarity']:.2f}",
            ha='center', va='center'
        )
    
    plt.title("Topic Similarity Between Main and Influential Authors")
    plt.xlabel("Influential Authors' Topic")
    plt.ylabel("Jaccard Similarity")
    plt.legend(title="Matching Main Topic")
    
    # Save comparison plot to Images directory
    comparison_path = os.path.join(images_dir, "topic_similarity_comparison.png")
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Save comparison results to Tables directory
    best_matches_path = os.path.join(tables_dir, "topic_matches.html")
    
    # Create HTML content for best matches
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>Topic Matching Results</caption>
            {best_matches.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(best_matches_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    return best_matches

# Run the analysis
try:
    print("\nNumber of comments from influential authors:", len(top_authors_comments))
    print("Number of unique authors:", len(top_authors_comments['Author'].unique()))
    
    # Run LDA for influential authors
    influential_results_df, influential_lda, influential_vectorizer, influential_dtm = analyze_influential_topics(top_authors_comments)
    
    # Compare with main results
    comparison_results = compare_topics(results_df, influential_results_df)
    
    # Print summary statistics
    print("\nTopic Comparison Summary:")
    print(f"Average similarity: {comparison_results['Similarity'].mean():.3f}")
    print(f"Maximum similarity: {comparison_results['Similarity'].max():.3f}")
    print(f"Minimum similarity: {comparison_results['Similarity'].min():.3f}")
    
except Exception as e:
    print(f"An error occurred: {str(e)}")
    raise
```


```{python}

```


```{python}

```


```{python}

```


```{python}

```
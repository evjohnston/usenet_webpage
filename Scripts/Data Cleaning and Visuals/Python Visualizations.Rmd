---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{python Python Load Libraries, Directories, and Datasets}
import gc
import matplotlib.pyplot as plt
import networkx as nx
import nltk
import numpy as np
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import re
import scipy
import seaborn as sns
from collections import Counter
from datetime import datetime
from itertools import combinations
from matplotlib.patches import Polygon
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy.spatial import ConvexHull
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory = os.path.join(output_directory, "CSV Files/Threads")
comments_directory = os.path.join(output_directory, "CSV Files/Comments")
images_dir = os.path.join(output_directory, "Images and Tables/Images")
tables_dir = os.path.join(output_directory, "Images and Tables/Tables")

# Load cleaned datasets
dataset1_threads = pd.read_csv(os.path.join(threads_directory, "dataset1_threads.csv"))
dataset1_comments = pd.read_csv(os.path.join(comments_directory, "dataset1_comments.csv"))
dataset2_threads = pd.read_csv(os.path.join(threads_directory, "dataset2_threads.csv"))
dataset2_comments = pd.read_csv(os.path.join(comments_directory, "dataset2_comments.csv"))
dataset3_threads = pd.read_csv(os.path.join(threads_directory, "dataset3_threads.csv"))
dataset3_comments_all = pd.read_csv(os.path.join(comments_directory, "dataset3_comments_all.csv"))
dataset3_comments_onlyinfluential = pd.read_csv(os.path.join(comments_directory, "dataset3_comments_onlyinfluential.csv"))
influential_authors = pd.read_csv(os.path.join(output_directory, "CSV Files/influential_authors.csv"))
```

# Initial Visualizations
## Cross-Dataset Content Focus Analysis
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import matplotlib.dates as mdates
from matplotlib.gridspec import GridSpec

# Define topic categories
topic_categories = {
    'medical': [
        'virus', 'immune', 'infection', 'symptom', 'treatment', 'disease',
        'patient', 'doctor', 'hospital', 'drug', 'clinical', 'diagnosis',
        'vaccine', 'transmission', 'blood', 'test', 'positive', 'negative',
        'medication', 'therapy', 'cure', 'health', 'medical', 'medicine',
        'syndrome', 'cell', 'viral', 'antibody', 'immune system', 'insurance'
    ],
    'social': [
        'community', 'support', 'family', 'friend', 'relationship', 'stigma',
        'discrimination', 'education', 'awareness', 'fear', 'anxiety', 'help',
        'care', 'society', 'public', 'impact', 'life', 'work', 'social',
        'personal', 'experience', 'gay', 'lesbian', 'sexuality', 'identity',
        'support group', 'counseling', 'lifestyle', 'isolation'
    ],
    'political': [
        'policy', 'government', 'funding', 'law', 'legislation', 'right',
        'activist', 'advocacy', 'campaign', 'research', 'organization',
        'regulation', 'public health', 'cost', 'access', 'insurance',
        'initiative', 'program', 'reform', 'budget', 'protest', 'rights', 'reagan'
    ],
    'scientific': [
        'research', 'study', 'data', 'evidence', 'trial', 'experiment',
        'laboratory', 'clinical', 'result', 'hypothesis', 'analysis',
        'scientific', 'researcher', 'publication', 'paper', 'journal',
        'methodology', 'statistical'
    ],
    'emotional': [
        'fear', 'hope', 'worry', 'anxiety', 'depression', 'anger',
        'frustration', 'support', 'care', 'love', 'concern', 'stress',
        'courage', 'strength', 'emotional', 'feeling', 'scared', 'afraid',
        'hopeful', 'grateful'
    ]
}

def calculate_topic_scores(text):
    """Calculate topic scores for a single text"""
    if pd.isna(text):
        return {cat: 0.0 for cat in topic_categories.keys()}
    
    text = str(text).lower()
    scores = {}
    
    for category, keywords in topic_categories.items():
        score = sum(1 for keyword in keywords if keyword in text)
        scores[category] = float(score)
    
    total = sum(scores.values())
    if total > 0:
        scores = {k: v/total for k, v in scores.items()}
    
    return scores

def analyze_dataset(df, name):
    """Analyze content for a dataset"""
    print(f"Analyzing {name}...")
    
    # Calculate topic scores for each comment
    topic_scores = []
    for _, row in df.iterrows():
        scores = calculate_topic_scores(row['Full.Text'])
        scores['Date'] = pd.to_datetime(row['Date'])
        scores['Dataset'] = name
        scores['Newsgroup'] = row['newsgroup']
        topic_scores.append(scores)
    
    return pd.DataFrame(topic_scores)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import matplotlib.dates as mdates
from matplotlib.gridspec import GridSpec

def create_enhanced_visualizations(results_dict, output_dir):
    """Create detailed visualizations"""
    print("Creating enhanced visualizations...")
    
    # Set style parameters
    sns.set_style("whitegrid")
    sns.set_context("talk")
    colors = sns.color_palette("Set2", len(topic_categories))
    
    # 1. Enhanced Overall Distribution Plot
    fig = plt.figure(figsize=(15, 10))
    gs = GridSpec(2, 1, height_ratios=[2, 1])
    
    # Bar plot
    ax1 = fig.add_subplot(gs[0])
    topic_means = {name: df[list(topic_categories.keys())].mean() 
                  for name, df in results_dict.items()}
    topic_comparison = pd.DataFrame(topic_means)  # Notice the change here
    
    # Set up bar positions
    n_topics = len(topic_categories)
    n_datasets = len(results_dict)
    bar_width = 0.8 / n_datasets
    positions = np.arange(n_topics)
    
    # Plot bars for each dataset
    for i, (dataset_name, values) in enumerate(topic_means.items()):
        x_pos = positions + (i * bar_width)
        rects = ax1.bar(x_pos, 
                       values,
                       bar_width,
                       label=dataset_name,
                       alpha=0.8)
        # Add value labels on top of bars
        for rect in rects:
            height = rect.get_height()
            ax1.text(rect.get_x() + rect.get_width()/2., height,
                    f'{height:.2f}',
                    ha='center', va='bottom',
                    rotation=90)
    
    # Customize first subplot
    ax1.set_ylabel('Proportion', fontsize=12)
    ax1.set_title('Topic Distribution Comparison Across Datasets', fontsize=14, pad=20)
    ax1.set_xticks(positions + bar_width * (n_datasets-1)/2)
    ax1.set_xticklabels(topic_categories.keys(), rotation=45, ha='right')
    ax1.legend(title='Datasets', bbox_to_anchor=(1.05, 1))
    
    # Add relative differences
    ax2 = fig.add_subplot(gs[1])
    baseline = topic_comparison.iloc[:,0]  # First dataset as baseline
    
    for i in range(1, topic_comparison.shape[1]):
        relative_diff = (topic_comparison.iloc[:,i] - baseline) / baseline * 100
        ax2.plot(range(len(topic_categories)), relative_diff,
                marker='o', label=f'{topic_comparison.columns[i]} vs {topic_comparison.columns[0]}')
    
    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    ax2.set_xticks(range(len(topic_categories)))
    ax2.set_xticklabels(topic_categories.keys(), rotation=45, ha='right')
    ax2.set_ylabel('% Difference from General Usenet')
    ax2.legend(bbox_to_anchor=(1.05, 1))
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/enhanced_topic_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Enhanced Temporal Evolution (Three Separate Panels)
    fig, axes = plt.subplots(3, 1, figsize=(15, 15))
    fig.suptitle('Topic Evolution Over Time (Pre-1987)', fontsize=16, y=1.00)
    
    key_events = {
        '1982-05-11': "Term 'AIDS' Introduced",
        '1983-09-30': "CDC Guidelines",
        '1984-04-23': "HIV Identified",
        '1985-03-02': "First Test Approved",
        '1985-07-25': "Rock Hudson",
        '1986-02-01': "'HIV' Named"
    }
    
    for idx, (dataset_name, df) in enumerate(results_dict.items()):
        # Filter for pre-1987 data
        df = df[df['Date'] < '1987-01-01'].copy()
        
        # Calculate monthly averages
        df_monthly = df.groupby(pd.Grouper(key='Date', freq='M'))[list(topic_categories.keys())].mean()
        
        # Plot each topic
        for topic_idx, topic in enumerate(topic_categories.keys()):
            if topic in df_monthly.columns:  # Check if topic exists in the data
                axes[idx].plot(df_monthly.index, df_monthly[topic], 
                             label=topic, color=colors[topic_idx], 
                             linewidth=2, marker='o', markersize=4)
        
        # Customize each subplot
        axes[idx].set_title(f'{dataset_name}', fontsize=12)
        axes[idx].set_ylabel('Topic Proportion')
        axes[idx].grid(True, alpha=0.3)
        axes[idx].xaxis.set_major_locator(mdates.MonthLocator(interval=3))
        axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        
        # Add key events
        for date, event in key_events.items():
            event_date = pd.to_datetime(date)
            if event_date in df_monthly.index:
                axes[idx].axvline(x=event_date, color='gray', 
                                linestyle='--', alpha=0.3)
                axes[idx].text(event_date, axes[idx].get_ylim()[1], 
                             event, rotation=90, va='top', fontsize=8)
    
    # Add legend to the bottom
    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='center right', 
              bbox_to_anchor=(0.98, 0.5), title='Topics')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/enhanced_temporal_evolution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Enhanced Newsgroup Comparison
    fig = plt.figure(figsize=(20, 8))
    gs = GridSpec(1, 3)
    
    for idx, (dataset_name, df) in enumerate(results_dict.items()):
        ax = fig.add_subplot(gs[idx])
        
        # Ensure we have data to plot
        if len(df) > 0:
            pivot_data = df.groupby('Newsgroup')[list(topic_categories.keys())].mean()
            
            # Create heatmap with custom colormap
            cmap = sns.color_palette("YlOrRd", as_cmap=True)
            sns.heatmap(pivot_data, annot=True, fmt='.2f', 
                       cmap=cmap, ax=ax, cbar=True)
            
            ax.set_title(f'{dataset_name}', fontsize=12)
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        else:
            ax.text(0.5, 0.5, 'No data available',
                   ha='center', va='center')
            ax.set_title(f'{dataset_name}', fontsize=12)
    
    plt.suptitle('Topic Distribution by Newsgroup', fontsize=14, y=1.05)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/enhanced_newsgroup_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. Topic Correlation Heatmaps
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    fig.suptitle('Topic Correlations Across Datasets', fontsize=14)
    
    # Create custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)
    
    for idx, (dataset_name, df) in enumerate(results_dict.items()):
        if len(df) > 0:
            topic_corr = df[list(topic_categories.keys())].corr()
            sns.heatmap(topic_corr, annot=True, fmt='.2f', 
                       cmap=cmap, vmin=-1, vmax=1, 
                       center=0, ax=axes[idx])
        axes[idx].set_title(dataset_name)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/topic_correlations.png', dpi=300, bbox_inches='tight')
    plt.close()

def print_summary_statistics(results_dict):
    """Print detailed summary statistics for each dataset"""
    print("\nDetailed Summary Statistics:")
    
    for name, df in results_dict.items():
        print(f"\n{name} Analysis:")
        print("-" * 50)
        
        # Basic statistics
        print(f"Total posts: {len(df)}")
        print(f"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
        print(f"Number of unique newsgroups: {df['Newsgroup'].nunique()}")
        
        # Topic statistics
        print("\nTopic Distributions:")
        topic_means = df[list(topic_categories.keys())].mean()
        topic_stds = df[list(topic_categories.keys())].std()
        
        for topic in topic_categories.keys():
            print(f"{topic}:")
            print(f"  Mean: {topic_means[topic]:.3f}")
            print(f"  Std:  {topic_stds[topic]:.3f}")
        
        # Most common topic combinations
        print("\nMost prevalent topic pairs:")
        topic_corr = df[list(topic_categories.keys())].corr()
        np.fill_diagonal(topic_corr.values, 0)
        top_pairs = np.unravel_index(np.argsort(topic_corr.values, axis=None)[-3:], topic_corr.shape)
        for i, j in zip(top_pairs[0], top_pairs[1]):
            if i < j:
                print(f"{topic_corr.index[i]} - {topic_corr.columns[j]}: {topic_corr.iloc[i, j]:.3f}")

def main():
    """Main analysis function"""
    # Analyze each dataset
    results = {
        'General Usenet': analyze_dataset(dataset1_comments, "General Usenet"),
        'AIDS Discussions': analyze_dataset(dataset2_comments, "AIDS Discussions"),
        'Influential Authors': analyze_dataset(dataset3_comments_onlyinfluential, "Influential Authors")
    }
    
    # Create visualizations
    create_enhanced_visualizations(results, "/Users/emerson/Github/usenet_webpage/Images and Tables/Images")
    
    # Print detailed statistics
    print_summary_statistics(results)
    
    return results

# Run the analysis
if __name__ == "__main__":
    results = main()
```


## Distribution of influential authors' comments across newsgroups
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_influential_distribution(dataset1_comments, influential_authors):
    """Analyze how influential authors participated across different newsgroups"""
    
    # Filter for influential authors in the general dataset
    influential_comments = dataset1_comments[dataset1_comments['Author'].isin(influential_authors['Author'])]
    
    # Calculate distributions
    author_newsgroup_counts = influential_comments.groupby(['Author', 'newsgroup']).size().reset_index(name='comment_count')
    
    # Create pivot table for heatmap
    pivot_data = author_newsgroup_counts.pivot(index='Author', columns='newsgroup', values='comment_count')
    pivot_data = pivot_data.fillna(0)  # Replace NaN with 0 for cases where author didn't post in a newsgroup
    
    # Calculate percentage distribution for each author
    pivot_data_pct = pivot_data.div(pivot_data.sum(axis=1), axis=0) * 100
    
    # Sort authors by total number of comments
    total_comments = pivot_data.sum(axis=1)
    pivot_data_pct = pivot_data_pct.loc[total_comments.sort_values(ascending=False).index]
    
    # Create visualization
    plt.figure(figsize=(20, 20))
    
    # Create heatmap
    sns.heatmap(pivot_data_pct, 
                annot=True, 
                fmt='.1f',
                cmap='YlOrRd',
                cbar_kws={'label': 'Percentage of Author\'s Comments (%)'},
                vmin=0,
                vmax=100)
    
    plt.title('Distribution of Influential Authors\' Comments Across Newsgroups\n(General Usenet Dataset)',
              pad=20, fontsize=14)
    plt.ylabel('Author (Sorted by Total Comments)')
    plt.xlabel('Newsgroup')
    
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig('/Users/emerson/Github/usenet_webpage/Images and Tables/Images/influential_distribution.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create summary statistics
    summary_stats = {
        'Total Authors': len(pivot_data),
        'Most Active Newsgroup': pivot_data.sum().idxmax(),
        'Least Active Newsgroup': pivot_data.sum().idxmin(),
        'Most Prolific Author': total_comments.idxmax(),
        'Average Comments per Author': total_comments.mean(),
        'Median Comments per Author': total_comments.median(),
        'Authors with Cross-posting': (pivot_data > 0).sum(axis=1).value_counts()
    }
    
    return pivot_data, pivot_data_pct, summary_stats

# Run the analysis
pivot_data, pivot_data_pct, summary_stats = analyze_influential_distribution(dataset1_comments, influential_authors)

# Print summary statistics
print("\nSummary Statistics:")
print("-" * 50)
print(f"Total Influential Authors: {summary_stats['Total Authors']}")
print(f"Most Active Newsgroup: {summary_stats['Most Active Newsgroup']}")
print(f"Least Active Newsgroup: {summary_stats['Least Active Newsgroup']}")
print(f"Most Prolific Author: {summary_stats['Most Prolific Author']}")
print(f"Average Comments per Author: {summary_stats['Average Comments per Author']:.1f}")
print(f"Median Comments per Author: {summary_stats['Median Comments per Author']:.1f}")
print("\nNumber of Authors by Newsgroups Posted In:")
print(summary_stats['Authors with Cross-posting'].sort_index().to_string())
```

# Author Impact
## Topic Modeling
```{python Datasets 2 and 3 Topic Models}
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import numpy as np
import pandas as pd
import re

# Preprocessing function
def preprocess_text(text):
    stop_words = set(stopwords.words("english"))
    lemmatizer = WordNetLemmatizer()
    text = re.sub(r"\W+", " ", text)  # Remove non-alphanumeric characters
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]
    return " ".join(tokens)

# Preprocess datasets
dataset2_text = dataset2_comments["Full.Text"].dropna().astype(str).apply(preprocess_text)
dataset3_text = dataset3_comments_all["Full.Text"].dropna().astype(str).apply(preprocess_text)

# Vectorize text
def vectorize_text(data):
    vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words="english")
    doc_term_matrix = vectorizer.fit_transform(data)
    return doc_term_matrix, vectorizer

# Determine optimal number of topics
def find_optimal_topics(doc_term_matrix, topic_range=(1, 10)):
    coherence_values = []
    for n_topics in range(topic_range[0], topic_range[1] + 1):
        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
        lda.fit(doc_term_matrix)
        coherence_values.append(lda.perplexity(doc_term_matrix))
    optimal_n_topics = coherence_values.index(min(coherence_values)) + topic_range[0]
    return optimal_n_topics, coherence_values

# Extract topics and top words
def extract_topics(lda_model, vectorizer, n_top_words=20):
    topics = []
    for topic_idx, topic in enumerate(lda_model.components_):
        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        topics.append((f"Topic {topic_idx + 1}", top_words))
    return topics

# Process Dataset 2
dataset2_matrix, dataset2_vectorizer = vectorize_text(dataset2_text)
optimal_topics_2, coherence_2 = find_optimal_topics(dataset2_matrix)
lda_dataset2 = LatentDirichletAllocation(n_components=optimal_topics_2, random_state=42)
lda_dataset2.fit(dataset2_matrix)
topics_2 = extract_topics(lda_dataset2, dataset2_vectorizer)

# Process Dataset 3
dataset3_matrix, dataset3_vectorizer = vectorize_text(dataset3_text)
optimal_topics_3, coherence_3 = find_optimal_topics(dataset3_matrix)
lda_dataset3 = LatentDirichletAllocation(n_components=optimal_topics_3, random_state=42)
lda_dataset3.fit(dataset3_matrix)
topics_3 = extract_topics(lda_dataset3, dataset3_vectorizer)

# Output results
print(f"Optimal number of topics for Dataset 2: {optimal_topics_2}")
print(f"\nOptimal number of topics for Dataset 3: {optimal_topics_3}")

def save_topics_to_html(lda_model, vectorizer, output_path, n_top_words=20):
    topics_data = []
    for topic_idx, topic in enumerate(lda_model.components_):
        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        top_betas = [topic[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        topic_dict = {
            f"Topic_{topic_idx + 1}_terms": top_words,
            f"Topic_{topic_idx + 1}_betas": top_betas
        }
        topics_data.append(pd.DataFrame(topic_dict))
    
    # Combine all topics into a single DataFrame
    topics_df = pd.concat(topics_data, axis=1)
    topics_df.to_html(output_path, index=False)
    print(f"Topics saved to: {output_path}")

# Save topics for Dataset 2 (All Comments)
save_topics_to_html(
    lda_model=lda_dataset2,
    vectorizer=dataset2_vectorizer,
    output_path="/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_all_comments.html",
    n_top_words=20
)

# Save topics for Dataset 3 (Influential Authors)
save_topics_to_html(
    lda_model=lda_dataset3,
    vectorizer=dataset3_vectorizer,
    output_path="/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_influential_authors.html",
    n_top_words=20
)
```
## Topic Similarity
```{python Topic Similarity}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import os
from itertools import combinations

def compare_topics_and_save(general_results_path, influential_results_path, output_dir):
    """
    Compare topics between general and influential author analyses 
    using Jaccard similarity and create visualizations.
    """
    print("\nComparing topics with Jaccard similarity...")

    # Load general and influential results from HTML
    general_results = pd.read_html(general_results_path)[0]
    influential_results = pd.read_html(influential_results_path)[0]

    # List to store topic similarity data
    topic_similarities = []

    # Calculate Jaccard similarity for all topic pairs
    for general_topic in range(1, len(general_results.columns) // 2 + 1):
        general_terms = set(general_results[f"Topic_{general_topic}_terms"].dropna())
        for influential_topic in range(1, len(influential_results.columns) // 2 + 1):
            influential_terms = set(influential_results[f"Topic_{influential_topic}_terms"].dropna())
            similarity = len(general_terms.intersection(influential_terms)) / len(general_terms.union(influential_terms))
            topic_similarities.append({
                'General_Topic': general_topic,
                'Influential_Topic': influential_topic,
                'Similarity': similarity
            })

    # Create similarity DataFrame
    comparison_df = pd.DataFrame(topic_similarities)
    similarity_matrix = comparison_df.pivot(
        index='General_Topic',
        columns='Influential_Topic',
        values='Similarity'
    )

    # Reorder the similarity_matrix to reverse the order of General Topics
    similarity_matrix = similarity_matrix.iloc[::-1]  # Reverse the row order

    # Generate heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Jaccard Similarity'})
    plt.title('Topic Similarity Heatmap')
    plt.xlabel('Influential Topics')
    plt.ylabel('General Topics')

    # Save heatmap
    os.makedirs(output_dir, exist_ok=True)
    heatmap_path = os.path.join(output_dir, "topic_similarity_heatmap.png")
    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Heatmap saved to: {heatmap_path}")

    # Find the best matches for general topics
    best_matches = []
    used_influential_topics = set()

    for general_topic in range(1, len(general_results.columns) // 2 + 1):
        topic_similarities = comparison_df[comparison_df['General_Topic'] == general_topic]
        remaining_similarities = topic_similarities[~topic_similarities['Influential_Topic'].isin(used_influential_topics)]

        if not remaining_similarities.empty:
            best_match = remaining_similarities.loc[remaining_similarities['Similarity'].idxmax()]
            best_matches.append({
                'General_Topic': general_topic,
                'Influential_Topic': int(best_match['Influential_Topic']),
                'Similarity': best_match['Similarity']
            })
            used_influential_topics.add(best_match['Influential_Topic'])

    best_matches_df = pd.DataFrame(best_matches)

    # Generate bar plot for best matches
    plt.figure(figsize=(12, 6))
    bars = plt.bar(
        best_matches_df['Influential_Topic'], 
        best_matches_df['Similarity'], 
        color=[plt.cm.Set2(i / 7) for i in best_matches_df['General_Topic']]
    )

    # Add value labels to bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height,
                 f'{height:.2f}',
                 ha='center', va='bottom')

    # Customize bar plot
    plt.title('Best Topic Matches Between General and Influential Authors')
    plt.xlabel('Influential Topics')
    plt.ylabel('Jaccard Similarity')
    plt.ylim(0, max(best_matches_df['Similarity']) * 1.1)  # Add padding above highest bar

    # Add legend for general topics
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=plt.cm.Set2(i / 7), label=f'Topic {i}') 
        for i in best_matches_df['General_Topic']
    ]
    plt.legend(handles=legend_elements, title='Matching General Topic', 
               bbox_to_anchor=(1.05, 1), loc='upper left')

    # Save bar plot
    barplot_path = os.path.join(output_dir, "topic_similarity_barplot.png")
    plt.savefig(barplot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Bar plot saved to: {barplot_path}")

    # Save comparison DataFrame to CSV
    comparison_csv_path = os.path.join(output_dir, "topic_similarity_comparison.csv")
    comparison_df.to_csv(comparison_csv_path, index=False)
    print(f"Comparison data saved to: {comparison_csv_path}")

    # Print similarity statistics
    print("\nTopic Similarity Statistics:")
    print(f"Average similarity: {comparison_df['Similarity'].mean():.3f}")
    print(f"Maximum similarity: {comparison_df['Similarity'].max():.3f}")
    print(f"Minimum similarity: {comparison_df['Similarity'].min():.3f}")

    return comparison_df, best_matches_df

# Example usage
general_results_path = "/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_all_comments.html"
influential_results_path = "/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/lda_analysis_influential_authors.html"
output_dir = "/Users/emerson/Github/usenet_webpage/Images and Tables/Images"

try:
    comparison_df, best_matches_df = compare_topics_and_save(
        general_results_path, 
        influential_results_path, 
        output_dir
    )
    print("\nJaccard Similarity Analysis Complete.")
    print("\nBest Matches DataFrame:")
    print(best_matches_df)
except Exception as e:
    print(f"An error occurred during topic comparison: {str(e)}")
```

## Statistical Signifcance
```{python}
import pandas as pd
import statsmodels.api as sm
from scipy.stats import ttest_ind
from scipy.sparse import SparseEfficiencyWarning
import warnings

# Suppress warnings
warnings.simplefilter("ignore", category=SparseEfficiencyWarning)
warnings.simplefilter("ignore", category=FutureWarning)

# Load the comparison data
comparison_csv_path = "/Users/emerson/Github/usenet_webpage/Images and Tables/Images/topic_similarity_comparison.csv"
comparison_data = pd.read_csv(comparison_csv_path)

# Regression Analysis
print("Preparing data for regression analysis...")

# Ensure General_Topic and Influential_Topic are categorical
comparison_data['General_Topic'] = comparison_data['General_Topic'].astype('category')
comparison_data['Influential_Topic'] = comparison_data['Influential_Topic'].astype('category')

# Create dummy variables for categorical predictors
X = pd.get_dummies(comparison_data[['General_Topic', 'Influential_Topic']], drop_first=True)

# Add a constant for the regression model
X = sm.add_constant(X)

# Convert boolean predictors to integers and ensure all data is numeric
X = X.applymap(lambda value: int(value) if isinstance(value, bool) else value)
X = X.apply(pd.to_numeric, errors='coerce')

# Dependent variable: Similarity
y = pd.to_numeric(comparison_data['Similarity'], errors='coerce')

# Drop rows with NaN in X or y
if X.isnull().any().any() or y.isnull().any():
    print("Warning: Non-numeric or missing values detected. Dropping invalid rows.")
    valid_rows = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[valid_rows]
    y = y[valid_rows]

# Verify the cleaned data
print("\nCleaned Predictors (X):")
print(X.head())
print("\nCleaned Dependent Variable (y):")
print(y.head())

# Fit the regression model
try:
    model = sm.OLS(y, X).fit()
    print("\nRegression Analysis Summary:")
    print(model.summary())
except Exception as e:
    print(f"An error occurred during regression analysis: {str(e)}")

# Pairwise T-Test Analysis
print("\nPerforming pairwise T-tests between General Topics...")

# Group Jaccard similarities by General_Topic
grouped_data = comparison_data.groupby('General_Topic')['Similarity']

# Perform pairwise T-tests between General_Topic groups
t_test_results = []
for (group1, group2) in combinations(grouped_data.groups.keys(), 2):
    similarity_group1 = grouped_data.get_group(group1)
    similarity_group2 = grouped_data.get_group(group2)
    t_stat, p_value = ttest_ind(similarity_group1, similarity_group2, equal_var=False)
    t_test_results.append({'Group1': group1, 'Group2': group2, 't-statistic': t_stat, 'p-value': p_value})

# Convert T-test results into a DataFrame
t_test_results_df = pd.DataFrame(t_test_results)

# Display significant T-test results
significant_results = t_test_results_df[t_test_results_df['p-value'] < 0.05]
print("\nPairwise T-Test Results (Significant at p < 0.05):")
print(significant_results)

# Save T-test results to CSV
t_test_results_path = "/Users/emerson/Github/usenet_webpage/Images and Tables/Images/t_test_results.csv"
t_test_results_df.to_csv(t_test_results_path, index=False)
print(f"\nT-test results saved to: {t_test_results_path}")

```

## Co-occurance Network
```{python co-occurance network}
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull
import os
import seaborn as sns

def create_static_network(doc_term_matrix, lda_model, vectorizer, output_path='topic_network_visualization.png'):
    """Create static co-occurrence network visualization with topic regions."""
    print("Creating static co-occurrence network...")
    
    # Get the document-term matrix as array
    dtm_array = doc_term_matrix.toarray()
    co_occurrence = np.dot(dtm_array.T, dtm_array)  # Calculate co-occurrence matrix
    
    # Get term frequencies
    term_frequency = np.sum(dtm_array, axis=0)
    
    # Get feature names from the vectorizer
    feature_names = vectorizer.get_feature_names_out()
    
    # Get the top terms from LDA topics
    top_terms = []
    for topic_idx, topic in enumerate(lda_model.components_):
        top_terms.extend([feature_names[i] for i in topic.argsort()[-15:]])  # Top 15 terms for each topic
    top_terms = list(set(top_terms))
    
    # Filter terms in the co-occurrence matrix
    filtered_indices = [i for i, term in enumerate(feature_names) if term in top_terms]
    filtered_co_occurrence = co_occurrence[filtered_indices][:, filtered_indices]
    filtered_terms = [feature_names[i] for i in filtered_indices]
    
    # Create a graph using the filtered terms
    G = nx.Graph()
    
    # Add edges and nodes
    for i in range(len(filtered_terms)):
        for j in range(i + 1, len(filtered_terms)):
            weight = filtered_co_occurrence[i, j]
            if weight > 0:
                G.add_edge(filtered_terms[i], filtered_terms[j], weight=weight)
    
    # Set up the plot
    plt.figure(figsize=(20, 20), facecolor='white')
    
    # Layout for graph nodes
    pos = nx.spring_layout(G, seed=42)
    
    # Node sizes based on term frequencies
    node_sizes = [np.log1p(term_frequency[filtered_indices][i]) * 500 for i in range(len(filtered_terms))]
    
    # Get topic assignments for terms
    term_topic_assignment = []
    for term in filtered_terms:
        term_topics = []
        for topic_idx, topic in enumerate(lda_model.components_):
            if term in [feature_names[i] for i in topic.argsort()[-15:]]:
                term_topics.append((topic_idx, topic[feature_names.tolist().index(term)]))
        if term_topics:
            term_topic_assignment.append(max(term_topics, key=lambda x: x[1])[0])
        else:
            term_topic_assignment.append(0)
    
    # Draw nodes and edges
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    max_edge_weight = max(edge_weights) if edge_weights else 1
    edge_widths = [0.3 + (w / max_edge_weight) for w in edge_weights]
    
    nx.draw_networkx_edges(G, pos, alpha=0.2, width=edge_widths, edge_color='gray')
    
    colors = sns.color_palette("Set2", len(lda_model.components_))
    for topic in range(len(lda_model.components_)):
        topic_nodes = [filtered_terms[i] for i, t in enumerate(term_topic_assignment) if t == topic]
        nx.draw_networkx_nodes(
            G, pos, nodelist=topic_nodes, 
            node_color=[colors[topic]] * len(topic_nodes), 
            node_size=[node_sizes[i] for i, t in enumerate(term_topic_assignment) if t == topic], 
            alpha=0.8
        )
    
    # Draw labels
    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', alpha=0.9)
    
    # Add legend
    legend_elements = [
        plt.Line2D([0], [0], marker='o', color='w', label=f'Topic {i+1}', 
                   markerfacecolor=colors[i], markersize=15)
        for i in range(len(lda_model.components_))
    ]
    plt.legend(handles=legend_elements, loc='upper right', title='Topics')
    
    # Save the visualization
    plt.title("Topic Co-occurrence Network", fontsize=16)
    plt.axis('off')
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"Co-occurrence network saved to {output_path}.")
    
    return G

# Call the function
output_path = os.path.join(output_directory, "Images and Tables/Images/cooccurrence_network.png")
try:
    print("Generating the co-occurrence network...")
    G = create_static_network(
        doc_term_matrix=dataset2_matrix,  # Replace with dataset3_matrix for Dataset 3
        lda_model=lda_dataset2,          # Replace with lda_dataset3 for Dataset 3
        vectorizer=dataset2_vectorizer,  # Replace with dataset3_vectorizer for Dataset 3
        output_path=output_path
    )
    print("Co-occurrence network generation complete!")
except Exception as e:
    print(f"An error occurred: {e}")
```

## Keyword Adoption
```{python Keyword Adoption}
import pandas as pd
import numpy as np
import re
from nltk.tokenize import word_tokenize
from collections import Counter
import matplotlib.pyplot as plt
import os


def preprocess_text_for_matching(text):
    """
    Preprocess text for keyword matching: tokenize, lowercase, and clean.
    """
    if pd.isna(text) or not isinstance(text, str):
        return []
    # Tokenize, lowercase, and remove non-alphanumeric characters
    text = re.sub(r"\W+", " ", text)
    tokens = word_tokenize(text.lower())
    return tokens


def extract_lda_keywords_with_betas(lda_model, vectorizer, n_top_words=20):
    """
    Extract top keywords and their beta values from LDA model topics.
    """
    # Define custom stop words
    custom_stop_words = {"iii"}

    keyword_betas = {}
    for topic_idx, topic in enumerate(lda_model.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        for word, beta in zip(top_words, top_betas):
            if word not in custom_stop_words:
                if word in keyword_betas:
                    keyword_betas[word] += beta
                else:
                    keyword_betas[word] = beta
    return keyword_betas  # Return dictionary of keywords and their combined betas


def calculate_keyword_prevalence(df, keywords, text_column='Full.Text'):
    """
    Calculate monthly prevalence of shared keywords.
    """
    # Ensure 'Date' column is datetime
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.dropna(subset=['Date'])  # Drop rows with invalid dates

    monthly_counts = []
    for _, row in df.iterrows():
        tokens = preprocess_text_for_matching(row[text_column])
        keyword_counts = Counter(word for word in tokens if word in keywords)
        for word, count in keyword_counts.items():
            monthly_counts.append({
                'Date': row['Date'],
                'word': word,
                'count': count
            })

    counts_df = pd.DataFrame(monthly_counts)
    if counts_df.empty:
        raise ValueError("No shared keywords found in the dataset after processing.")

    # Group by month and calculate prevalence
    monthly_prevalence = counts_df.groupby([
        pd.Grouper(key='Date', freq='M'),
        'word'
    ])['count'].sum().reset_index()

    total_counts = monthly_prevalence.groupby('Date')['count'].sum().reset_index()
    monthly_prevalence = monthly_prevalence.merge(total_counts, on='Date', suffixes=('', '_total'))
    monthly_prevalence['prevalence'] = monthly_prevalence['count'] / monthly_prevalence['count_total']

    return monthly_prevalence


def plot_keyword_adoption(influential_prevalence, overall_prevalence, save_path):
    """
    Plot keyword adoption patterns for influential authors and general discussions.
    """
    influential_prevalence['Group'] = 'Influential Authors'
    overall_prevalence['Group'] = 'Overall Discussion'
    combined_data = pd.concat([influential_prevalence, overall_prevalence])

    keywords = combined_data['word'].unique()
    n_keywords = len(keywords)

    n_cols = 4
    n_rows = int(np.ceil(n_keywords / n_cols))
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3))
    axes = axes.flatten()

    for idx, keyword in enumerate(sorted(keywords)):
        keyword_data = combined_data[combined_data['word'] == keyword]
        for group in ['Influential Authors', 'Overall Discussion']:
            group_data = keyword_data[keyword_data['Group'] == group]
            color = '#ff7f0e' if group == 'Influential Authors' else '#1f77b4'
            axes[idx].plot(group_data['Date'], group_data['prevalence'], label=group, color=color)
        axes[idx].set_title(keyword)
        axes[idx].tick_params(axis='x', rotation=45)
        axes[idx].grid(True, alpha=0.3)

    for idx in range(len(keywords), len(axes)):
        fig.delaxes(axes[idx])

    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 0.96))

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()


# Execution for Top 20 Shared Keywords
try:
    print("\nAnalyzing adoption patterns for top 20 shared LDA keywords...")

    # Extract LDA keywords with betas from both models
    lda_keywords_betas_all_comments = extract_lda_keywords_with_betas(lda_dataset2, dataset2_vectorizer)
    lda_keywords_betas_influential_authors = extract_lda_keywords_with_betas(lda_dataset3, dataset3_vectorizer)

    # Find shared keywords between general and influential topics
    shared_keywords = set(lda_keywords_betas_all_comments.keys()).intersection(
        set(lda_keywords_betas_influential_authors.keys())
    )

    # Combine beta values for shared keywords
    combined_keyword_betas = {
        word: lda_keywords_betas_all_comments[word] + lda_keywords_betas_influential_authors[word]
        for word in shared_keywords
    }

    # Select top 20 shared keywords based on combined beta values
    top_20_keywords = sorted(combined_keyword_betas.items(), key=lambda x: x[1], reverse=True)[:20]
    top_20_keywords = {word for word, beta in top_20_keywords}
    print(f"\nTop 20 Shared Keywords: {top_20_keywords}")

    # Calculate keyword prevalence for top 20 shared keywords
    influential_prevalence = calculate_keyword_prevalence(
        dataset3_comments_onlyinfluential, top_20_keywords
    )
    overall_prevalence = calculate_keyword_prevalence(
        dataset3_comments_all, top_20_keywords
    )

    # Plot and save results
    output_path = os.path.join(images_dir, "lda_keyword_adoption_over_time.png")
    plot_keyword_adoption(influential_prevalence, overall_prevalence, output_path)

    print("\nTop 20 shared keyword adoption analysis completed successfully!")
    print(f"Visualization saved to: {output_path}")

except Exception as e:
    print(f"An error occurred: {str(e)}")
```

## Influence Propogation Network
```{python}
import os

# Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory = os.path.join(output_directory, "CSV Files/Threads")
comments_directory = os.path.join(output_directory, "CSV Files/Comments")
images_dir = os.path.join(output_directory, "Images and Tables/Images")
tables_dir = os.path.join(output_directory, "Images and Tables/Tables")

# Ensure the images directory exists
os.makedirs(images_dir, exist_ok=True)

# Full Influence Propagation Network Code
def create_influence_network(dataset1_comments, influential_authors, output_path):
    """Create and visualize influence network based on thread interactions."""
    
    # Convert 'Date' column to datetime if not already done
    dataset1_comments['Date'] = pd.to_datetime(dataset1_comments['Date'], errors='coerce')
    dataset1_comments = dataset1_comments.dropna(subset=['Date'])  # Drop rows with invalid dates

    # Filter threads involving influential authors
    influential_authors_set = set(influential_authors['Author'])
    dataset1_comments = dataset1_comments[dataset1_comments['Author'].isin(influential_authors_set)]

    # Group by thread ID
    interactions = []
    for thread_id, group in dataset1_comments.groupby('TH_ID'):
        group = group.sort_values('Date')  # Sort comments in the thread by date
        authors = group['Author'].tolist()
        dates = group['Date'].tolist()
        
        # Create interactions between consecutive authors in the thread
        for i in range(1, len(authors)):
            source = authors[i - 1]
            target = authors[i]
            response_time = (dates[i] - dates[i - 1]).total_seconds()  # Calculate response time in seconds
            interactions.append((source, target, response_time))
    
    # Build the directed graph
    G = nx.DiGraph()

    # Add nodes (authors)
    for author in influential_authors_set:
        comment_count = len(dataset1_comments[dataset1_comments['Author'] == author])
        G.add_node(author, size=np.log2(comment_count + 1) * 100)

    # Add edges (interactions between authors)
    edge_weights = {}
    response_times = {}
    for source, target, response_time in interactions:
        if (source, target) in edge_weights:
            edge_weights[(source, target)] += 1
            response_times[(source, target)].append(response_time)
        else:
            edge_weights[(source, target)] = 1
            response_times[(source, target)] = [response_time]

    for (source, target), weight in edge_weights.items():
        avg_response_time = np.mean(response_times[(source, target)]) / 3600  # Average response time in hours
        G.add_edge(source, target, weight=weight, response_time=avg_response_time)

    # Create visualization
    fig, ax = plt.subplots(figsize=(20, 16))
    pos = nx.spring_layout(G, seed=42)  # Use spring layout for positioning nodes

    # Get edge properties for visualization
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    response_times = [G[u][v]['response_time'] for u, v in edges]

    # Normalize edge colors and widths
    max_weight = max(weights) if weights else 1
    max_response_time = max(response_times) if response_times else 1
    edge_colors = [plt.cm.viridis(rt / max_response_time) for rt in response_times]
    edge_widths = [3 * w / max_weight for w in weights]

    # Draw nodes
    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]
    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', edgecolors='black', alpha=0.9)

    # Draw edges
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, alpha=0.7, edge_cmap=plt.cm.viridis)

    # Draw labels
    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')

    # Add legend for node sizes
    size_labels = [min(node_sizes), np.median(node_sizes), max(node_sizes)]
    legend_labels = [f'{int(2 ** (size / 100) - 1)} comments' for size in size_labels]
    for size, label in zip(size_labels, legend_labels):
        plt.scatter([], [], s=size, c='lightblue', label=label, edgecolors='black', alpha=0.9)
    plt.legend(title='Author Activity', loc='upper right')

    # Add colorbar for edge response times
    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(0, max_response_time))
    sm.set_array([])
    plt.colorbar(sm, ax=ax, label='Average Response Time (hours)')

    # Set title and remove axes
    plt.title('Influence Network of Key Authors\nEdge thickness = interaction frequency, Edge color = response time', fontsize=16)
    plt.axis('off')

    # Save the visualization
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # Return graph statistics
    stats = {
        'num_nodes': G.number_of_nodes(),
        'num_edges': G.number_of_edges(),
        'avg_degree': np.mean([d for n, d in G.degree()]),
        'most_connected': sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:5]
    }
    return stats


# Example usage
output_path = os.path.join(images_dir, "influence_network.png")
stats = create_influence_network(dataset1_comments, influential_authors, output_path=output_path)

# Print summary statistics
print("\nNetwork Summary:")
print(f"Number of authors: {stats['num_nodes']}")
print(f"Number of interactions: {stats['num_edges']}")
print(f"Average degree: {stats['avg_degree']:.2f}")
print("\nMost Connected Authors (by total interactions):")
for author, degree in stats['most_connected']:
    print(f"{author}: {degree} interactions")
```

# Tone
##Sentiment Descriptive Statistics
```{python Descriptive Statistics}
import pandas as pd
from scipy.stats import skew, kurtosis

# Function to calculate descriptive statistics
def calculate_descriptive_stats(df, column_name):
    """Calculate descriptive statistics for a given dataframe and column."""
    min_value = df[column_name].min()
    max_value = df[column_name].max()
    range_value = max_value - min_value
    mean_value = df[column_name].mean()
    median_value = df[column_name].median()
    sd_value = df[column_name].std()
    skewness_value = skew(df[column_name], nan_policy='omit')
    kurtosis_value = kurtosis(df[column_name], nan_policy='omit')
    
    return {
        'Min': min_value,
        'Max': max_value,
        'Range': range_value,
        'Mean': mean_value,
        'Median': median_value,
        'SD': sd_value,
        'Skewness': skewness_value,
        'Kurtosis': kurtosis_value
    }

# List of datasets and their labels
datasets = {
    "Dataset One (All Comments)": dataset1_comments,
    "Dataset Two (Relevant Comments)": dataset2_comments,
    "Dataset Three (Influential Authors)": dataset3_comments_onlyinfluential
}

# Calculate statistics for each dataset
results = {}
for dataset_name, df in datasets.items():
    stats = calculate_descriptive_stats(df, column_name="SentimentScore")  # Replace 'SentimentScore' with the actual column name
    results[dataset_name] = stats

# Convert results into a DataFrame for display
stats_df = pd.DataFrame(results).T
stats_df = stats_df.reset_index().rename(columns={'index': 'Dataset'})

# Save as an HTML table
output_path = '/Users/emerson/Github/usenet_webpage/Images and Tables/Tables/sentiment_descriptive_stats.html'
stats_df.to_html(output_path, index=False, float_format="%.2f", border=1)
print(f"HTML table saved to: {output_path}")

# Optional: Display the HTML table in a pretty format (useful in notebooks or for debugging)
from IPython.core.display import display, HTML
display(HTML(stats_df.to_html(index=False, float_format="%.2f", border=1)))
```
## Sentiment Visualizations
```{python Sentiment Visualization Plot}
def create_sentiment_visualization():
    """Create time series visualization of sentiment scores using existing dataframes."""
    print("Creating sentiment visualization...")
    
    # Create a figure
    plt.figure(figsize=(15, 8))
    
    # Helper function to calculate monthly sentiment averages
    def calculate_monthly_sentiment(df):
        df['Date'] = pd.to_datetime(df['Date'])
        # Filter for the date range
        df = df[(df['Date'] >= '1982-01-01') & (df['Date'] <= '1987-01-01')]
        return df.groupby(pd.Grouper(key='Date', freq='ME'))['SentimentScore'].mean().reset_index()
    
    # Get monthly averages for each dataset
    monthly_sentiment_dataset1 = calculate_monthly_sentiment(dataset1_comments)
    monthly_sentiment_dataset3 = calculate_monthly_sentiment(dataset2_comments)
    monthly_sentiment_influential = calculate_monthly_sentiment(dataset3_comments_onlyinfluential)
    
    # Plot the lines
    plt.plot(monthly_sentiment_dataset1['Date'], 
             monthly_sentiment_dataset1['SentimentScore'],
             label='Dataset One', linewidth=2, color='#1f77b4')
    plt.plot(monthly_sentiment_dataset3['Date'],
             monthly_sentiment_dataset3['SentimentScore'],
             label='Dataset Three', linewidth=2, color='#ff7f0e')
    plt.plot(monthly_sentiment_influential['Date'],
             monthly_sentiment_influential['SentimentScore'],
             label='Influential Authors', linewidth=2, color='#2ca02c')
    
    # Add key events annotations
    key_events = {
        '1982-05-11': "Term 'AIDS' Introduced",
        '1983-09-30': "CDC AIDS Guidelines",
        '1984-04-23': "HHS HIV/AIDS",
        '1984-10-01': "First HIV Blood Test",
        '1985-03-02': "First HIV Test Approved",
        '1985-07-25': "Rock Hudson's Diagnosis",
        '1985-10-02': "HIV Transmission Routes",
        '1986-02-01': "'HIV' Renamed",
        '1986-08-14': "AZT Approved"
    }
    
    # Add event annotations
    y_min = plt.ylim()[0]
    for date, event in key_events.items():
        date_obj = pd.to_datetime(date)
        plt.axvline(x=date_obj, color='gray', linestyle='--', alpha=0.5)
        plt.text(date_obj, y_min, event,
                 rotation=90, verticalalignment='bottom',
                 horizontalalignment='right', fontsize=8)
    
    # Customize plot
    plt.title('Time Series of Average Sentiment Scores Over Time (19821987)', fontsize=14, pad=20)
    plt.xlabel('Month', fontsize=12)
    plt.ylabel('Average Sentiment Score', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),
               ncol=3, frameon=False)
    plt.gca().set_facecolor('white')
    plt.gcf().set_facecolor('white')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Ensure output directory exists
    images_dir = os.path.join(output_directory, "Images and Tables/Images")
    os.makedirs(images_dir, exist_ok=True)
    
    # Save the plot
    output_path = os.path.join(images_dir, "sentiment_time_series.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"Visualization saved to: {output_path}")
    return monthly_sentiment_dataset1, monthly_sentiment_dataset3, monthly_sentiment_influential

# Run the analysis
try:
    print("\nStarting sentiment analysis...")
    results = create_sentiment_visualization()
    print("\nAnalysis completed successfully!")
    
    # Print summary statistics
    print("\nSentiment Statistics:")
    for name, data in zip(['Dataset One', 'Dataset Three', 'Influential Authors'], results):
        print(f"\n{name}:")
        print(f"Average sentiment: {data['SentimentScore'].mean():.3f}")
        print(f"Number of months: {len(data)}")
        
except Exception as e:
    print(f"An error occurred: {str(e)}")
```


## Statistical Significance
```{python}
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Simulate data if not already loaded
dates = pd.date_range(start='1982-01-01', end='1987-01-01', freq='M')

monthly_sentiment_dataset1 = pd.DataFrame({
    'Date': dates,
    'SentimentScore': np.random.uniform(-1, 1, size=len(dates))
})

monthly_sentiment_dataset3 = pd.DataFrame({
    'Date': dates,
    'SentimentScore': np.random.uniform(-1, 1, size=len(dates))
})

monthly_sentiment_influential = pd.DataFrame({
    'Date': dates,
    'SentimentScore': np.random.uniform(-1, 1, size=len(dates))
})

# Combine sentiment data
def combine_sentiment_data(sentiment_dfs, group_names):
    combined_data = pd.concat(
        [df.assign(Group=group_name) for df, group_name in zip(sentiment_dfs, group_names)],
        ignore_index=True
    )
    return combined_data

# Perform ANOVA
def perform_anova(combined_data, value_column='SentimentScore', group_column='Group'):
    model = ols(f"{value_column} ~ C({group_column})", data=combined_data).fit()
    anova_results = sm.stats.anova_lm(model, typ=2)
    return anova_results

# Perform pairwise t-tests
def perform_pairwise_ttests(combined_data, value_column='SentimentScore', group_column='Group'):
    groups = combined_data[group_column].unique()
    pairwise_results = []
    for i, g1 in enumerate(groups):
        for g2 in groups[i+1:]:
            group1_data = combined_data[combined_data[group_column] == g1][value_column]
            group2_data = combined_data[combined_data[group_column] == g2][value_column]
            t_stat, p_value = ttest_ind(group1_data, group2_data, equal_var=False)
            pairwise_results.append({'Group1': g1, 'Group2': g2, 't-statistic': t_stat, 'p-value': p_value})
    return pd.DataFrame(pairwise_results)

# Define groups and combine data
sentiment_dfs = [monthly_sentiment_dataset1, monthly_sentiment_dataset3, monthly_sentiment_influential]
group_names = ['Dataset One', 'Dataset Three', 'Influential Authors']
combined_sentiment_data = combine_sentiment_data(sentiment_dfs, group_names)

# Perform ANOVA and t-tests
anova_results = perform_anova(combined_sentiment_data)
pairwise_results = perform_pairwise_ttests(combined_sentiment_data)

# Output results
print("\nANOVA Results:")
print(anova_results)
print("\nPairwise t-test Results:")
print(pairwise_results)
```

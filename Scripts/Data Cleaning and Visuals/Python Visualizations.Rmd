---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{python Python Load Libraries, Directories, and Datasets, eval=FALSE, include=FALSE}
import gc
import matplotlib.pyplot as plt
import networkx as nx
import nltk
import numpy as np
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import re
import scipy
import seaborn as sns
from collections import Counter
from datetime import datetime
from itertools import combinations
from matplotlib.patches import Polygon
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy.spatial import ConvexHull
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory = os.path.join(output_directory, "CSV Files/Threads")
comments_directory = os.path.join(output_directory, "CSV Files/Comments")
images_dir = os.path.join(output_directory, "Images and Tables/Images")
tables_dir = os.path.join(output_directory, "Images and Tables/Tables")

# Load cleaned datasets
dataset1_threads = pd.read_csv(os.path.join(threads_directory, "dataset1_threads.csv"))
dataset1_comments = pd.read_csv(os.path.join(comments_directory, "dataset1_comments.csv"))
dataset2_threads = pd.read_csv(os.path.join(threads_directory, "dataset2_threads.csv"))
dataset2_comments = pd.read_csv(os.path.join(comments_directory, "dataset2_comments.csv"))
dataset3_threads = pd.read_csv(os.path.join(threads_directory, "dataset3_threads.csv"))
dataset3_comments = pd.read_csv(os.path.join(comments_directory, "dataset3_comments.csv"))
dataset4_threads = pd.read_csv(os.path.join(threads_directory, "dataset4_threads.csv"))
dataset4_comments_all = pd.read_csv(os.path.join(comments_directory, "dataset4_comments_all.csv"))
dataset4_comments_onlyinfluential = pd.read_csv(os.path.join(comments_directory, "dataset4_comments_onlyinfluential.csv"))
influential_authors = pd.read_csv(os.path.join(output_directory, "CSV Files/influential_authors.csv"))
```

## Cool Initial Visualizations
```{python}

```

## Theme Prevelance Hypothesis
### LDA Topic Modeling
```{python LDA Topic Model + Betas, 6 Topics}
# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

def preprocess_text(text):
    """Preprocess text data"""
    # Convert to lowercase
    text = str(text).lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

def create_styled_html(general_results_df):
    """Create a styled HTML table from the results DataFrame"""
    # Create a copy of the DataFrame for styling
    styled_df = general_results_df.copy()
    
    # Define CSS styles
    styles = [
        dict(selector="caption", 
             props=[("caption-side", "top"),
                   ("font-size", "16px"),
                   ("font-weight", "bold"),
                   ("text-align", "center"),
                   ("padding", "10px")]),
        dict(selector="th", 
             props=[("font-size", "14px"),
                   ("text-align", "center"),
                   ("background-color", "#f0f0f0"),
                   ("padding", "8px")]),
        dict(selector="td", 
             props=[("padding", "8px"),
                   ("text-align", "left")]),
        dict(selector="tr:nth-child(even)",
             props=[("background-color", "#f9f9f9")]),
        dict(selector="table",
             props=[("border-collapse", "collapse"),
                   ("width", "100%"),
                   ("margin", "20px 0"),
                   ("font-family", "Arial, sans-serif")]),
        dict(selector="", 
             props=[("border", "1px solid #ddd")])
    ]
    
    # Apply styling
    styled_table = (styled_df.style
                   .set_table_styles(styles)
                   .set_caption("LDA Topic Analysis Results")
                   .format(precision=4)
                   .background_gradient(subset=[col for col in styled_df.columns if 'betas' in col],
                                     cmap='Blues')
                   .hide(axis='index'))
    
    # Add custom CSS for alternating topic columns
    for i in range(1, 7):
        styled_table = styled_table.set_properties(**{
            f'Topic_{i}_terms': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            },
            f'Topic_{i}_betas': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            }
        })
    
    return styled_table

def analyze_topics(filtered_comments, n_topics=6, n_top_words=20):
    """Perform topic analysis on the comments with adjusted parameters"""
    print("Starting topic analysis...")
    
    # Convert dates to datetime and sort
    filtered_comments['Date'] = pd.to_datetime(filtered_comments['Date'])
    filtered_comments = filtered_comments.sort_values('Date')
    
    # Check temporal distribution
    print("\nTemporal distribution of documents:")
    print(filtered_comments['Date'].dt.year.value_counts().sort_index())
    print("\nMonthly document counts:")
    print(filtered_comments.groupby(pd.Grouper(key='Date', freq='M')).size().sort_index())
    
    # Preprocess texts
    texts = [preprocess_text(text) for text in filtered_comments['Full.Text']]
    
    # Create document-term matrix with adjusted parameters
    print("Creating document-term matrix...")
    vectorizer = CountVectorizer(
        max_df=0.8,
        min_df=3,
        stop_words='english',
        max_features=5000
    )
    doc_term_matrix = vectorizer.fit_transform(texts)
    
    # Train LDA model with adjusted parameters
    print("Training LDA model...")
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=123,
        max_iter=50,
        learning_decay=0.7,
        batch_size=128,
        evaluate_every=5
    )
    
    # Fit the model
    lda.fit(doc_term_matrix)
    
    # Get feature names
    feature_names = vectorizer.get_feature_names_out()
    
    # Create top terms dataframe
    print("Extracting top terms...")
    top_terms_dict = {}
    
    for topic_idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words-1:-1]
        top_terms = [feature_names[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        
        top_terms_dict[f'Topic_{topic_idx+1}_terms'] = top_terms
        top_terms_dict[f'Topic_{topic_idx+1}_betas'] = [round(beta, 4) for beta in top_betas]
    
    # Create DataFrame
    general_results_df = pd.DataFrame(top_terms_dict)
    
    # Add temporal information
    doc_topics = lda.transform(doc_term_matrix)
    document_results = pd.DataFrame(doc_topics)
    document_results.columns = [f'Topic_{i+1}' for i in range(n_topics)]
    document_results['Date'] = filtered_comments['Date']
    
    # Save table to Tables directory
    tables_path = os.path.join(tables_dir, "lda_analysis_dataset3.html")
    
    # Create HTML content
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>Dataset 3 LDA Topic Analysis Results</caption>
            {general_results_df.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(tables_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Create visualization
    print("Creating visualization...")
    plt.figure(figsize=(15, 10))
    for topic_idx in range(n_topics):
        plt.subplot(2, 3, topic_idx + 1)
        top_terms = top_terms_dict[f'Topic_{topic_idx+1}_terms'][:10]
        top_betas = top_terms_dict[f'Topic_{topic_idx+1}_betas'][:10]
        plt.barh(range(len(top_terms)), top_betas)
        plt.yticks(range(len(top_terms)), top_terms)
        plt.title(f'Topic {topic_idx + 1}')
    
    plt.tight_layout()
    
    # Save plot to Images directory
    plot_path = os.path.join(images_dir, "lda_visualization_dataset3.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print("Analysis complete! Results saved to:")
    print(f"- Table: {tables_path}")
    print(f"- Plot: {plot_path}")
    
    return general_results_df, lda, vectorizer, doc_term_matrix, document_results

# Run the analysis
try:
    print("\nStarting topic analysis...")
    general_results_df, lda_model, vectorizer, doc_term_matrix, document_results = analyze_topics(dataset3_comments)
    print("\nAnalysis completed successfully!")
    
    # Display top terms for each topic
    print("\nTop terms for each topic:")
    print(general_results_df.head())
    
except Exception as e:
    print(f"An error occurred: {str(e)}")
    raise
```
### Co-occurance Network
```{python co-occurance network}
def create_static_network(doc_term_matrix, lda_model, vectorizer, output_path='topic_network_visualization.png'):
    """Create static co-occurrence network visualization with topic regions"""
    print("Creating static co-occurrence network...")
    
    # Get the document-term matrix as array
    dtm_array = doc_term_matrix.toarray()
    co_occurrence = np.dot(dtm_array.T, dtm_array)
    
    # Create binary DTM
    binary_dtm = (dtm_array > 0).astype(int)
    term_frequency = np.sum(binary_dtm, axis=0)
    
    # Filter for terms that appear in the top terms from LDA results
    top_terms = []
    for i in range(6):
        col_name = f'Topic_{i+1}_terms'
        top_terms.extend(general_results_df[col_name].head(15).tolist())  # Limit to top 15 terms per topic
    top_terms = list(set(top_terms))
    
    # Get indices of these terms
    feature_names = vectorizer.get_feature_names_out()
    filtered_terms = [i for i, term in enumerate(feature_names) if term in top_terms]
    filtered_co_occurrence = co_occurrence[filtered_terms][:, filtered_terms]
    terms = feature_names[filtered_terms]
    
    # Create networkx graph
    G = nx.Graph()
    
    # Add edges with weights
    for i in range(len(terms)):
        for j in range(i + 1, len(terms)):
            if filtered_co_occurrence[i, j] > 0:
                G.add_edge(terms[i], terms[j], weight=filtered_co_occurrence[i, j])
    
    # Get topic assignments for terms
    term_topic_assignment = []
    for term in terms:
        term_topics = []
        for i in range(6):
            terms_col = f'Topic_{i+1}_terms'
            betas_col = f'Topic_{i+1}_betas'
            if term in general_results_df[terms_col].values:
                idx = general_results_df[terms_col][general_results_df[terms_col] == term].index[0]
                term_topics.append((i, general_results_df[betas_col].iloc[idx]))
        if term_topics:
            term_topic_assignment.append(max(term_topics, key=lambda x: x[1])[0])
        else:
            term_topic_assignment.append(0)
    
    # Set up the plot with white background
    plt.figure(figsize=(20, 20), facecolor='white')
    
    # Create layout with more spacing
    pos = nx.spring_layout(G, k=4, iterations=100, seed=42)
    
    # Convert pos dict to numpy arrays for each topic
    topic_positions = {i: [] for i in range(6)}
    for node, position in pos.items():
        idx = list(terms).index(node)
        topic = term_topic_assignment[idx]
        topic_positions[topic].append(position)
    
    # Draw topic regions
    colors = sns.color_palette("Set2", n_colors=6)
    alpha_fill = 0.2
    alpha_edge = 0.5
    
    # Draw convex hulls for each topic
    for topic in range(6):
        if len(topic_positions[topic]) > 2:  # Need at least 3 points for convex hull
            points = np.array(topic_positions[topic])
            hull = ConvexHull(points)
            hull_points = points[hull.vertices]
            plt.fill(hull_points[:, 0], hull_points[:, 1], 
                    alpha=alpha_fill, color=colors[topic])
            plt.plot(hull_points[:, 0], hull_points[:, 1], 
                    color=colors[topic], alpha=alpha_edge)
    
    # Draw edges
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    max_edge_weight = max(edge_weights) if edge_weights else 1
    edge_widths = [0.3 + (w / max_edge_weight) for w in edge_weights]
    nx.draw_networkx_edges(G, pos, alpha=0.1, width=edge_widths, edge_color='gray')
    
    # Draw nodes
    node_sizes = [np.log1p(term_frequency[filtered_terms][i]) * 500 for i in range(len(terms))]
    for topic in range(6):
        # Get nodes for this topic
        topic_nodes = [node for node, idx in enumerate(term_topic_assignment) if idx == topic]
        if topic_nodes:
            nx.draw_networkx_nodes(G, pos, 
                                 nodelist=[terms[i] for i in topic_nodes],
                                 node_color=[colors[topic]],
                                 node_size=[node_sizes[i] for i in topic_nodes],
                                 alpha=0.7)
    
    # Add labels with better spacing and formatting
    labels = {node: node for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels,
                          font_size=10,
                          font_weight='bold',
                          bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.5))
    
    # Add legend
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                label=f'Topic {i+1}',
                                markerfacecolor=colors[i], markersize=15)
                      for i in range(6)]
    plt.legend(handles=legend_elements, loc='upper right',
              title='Topics', title_fontsize=12, fontsize=10)
    
    # Remove axes
    plt.axis('on')
    
    # Save with high quality
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return G

# Create the visualization using existing data
try:
    print("\nCreating static network visualization...")
    G = create_static_network(
        doc_term_matrix=doc_term_matrix,
        lda_model=lda_model,
        vectorizer=vectorizer,
        output_path=os.path.join(output_directory, "Images and Tables/Images/topic_network_visualization.png")
    )

    print("\nNetwork visualization completed successfully!")
    print("Check 'topic_network_visualization.png' for the static network visualization.")

except Exception as e:
    print(f"An error occurred while creating the network: {str(e)}")
```

## Emotional Tone Hypothesis
```{python Sentiment Visualization Plot}
def create_sentiment_visualization():
    """Create time series visualization of sentiment scores using existing dataframes."""
    print("Creating sentiment visualization...")
    
    # Create a figure
    plt.figure(figsize=(15, 8))
    
    # Helper function to calculate monthly sentiment averages
    def calculate_monthly_sentiment(df):
        df['Date'] = pd.to_datetime(df['Date'])
        # Filter for the date range
        df = df[(df['Date'] >= '1982-01-01') & (df['Date'] <= '1987-01-01')]
        return df.groupby(pd.Grouper(key='Date', freq='ME'))['SentimentScore'].mean().reset_index()
    
    # Get monthly averages for each dataset
    monthly_sentiment_dataset1 = calculate_monthly_sentiment(dataset1_comments)
    monthly_sentiment_dataset3 = calculate_monthly_sentiment(dataset3_comments)
    monthly_sentiment_influential = calculate_monthly_sentiment(dataset4_comments_onlyinfluential)
    
    # Plot the lines
    plt.plot(monthly_sentiment_dataset1['Date'], 
             monthly_sentiment_dataset1['SentimentScore'],
             label='Dataset One', linewidth=2, color='#1f77b4')
    plt.plot(monthly_sentiment_dataset3['Date'],
             monthly_sentiment_dataset3['SentimentScore'],
             label='Dataset Three', linewidth=2, color='#ff7f0e')
    plt.plot(monthly_sentiment_influential['Date'],
             monthly_sentiment_influential['SentimentScore'],
             label='Influential Authors', linewidth=2, color='#2ca02c')
    
    # Add key events annotations
    key_events = {
        '1982-05-11': "Term 'AIDS' Introduced",
        '1983-09-30': "CDC AIDS Guidelines",
        '1984-04-23': "HHS HIV/AIDS",
        '1984-10-01': "First HIV Blood Test",
        '1985-03-02': "First HIV Test Approved",
        '1985-07-25': "Rock Hudson's Diagnosis",
        '1985-10-02': "HIV Transmission Routes",
        '1986-02-01': "'HIV' Renamed",
        '1986-08-14': "AZT Approved"
    }
    
    # Add event annotations
    y_min = plt.ylim()[0]
    for date, event in key_events.items():
        date_obj = pd.to_datetime(date)
        plt.axvline(x=date_obj, color='gray', linestyle='--', alpha=0.5)
        plt.text(date_obj, y_min, event,
                 rotation=90, verticalalignment='bottom',
                 horizontalalignment='right', fontsize=8)
    
    # Customize plot
    plt.title('Time Series of Average Sentiment Scores Over Time (1982â€“1987)', fontsize=14, pad=20)
    plt.xlabel('Month', fontsize=12)
    plt.ylabel('Average Sentiment Score', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),
               ncol=3, frameon=False)
    plt.gca().set_facecolor('white')
    plt.gcf().set_facecolor('white')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Ensure output directory exists
    images_dir = os.path.join(output_directory, "Images and Tables/Images")
    os.makedirs(images_dir, exist_ok=True)
    
    # Save the plot
    output_path = os.path.join(images_dir, "sentiment_time_series.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"Visualization saved to: {output_path}")
    return monthly_sentiment_dataset1, monthly_sentiment_dataset3, monthly_sentiment_influential

# Run the analysis
try:
    print("\nStarting sentiment analysis...")
    results = create_sentiment_visualization()
    print("\nAnalysis completed successfully!")
    
    # Print summary statistics
    print("\nSentiment Statistics:")
    for name, data in zip(['Dataset One', 'Dataset Three', 'Influential Authors'], results):
        print(f"\n{name}:")
        print(f"Average sentiment: {data['SentimentScore'].mean():.3f}")
        print(f"Number of months: {len(data)}")
        
except Exception as e:
    print(f"An error occurred: {str(e)}")
```

## Author Impact Hypothesis
### Topic Similarity
```{python LDA Topic Model For Influential Authors"}
# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

def preprocess_text(text):
    """Preprocess text data"""
    # Convert to lowercase
    text = str(text).lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

def create_styled_html(influential_results_df):
    """Create a styled HTML table from the results DataFrame"""
    # Create a copy of the DataFrame for styling
    styled_df = influential_results_df.copy()
    
    # Define CSS styles
    styles = [
        dict(selector="caption", 
             props=[("caption-side", "top"),
                   ("font-size", "16px"),
                   ("font-weight", "bold"),
                   ("text-align", "center"),
                   ("padding", "10px")]),
        dict(selector="th", 
             props=[("font-size", "14px"),
                   ("text-align", "center"),
                   ("background-color", "#f0f0f0"),
                   ("padding", "8px")]),
        dict(selector="td", 
             props=[("padding", "8px"),
                   ("text-align", "left")]),
        dict(selector="tr:nth-child(even)",
             props=[("background-color", "#f9f9f9")]),
        dict(selector="table",
             props=[("border-collapse", "collapse"),
                   ("width", "100%"),
                   ("margin", "20px 0"),
                   ("font-family", "Arial, sans-serif")]),
        dict(selector="", 
             props=[("border", "1px solid #ddd")])
    ]
    
    # Apply styling
    styled_table = (styled_df.style
                   .set_table_styles(styles)
                   .set_caption("LDA Topic Analysis Results")
                   .format(precision=4)
                   .background_gradient(subset=[col for col in styled_df.columns if 'betas' in col],
                                     cmap='Blues')
                   .hide(axis='index'))
    
    # Add custom CSS for alternating topic columns
    for i in range(1, 7):
        styled_table = styled_table.set_properties(**{
            f'Topic_{i}_terms': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            },
            f'Topic_{i}_betas': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            }
        })
    
    return styled_table

def analyze_topics(filtered_comments, n_topics=6, n_top_words=20):
    """Perform topic analysis on the comments with adjusted parameters"""
    print("Starting topic analysis...")
    
    # Convert dates to datetime and sort
    filtered_comments['Date'] = pd.to_datetime(filtered_comments['Date'])
    filtered_comments = filtered_comments.sort_values('Date')
    
    # Check temporal distribution
    print("\nTemporal distribution of documents:")
    print(filtered_comments['Date'].dt.year.value_counts().sort_index())
    print("\nMonthly document counts:")
    print(filtered_comments.groupby(pd.Grouper(key='Date', freq='M')).size().sort_index())
    
    # Preprocess texts
    texts = [preprocess_text(text) for text in filtered_comments['Full.Text']]
    
    # Create document-term matrix with adjusted parameters
    print("Creating document-term matrix...")
    vectorizer = CountVectorizer(
        max_df=0.8,
        min_df=3,
        stop_words='english',
        max_features=5000
    )
    doc_term_matrix = vectorizer.fit_transform(texts)
    
    # Train LDA model with adjusted parameters
    print("Training LDA model...")
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=123,
        max_iter=50,
        learning_decay=0.7,
        batch_size=128,
        evaluate_every=5
    )
    
    # Fit the model
    lda.fit(doc_term_matrix)
    
    # Get feature names
    feature_names = vectorizer.get_feature_names_out()
    
    # Create top terms dataframe
    print("Extracting top terms...")
    top_terms_dict = {}
    
    for topic_idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words-1:-1]
        top_terms = [feature_names[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        
        top_terms_dict[f'Topic_{topic_idx+1}_terms'] = top_terms
        top_terms_dict[f'Topic_{topic_idx+1}_betas'] = [round(beta, 4) for beta in top_betas]
    
    # Create DataFrame
    influential_results_df = pd.DataFrame(top_terms_dict)
    
    # Add temporal information
    doc_topics = lda.transform(doc_term_matrix)
    document_results = pd.DataFrame(doc_topics)
    document_results.columns = [f'Topic_{i+1}' for i in range(n_topics)]
    document_results['Date'] = filtered_comments['Date']
    
    # Save table to Tables directory
    tables_path = os.path.join(tables_dir, "lda_analysis_dataset4.html")
    
    # Create HTML content
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>Dataset 4 LDA Topic Analysis Results</caption>
            {influential_results_df.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(tables_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Create visualization
    print("Creating visualization...")
    plt.figure(figsize=(15, 10))
    for topic_idx in range(n_topics):
        plt.subplot(2, 3, topic_idx + 1)
        top_terms = top_terms_dict[f'Topic_{topic_idx+1}_terms'][:10]
        top_betas = top_terms_dict[f'Topic_{topic_idx+1}_betas'][:10]
        plt.barh(range(len(top_terms)), top_betas)
        plt.yticks(range(len(top_terms)), top_terms)
        plt.title(f'Topic {topic_idx + 1}')
    
    plt.tight_layout()
    
    # Save plot to Images directory
    plot_path = os.path.join(images_dir, "lda_visualization_dataset4.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print("Analysis complete! Results saved to:")
    print(f"- Table: {tables_path}")
    print(f"- Plot: {plot_path}")
    
    return influential_results_df, lda, vectorizer, doc_term_matrix, document_results

# Run the analysis
try:
    print("\nStarting topic analysis...")
    influential_results_df, lda_model, vectorizer, doc_term_matrix, document_results = analyze_topics(dataset4_comments_onlyinfluential)
    print("\nAnalysis completed successfully!")
    
    # Display top terms for each topic
    print("\nTop terms for each topic:")
    print(influential_results_df.head())
    
except Exception as e:
    print(f"An error occurred: {str(e)}")
    raise
```

```{python Topic Similarity}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import os

def compare_topics(general_results, influential_results, output_dir):
    """
    Compare topics between general and influential author analyses 
    using Jaccard similarity and create visualizations.
    """
    print("\nComparing topics with Jaccard similarity...")

    # List to store topic similarity data
    topic_similarities = []

    # Calculate Jaccard similarity for all topic pairs
    for i in range(1, len(general_results.columns) // 2 + 1):  # General topics
        general_terms = set(general_results[f'Topic_{i}_terms'])
        for j in range(1, len(influential_results.columns) // 2 + 1):  # Influential topics
            influential_terms = set(influential_results[f'Topic_{j}_terms'])
            similarity = len(general_terms.intersection(influential_terms)) / len(general_terms.union(influential_terms))
            topic_similarities.append({
                'General_Topic': i,
                'Influential_Topic': j,
                'Similarity': similarity
            })

    # Create similarity DataFrame
    comparison_df = pd.DataFrame(topic_similarities)
    similarity_matrix = comparison_df.pivot(
        index='General_Topic',
        columns='Influential_Topic',
        values='Similarity'
    )

    # Generate heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Jaccard Similarity'})
    plt.title('Topic Similarity Heatmap')
    plt.xlabel('Influential Authors Topics')
    plt.ylabel('General Topics')

    # Save heatmap
    os.makedirs(output_dir, exist_ok=True)
    heatmap_path = os.path.join(output_dir, "topic_similarity_heatmap.png")
    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Heatmap saved to: {heatmap_path}")

    # Find the best matches for general topics
    best_matches = []
    used_influential_topics = set()

    for general_topic in range(1, len(general_results.columns) // 2 + 1):
        topic_similarities = comparison_df[comparison_df['General_Topic'] == general_topic]
        remaining_similarities = topic_similarities[~topic_similarities['Influential_Topic'].isin(used_influential_topics)]

        if not remaining_similarities.empty:
            best_match = remaining_similarities.loc[remaining_similarities['Similarity'].idxmax()]
            best_matches.append({
                'General_Topic': general_topic,
                'Influential_Topic': int(best_match['Influential_Topic']),
                'Similarity': best_match['Similarity']
            })
            used_influential_topics.add(best_match['Influential_Topic'])

    best_matches_df = pd.DataFrame(best_matches)

    # Generate bar plot for best matches
    plt.figure(figsize=(12, 6))
    bars = plt.bar(
        best_matches_df['Influential_Topic'], 
        best_matches_df['Similarity'], 
        color=[plt.cm.Set2(i / 7) for i in best_matches_df['General_Topic']]
    )

    # Add value labels to bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height,
                 f'{height:.2f}',
                 ha='center', va='bottom')

    # Customize bar plot
    plt.title('Best Topic Matches Between General and Influential Authors')
    plt.xlabel('Influential Authors Topics')
    plt.ylabel('Jaccard Similarity')
    plt.ylim(0, max(best_matches_df['Similarity']) * 1.1)  # Add padding above highest bar

    # Add legend for general topics
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=plt.cm.Set2(i / 7), label=f'Topic {i}') 
        for i in best_matches_df['General_Topic']
    ]
    plt.legend(handles=legend_elements, title='Matching General Topic', 
               bbox_to_anchor=(1.05, 1), loc='upper left')

    # Save bar plot
    barplot_path = os.path.join(output_dir, "topic_similarity_barplot.png")
    plt.savefig(barplot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Bar plot saved to: {barplot_path}")

    # Print similarity statistics
    print("\nTopic Similarity Statistics:")
    print(f"Average similarity: {comparison_df['Similarity'].mean():.3f}")
    print(f"Maximum similarity: {comparison_df['Similarity'].max():.3f}")
    print(f"Minimum similarity: {comparison_df['Similarity'].min():.3f}")

    return comparison_df, best_matches_df

# Example usage
try:
    comparison_df, best_matches_df = compare_topics(
        general_results_df, 
        influential_results_df, 
        output_dir=os.path.join(output_directory, "Images and Tables/Images")
    )
    print("\nJaccard Similarity Analysis Complete.")
    print("\nBest Matches DataFrame:")
    print(best_matches_df)
except Exception as e:
    print(f"An error occurred during topic comparison: {str(e)}")
```

### Keyword Adoption
```{python Keyword Adoption}
def calculate_keyword_prevalence_aids(df, keywords, text_column='Full.Text'):
    """Calculate keyword prevalence for selected AIDS-related terms."""
    # Ensure the 'Date' column exists
    if 'Date' not in df.columns:
        raise KeyError("'Date' column is missing in the dataset. Ensure your dataset has a 'Date' column.")
    
    # Convert 'Date' column to datetime format
    try:
        df['Date'] = pd.to_datetime(df['Date'])
    except Exception as e:
        raise ValueError(f"Error converting 'Date' column to datetime: {e}")
    
    # Process texts and count keywords by month
    monthly_counts = []
    
    for idx, row in df.iterrows():
        tokens = preprocess_text(row[text_column])
        # Count keywords in this document
        keyword_counts = Counter(word for word in tokens if word in keywords)
        # Add date information
        for word, count in keyword_counts.items():
            monthly_counts.append({
                'Date': row['Date'],
                'word': word,
                'count': count
            })
    
    # Convert to DataFrame
    counts_df = pd.DataFrame(monthly_counts)
    
    # Check if 'Date' exists and grouping is possible
    if counts_df.empty or 'Date' not in counts_df.columns:
        raise ValueError("'Date' column is missing or no data available after keyword processing.")
    
    # Group by month and word
    monthly_prevalence = counts_df.groupby([
        pd.Grouper(key='Date', freq='M'),
        'word'
    ])['count'].sum().reset_index()
    
    # Calculate prevalence
    total_counts = monthly_prevalence.groupby('Date')['count'].sum().reset_index()
    monthly_prevalence = monthly_prevalence.merge(total_counts, on='Date', suffixes=('', '_total'))
    monthly_prevalence['prevalence'] = monthly_prevalence['count'] / monthly_prevalence['count_total']
    
    return monthly_prevalence

# Debugging Code Example
if __name__ == "__main__":
    try:
        print("\nAnalyzing adoption patterns for AIDS-related keywords...")
        
        # Check the datasets
        print("Checking datasets for 'Date' column and proper formatting...")
        print("Dataset 4 Comments (Influential Authors):", dataset4_comments_onlyinfluential.head())
        print("Dataset 3 Comments (Overall Discussion):", dataset3_comments.head())
        
        # Calculate keyword prevalence for AIDS-related terms
        influential_prevalence = calculate_keyword_prevalence_aids(
            dataset4_comments_onlyinfluential, aids_related_keywords
        )
        overall_prevalence = calculate_keyword_prevalence_aids(
            dataset3_comments, aids_related_keywords
        )
        
        # Create visualization
        output_path = os.path.join(images_dir, "aids_keyword_adoption_over_time.png")
        plot_keyword_adoption(influential_prevalence, overall_prevalence, output_path)
        
        print("\nAnalysis completed successfully!")
        print(f"Visualization saved to: {output_path}")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise
```

### Influence Propagation
```{python}
# Identify influential authors' threads
influential_threads = filtered_comments[
    filtered_comments['Author'].isin(top_authors['Author'])
]['Thread.ID'].unique()

# Create edges based on who commented after influential authors
# Select comments in threads where influential authors participated
comments_in_influential_threads = filtered_comments[
    filtered_comments['Thread.ID'].isin(influential_threads)
]

# Sort comments by Thread ID and Date
comments_in_influential_threads = comments_in_influential_threads.sort_values(['Thread.ID', 'Date'])

# Create edges: for each thread, find who commented after influential authors
edges_list = []

# Group by Thread ID
for thread_id, group in comments_in_influential_threads.groupby('Thread.ID'):
    authors = group['Author'].tolist()
    dates = group['Date'].tolist()
    
    # Iterate over comments in the thread
    for i in range(1, len(authors)):
        current_author = authors[i]
        previous_author = authors[i - 1]
        
        # Check if the previous author is an influential author and current author is different
        if previous_author in top_authors['Author'].values and current_author != previous_author:
            edges_list.append({'from': previous_author, 'to': current_author})

# Convert edges to DataFrame
edges_df = pd.DataFrame(edges_list)

# Group edges and calculate weights
edges_df = edges_df.groupby(['from', 'to']).size().reset_index(name='weight')

# Filter for stronger connections (edges with weight above the 25th percentile)
threshold = edges_df['weight'].quantile(0.25)
edges_df = edges_df[edges_df['weight'] > threshold]

# Create directed graph
G = nx.from_pandas_edgelist(edges_df, 'from', 'to', ['weight'], create_using=nx.DiGraph())

# Calculate node sizes based on in-degree
node_sizes = dict(G.in_degree(weight='weight'))

# Identify top 10 influential nodes based on in-degree
top_nodes = sorted(node_sizes.items(), key=lambda x: x[1], reverse=True)[:10]
top_node_names = [node for node, degree in top_nodes]

# Prepare node colors: red for influential authors, light blue for others
node_colors = []
for node in G.nodes():
    if node in top_authors['Author'].values:
        node_colors.append('red')
    else:
        node_colors.append('lightblue')

# Prepare node sizes for plotting (adjust scaling as needed)
node_size_values = [node_sizes.get(node, 1) * 100 for node in G.nodes()]

# Create layout
pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

# Create plot
plt.figure(figsize=(20, 16))

# Draw edges with weights
edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
nx.draw_networkx_edges(
    G, pos,
    edge_color='gray',
    alpha=0.2,
    arrows=True,
    arrowsize=10,
    width=edge_weights,
    arrowstyle='-|>'
)

# Draw nodes
nx.draw_networkx_nodes(
    G, pos,
    node_color=node_colors,
    node_size=node_size_values
)

# Draw labels for top nodes
labels = {node: node if node in top_node_names else '' for node in G.nodes()}
nx.draw_networkx_labels(
    G, pos,
    labels=labels,
    font_size=10,
    font_weight='bold'
)

# Remove axes
plt.axis('off')

# Save the plot
output_file_path = os.path.join(
    output_directory, "Images and Tables", "Images", "plot_influence_propagation_network_improved.png"
)
os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
plt.savefig(output_file_path, dpi=300, bbox_inches='tight')
plt.close()

print(f"Influence propagation network plot saved to {output_file_path}")
```
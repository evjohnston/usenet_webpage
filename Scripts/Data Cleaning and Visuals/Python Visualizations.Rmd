---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Maintenance for R and Python.
```{python Reset and Setup}
import gc

# Clear all variables
gc.collect()
for name in dir():
    if not name.startswith('_'):
        del globals()[name]

# Set the working directory
root_dir = '/Users/emerson/Github/usenet_webpage'
```

```{python Python Load Libraries, Directories, and Datasets, eval=FALSE, include=FALSE}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
import scipy

# Directories
output_directory = "/Users/emerson/Github/usenet_webpage"
threads_directory = os.path.join(output_directory, "CSV Files/Threads")
comments_directory = os.path.join(output_directory, "CSV Files/Comments")
images_dir = os.path.join(output_directory, "Images and Tables/Images")
tables_dir = os.path.join(output_directory, "Images and Tables/Tables")

# Load cleaned datasets
filtered_threads = pd.read_csv(os.path.join(threads_directory, "aids_related_threads_82TO86.csv"))
filtered_comments = pd.read_csv(os.path.join(comments_directory, "aids_related_comments_82TO86.csv"))

summary(filtered_threads)
summary(filtered_comments)
```

```{python Relevancy Filtering}
def create_relevancy_quartiles(all_threads):
    # Calculate min and max values
    min_value = all_threads['Thread_Relevancy'].min()
    max_value = all_threads['Thread_Relevancy'].max()
    
    # Create breaks for quartiles
    breaks = np.linspace(min_value, max_value, 5)
    
    # Create quartile labels
    all_threads['Quartile'] = pd.cut(
        all_threads['Thread_Relevancy'],
        bins=breaks,
        labels=['Q1', 'Q2', 'Q3', 'Q4'],
        include_lowest=True
    )
    
    return all_threads

def plot_quartile_frequencies(df, title="Frequency of Thread Relevancy Quartiles"):
    plt.figure(figsize=(10, 6))
    counts = df['Quartile'].value_counts().sort_index()
    
    bars = plt.bar(counts.index, counts.values, color='skyblue', edgecolor='black')
    
    # Add value labels above bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}',
                ha='center', va='bottom')
    
    plt.title(title)
    plt.xlabel("Quartile")
    plt.ylabel("Frequency")
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_relevancy_histogram(df, bin_width=0.5):
    plt.figure(figsize=(10, 6))
    
    # Create histogram
    plt.hist(df['Thread_Relevancy'], bins=np.arange(min(df['Thread_Relevancy']), 
            max(df['Thread_Relevancy']) + bin_width, bin_width),
            color='skyblue', edgecolor='black', alpha=0.7)
    
    # Add density curve
    density = sns.kdeplot(data=df['Thread_Relevancy'], 
                         color='orange', 
                         alpha=0.4,
                         fill=True)
    
    plt.title("Histogram and Density of Thread Relevancy Scores")
    plt.xlabel("Relevancy")
    plt.ylabel("Number of Threads")
    plt.grid(True, alpha=0.3)
    plt.show()

# Load your data (uncomment and modify paths as needed)
# all_threads = pd.read_csv('path_to_your_threads_file.csv')
# all_comments = pd.read_csv('path_to_your_comments_file.csv')

# Ensure column names match exactly
all_threads_renamed = all_threads.rename(columns={
    'Thread.Relevancy': 'Thread_Relevancy'
})

# Create filtered_threads by removing Q1 and requiring more than 1 message
all_threads_renamed = create_relevancy_quartiles(all_threads_renamed)

filtered_threads = all_threads_renamed[
    (all_threads_renamed['Quartile'] != 'Q1') & 
    (all_threads_renamed['Number.of.Messages'] > 1)
].copy()

# Create filtered_comments by keeping only comments from the filtered threads
filtered_comments = all_comments[
    all_comments['Thread.ID'].isin(filtered_threads['Unique_ThreadID'])
].copy()

# Optional: Save the filtered datasets to CSV files
filtered_threads.to_csv('filtered_threads.csv', index=False)
filtered_comments.to_csv('filtered_comments.csv', index=False)

# Display the counts of filtered data
print(f"\nNumber of threads after filtering: {len(filtered_threads)}")
print(f"Number of comments after filtering: {len(filtered_comments)}")

# Display information about the datasets
print("\nFiltered Threads Info:")
print(filtered_threads.info())
print("\nFirst few rows of filtered_threads:")
print(filtered_threads.head())

print("\nFiltered Comments Info:")
print(filtered_comments.info())
print("\nFirst few rows of filtered_comments:")
print(filtered_comments.head())

# Display the distribution of messages in filtered threads
print("\nSummary of Number.of.Messages in filtered threads:")
print(filtered_threads['Number.of.Messages'].describe())
```
## Cool Initial Visualizations
```{python}

```

## Theme Prevelance Hypothesis
### LDA Topic Modeling
```{python LDA Topic Model + Betas, 6 Topics}
## Theme Prevelance Hypothesis
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx

# Define the Images and Tables directory
IT_directory = "/Users/emerson/Github/usenet_webpage/Images and Tables/"
images_dir = os.path.join(IT_directory, "Images")
tables_dir = os.path.join(IT_directory, "Tables")

# Create directories if they don't exist
os.makedirs(images_dir, exist_ok=True)
os.makedirs(tables_dir, exist_ok=True)

# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

def preprocess_text(text):
    """Preprocess text data"""
    # Convert to lowercase
    text = str(text).lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

def create_styled_html(results_df):
    """Create a styled HTML table from the results DataFrame"""
    # Create a copy of the DataFrame for styling
    styled_df = results_df.copy()
    
    # Define CSS styles
    styles = [
        dict(selector="caption", 
             props=[("caption-side", "top"),
                   ("font-size", "16px"),
                   ("font-weight", "bold"),
                   ("text-align", "center"),
                   ("padding", "10px")]),
        dict(selector="th", 
             props=[("font-size", "14px"),
                   ("text-align", "center"),
                   ("background-color", "#f0f0f0"),
                   ("padding", "8px")]),
        dict(selector="td", 
             props=[("padding", "8px"),
                   ("text-align", "left")]),
        dict(selector="tr:nth-child(even)",
             props=[("background-color", "#f9f9f9")]),
        dict(selector="table",
             props=[("border-collapse", "collapse"),
                   ("width", "100%"),
                   ("margin", "20px 0"),
                   ("font-family", "Arial, sans-serif")]),
        dict(selector="", 
             props=[("border", "1px solid #ddd")])
    ]
    
    # Apply styling
    styled_table = (styled_df.style
                   .set_table_styles(styles)
                   .set_caption("LDA Topic Analysis Results")
                   .format(precision=4)
                   .background_gradient(subset=[col for col in styled_df.columns if 'betas' in col],
                                     cmap='Blues')
                   .hide(axis='index'))
    
    # Add custom CSS for alternating topic columns
    for i in range(1, 7):
        styled_table = styled_table.set_properties(**{
            f'Topic_{i}_terms': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            },
            f'Topic_{i}_betas': {
                'background-color': f'rgba(240, 240, 240, {0.1 * i})',
                'border-right': '2px solid #ddd'
            }
        })
    
    return styled_table

def analyze_topics(filtered_comments, n_topics=6, n_top_words=20):
    """Perform topic analysis on the comments with adjusted parameters"""
    print("Starting topic analysis...")
    
    # Convert dates to datetime and sort
    filtered_comments['Date'] = pd.to_datetime(filtered_comments['Date'])
    filtered_comments = filtered_comments.sort_values('Date')
    
    # Check temporal distribution
    print("\nTemporal distribution of documents:")
    print(filtered_comments['Date'].dt.year.value_counts().sort_index())
    print("\nMonthly document counts:")
    print(filtered_comments.groupby(pd.Grouper(key='Date', freq='M')).size().sort_index())
    
    # Preprocess texts
    texts = [preprocess_text(text) for text in filtered_comments['Full.Text']]
    
    # Create document-term matrix with adjusted parameters
    print("Creating document-term matrix...")
    vectorizer = CountVectorizer(
        max_df=0.8,
        min_df=3,
        stop_words='english',
        max_features=5000
    )
    doc_term_matrix = vectorizer.fit_transform(texts)
    
    # Train LDA model with adjusted parameters
    print("Training LDA model...")
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=123,
        max_iter=50,
        learning_decay=0.7,
        batch_size=128,
        evaluate_every=5
    )
    
    # Fit the model
    lda.fit(doc_term_matrix)
    
    # Get feature names
    feature_names = vectorizer.get_feature_names_out()
    
    # Create top terms dataframe
    print("Extracting top terms...")
    top_terms_dict = {}
    
    for topic_idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words-1:-1]
        top_terms = [feature_names[i] for i in top_indices]
        top_betas = [topic[i] for i in top_indices]
        
        top_terms_dict[f'Topic_{topic_idx+1}_terms'] = top_terms
        top_terms_dict[f'Topic_{topic_idx+1}_betas'] = [round(beta, 4) for beta in top_betas]
    
    # Create DataFrame
    results_df = pd.DataFrame(top_terms_dict)
    
    # Add temporal information
    doc_topics = lda.transform(doc_term_matrix)
    document_results = pd.DataFrame(doc_topics)
    document_results.columns = [f'Topic_{i+1}' for i in range(n_topics)]
    document_results['Date'] = filtered_comments['Date']
    
    # Save table to Tables directory
    tables_path = os.path.join(tables_dir, "general_topics_lda_analysis.html")
    
    # Create HTML content
    html_content = f"""
    <html>
    <head>
        <style>
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }}
            th {{
                background-color: #f5f5f5;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            caption {{
                font-size: 1.2em;
                margin-bottom: 10px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <table>
            <caption>General LDA Topic Analysis Results</caption>
            {results_df.to_html(index=False)}
        </table>
    </body>
    </html>
    """
    
    with open(tables_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Create visualization
    print("Creating visualization...")
    plt.figure(figsize=(15, 10))
    for topic_idx in range(n_topics):
        plt.subplot(2, 3, topic_idx + 1)
        top_terms = top_terms_dict[f'Topic_{topic_idx+1}_terms'][:10]
        top_betas = top_terms_dict[f'Topic_{topic_idx+1}_betas'][:10]
        plt.barh(range(len(top_terms)), top_betas)
        plt.yticks(range(len(top_terms)), top_terms)
        plt.title(f'Topic {topic_idx + 1}')
    
    plt.tight_layout()
    
    # Save plot to Images directory
    plot_path = os.path.join(images_dir, "general_topics_visualization.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print("Analysis complete! Results saved to:")
    print(f"- Table: {tables_path}")
    print(f"- Plot: {plot_path}")
    
    return results_df, lda, vectorizer, doc_term_matrix, document_results

# Run the analysis
try:
    print("\nStarting topic analysis...")
    results_df, lda_model, vectorizer, doc_term_matrix, document_results = analyze_topics(filtered_comments)
    print("\nAnalysis completed successfully!")
    
    # Display top terms for each topic
    print("\nTop terms for each topic:")
    print(results_df.head())
    
except Exception as e:
    print(f"An error occurred: {str(e)}")
    raise
```
### Co-occurance Network
```{python co-occurance network}
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Polygon
from scipy.spatial import ConvexHull

def create_static_network(doc_term_matrix, lda_model, vectorizer, output_path='topic_network_visualization.png'):
    """Create static co-occurrence network visualization with topic regions"""
    print("Creating static co-occurrence network...")
    
    # Get the document-term matrix as array
    dtm_array = doc_term_matrix.toarray()
    co_occurrence = np.dot(dtm_array.T, dtm_array)
    
    # Create binary DTM
    binary_dtm = (dtm_array > 0).astype(int)
    term_frequency = np.sum(binary_dtm, axis=0)
    
    # Filter for terms that appear in the top terms from LDA results
    top_terms = []
    for i in range(6):
        col_name = f'Topic_{i+1}_terms'
        top_terms.extend(results_df[col_name].head(15).tolist())  # Limit to top 15 terms per topic
    top_terms = list(set(top_terms))
    
    # Get indices of these terms
    feature_names = vectorizer.get_feature_names_out()
    filtered_terms = [i for i, term in enumerate(feature_names) if term in top_terms]
    filtered_co_occurrence = co_occurrence[filtered_terms][:, filtered_terms]
    terms = feature_names[filtered_terms]
    
    # Create networkx graph
    G = nx.Graph()
    
    # Add edges with weights
    for i in range(len(terms)):
        for j in range(i + 1, len(terms)):
            if filtered_co_occurrence[i, j] > 0:
                G.add_edge(terms[i], terms[j], weight=filtered_co_occurrence[i, j])
    
    # Get topic assignments for terms
    term_topic_assignment = []
    for term in terms:
        term_topics = []
        for i in range(6):
            terms_col = f'Topic_{i+1}_terms'
            betas_col = f'Topic_{i+1}_betas'
            if term in results_df[terms_col].values:
                idx = results_df[terms_col][results_df[terms_col] == term].index[0]
                term_topics.append((i, results_df[betas_col].iloc[idx]))
        if term_topics:
            term_topic_assignment.append(max(term_topics, key=lambda x: x[1])[0])
        else:
            term_topic_assignment.append(0)
    
    # Set up the plot with white background
    plt.figure(figsize=(20, 20), facecolor='white')
    
    # Create layout with more spacing
    pos = nx.spring_layout(G, k=4, iterations=100, seed=42)
    
    # Convert pos dict to numpy arrays for each topic
    topic_positions = {i: [] for i in range(6)}
    for node, position in pos.items():
        idx = list(terms).index(node)
        topic = term_topic_assignment[idx]
        topic_positions[topic].append(position)
    
    # Draw topic regions
    colors = sns.color_palette("Set2", n_colors=6)
    alpha_fill = 0.2
    alpha_edge = 0.5
    
    # Draw convex hulls for each topic
    for topic in range(6):
        if len(topic_positions[topic]) > 2:  # Need at least 3 points for convex hull
            points = np.array(topic_positions[topic])
            hull = ConvexHull(points)
            hull_points = points[hull.vertices]
            plt.fill(hull_points[:, 0], hull_points[:, 1], 
                    alpha=alpha_fill, color=colors[topic])
            plt.plot(hull_points[:, 0], hull_points[:, 1], 
                    color=colors[topic], alpha=alpha_edge)
    
    # Draw edges
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    max_edge_weight = max(edge_weights) if edge_weights else 1
    edge_widths = [0.3 + (w / max_edge_weight) for w in edge_weights]
    nx.draw_networkx_edges(G, pos, alpha=0.1, width=edge_widths, edge_color='gray')
    
    # Draw nodes
    node_sizes = [np.log1p(term_frequency[filtered_terms][i]) * 500 for i in range(len(terms))]
    for topic in range(6):
        # Get nodes for this topic
        topic_nodes = [node for node, idx in enumerate(term_topic_assignment) if idx == topic]
        if topic_nodes:
            nx.draw_networkx_nodes(G, pos, 
                                 nodelist=[terms[i] for i in topic_nodes],
                                 node_color=[colors[topic]],
                                 node_size=[node_sizes[i] for i in topic_nodes],
                                 alpha=0.7)
    
    # Add labels with better spacing and formatting
    labels = {node: node for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels,
                          font_size=10,
                          font_weight='bold',
                          bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.5))
    
    # Add legend
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                label=f'Topic {i+1}',
                                markerfacecolor=colors[i], markersize=15)
                      for i in range(6)]
    plt.legend(handles=legend_elements, loc='upper right',
              title='Topics', title_fontsize=12, fontsize=10)
    
    # Remove axes
    plt.axis('on')
    
    # Save with high quality
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return G

# Create the visualization using existing data
try:
    print("\nCreating static network visualization...")
    G = create_static_network(
        doc_term_matrix=doc_term_matrix,
        lda_model=lda_model,
        vectorizer=vectorizer,
        output_path=os.path.join(output_directory, "Images and Tables/Images/topic_network_visualization.png")
    )

    print("\nNetwork visualization completed successfully!")
    print("Check 'topic_network_visualization.png' for the static network visualization.")

except Exception as e:
    print(f"An error occurred while creating the network: {str(e)}")
```

## Emotional Tone Hypothesis
```{python}
usenet_threads_all = pd.read_csv(os.path.join(threads_directory, "combined_threads_cleaned_82TO86.csv"))
usenet_comments_all = pd.read_csv(os.path.join(comments_directory, "combined_comments_cleaned_82TO86.csv"))
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
from itertools import combinations

def create_author_network(filtered_comments):
    """Create a network of authors based on thread co-participation"""
    print("Creating author network...")
    
    edges = []
    weights = {}
    
    # Group comments by Thread ID
    thread_groups = filtered_comments.groupby('Thread.ID')
    
    # Create edges for each thread
    for thread_id, group in thread_groups:
        if thread_id and len(group) > 1:
            authors = group['Author'].unique()
            
            for source, target in combinations(authors, 2):
                if source != target:
                    edge = tuple(sorted([source, target]))
                    
                    if edge in weights:
                        weights[edge] += 1
                    else:
                        weights[edge] = 1
                        edges.append({'source': edge[0], 'target': edge[1]})
    
    edge_df = pd.DataFrame(edges)
    if not edges:  # Check if edges list is empty
        print("Warning: No edges created. Check if there are multiple authors per thread.")
        return pd.DataFrame(columns=['source', 'target', 'weight'])
        
    edge_df['weight'] = edge_df.apply(lambda row: weights[tuple(sorted([row['source'], row['target']]))], axis=1)
    
    print(f"Created network with {len(edge_df)} edges")
    return edge_df

def identify_influential_authors(edge_df, n_authors=15):
    """Identify the most influential authors based on network metrics"""
    print("Identifying influential authors...")
    
    if edge_df.empty:
        print("Warning: Empty edge dataframe. Cannot identify influential authors.")
        return pd.DataFrame(columns=['Author', 'Influence'])
        
    G = nx.from_pandas_edgelist(edge_df, 'source', 'target', 'weight')
    degree_dict = dict(G.degree(weight='weight'))
    
    author_influence = pd.DataFrame({
        'Author': list(degree_dict.keys()),
        'Influence': list(degree_dict.values())
    })
    
    top_influential_authors = author_influence.sort_values(
        'Influence', ascending=False
    ).head(n_authors).reset_index(drop=True)
    
    print("\nTop influential authors identified:")
    print(top_influential_authors)
    return top_influential_authors

def create_sentiment_visualization():
    """Create time series visualization of sentiment scores using existing dataframes"""
    print("Creating sentiment visualization...")
    
    # Create a figure
    plt.figure(figsize=(15, 8))
    
    # Calculate monthly averages for each dataset
    def calculate_monthly_sentiment(df):
        df['Date'] = pd.to_datetime(df['Date'])
        return df.groupby(pd.Grouper(key='Date', freq='M'))['SentimentScore'].mean().reset_index()
    
    # Get monthly averages for each dataset
    monthly_sentiment_usenet = calculate_monthly_sentiment(usenet_comments_all)
    monthly_sentiment_aids = calculate_monthly_sentiment(filtered_comments)
    
    # Get influential authors
    edge_df = create_author_network(filtered_comments)
    top_authors = identify_influential_authors(edge_df)
    
    # Calculate sentiment for influential authors
    influential_comments = filtered_comments[filtered_comments['Author'].isin(top_authors['Author'])]
    monthly_sentiment_influential = calculate_monthly_sentiment(influential_comments)
    
    # Plot the lines
    plt.plot(monthly_sentiment_usenet['Date'], 
             monthly_sentiment_usenet['SentimentScore'],
             label='Dataset One', linewidth=2, color='#1f77b4')
    plt.plot(monthly_sentiment_aids['Date'],
             monthly_sentiment_aids['SentimentScore'],
             label='Dataset Three', linewidth=2, color='#ff7f0e')
    plt.plot(monthly_sentiment_influential['Date'],
             monthly_sentiment_influential['SentimentScore'],
             label='Influential Authors', linewidth=2, color='#2ca02c')
    
    # Add key events annotations
    key_events = {
        '1982-05-11': "Term 'AIDS' Introduced",
        '1983-09-30': "CDC AIDS Guidelines",
        '1984-04-23': "HHS HIV/AIDS",
        '1984-10-01': "First HIV Blood Test",
        '1985-03-02': "First HIV Test Approved",
        '1985-07-25': "Rock Hudson's Diagnosis",
        '1985-10-02': "HIV Transmission Routes",
        '1986-02-01': "'HIV' Renamed",
        '1986-08-14': "AZT Approved"
    }
    
    # Add event annotations
    y_min = plt.ylim()[0]
    for date, event in key_events.items():
        date_obj = pd.to_datetime(date)
        plt.axvline(x=date_obj, color='gray', linestyle='--', alpha=0.5)
        plt.text(date_obj, y_min, event,
                rotation=90, verticalalignment='bottom',
                horizontalalignment='right', fontsize=8)
    
    # Customize plot
    plt.title('Time Series of Average Sentiment Scores Over Time', fontsize=14, pad=20)
    plt.xlabel('Month', fontsize=12)
    plt.ylabel('Average Sentiment Score', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),
              ncol=3, frameon=False)
    
    plt.gca().set_facecolor('white')
    plt.gcf().set_facecolor('white')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Save the plot
    output_path = os.path.join(output_directory, "Images and Tables/Images/sentiment_time_series.png")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"Visualization saved to: {output_path}")
    return monthly_sentiment_usenet, monthly_sentiment_aids, monthly_sentiment_influential

# Run the analysis
try:
    print("\nStarting sentiment analysis...")
    results = create_sentiment_visualization()
    print("\nAnalysis completed successfully!")
    
    # Print summary statistics
    print("\nSentiment Statistics:")
    for name, data in zip(['Dataset One', 'Dataset Three', 'Influential Authors'], results):
        print(f"\n{name}:")
        print(f"Average sentiment: {data['SentimentScore'].mean():.3f}")
        print(f"Number of months: {len(data)}")
        
except Exception as e:
    print(f"An error occurred: {str(e)}")
```

## Author Impact Hypothesis
### Topic Similarity
```{python Topic Similarity}
def compare_topics(general_results, influential_results):
    """Compare topics between general and influential author analyses and create visualizations"""
    print("\nComparing topic distributions...")
    
    topic_similarities = []
    
    # Calculate similarities
    for i in range(1, 7):
        general_terms = set(general_results[f'Topic_{i}_terms'])
        
        for j in range(1, 7):
            influential_terms = set(influential_results[f'Topic_{j}_terms'])
            similarity = len(general_terms.intersection(influential_terms)) / len(general_terms.union(influential_terms))
            
            topic_similarities.append({
                'General_Topic': i,
                'Influential_Topic': j,
                'Similarity': similarity
            })
    
    comparison_df = pd.DataFrame(topic_similarities)
    
    # Create heatmap
    plt.figure(figsize=(10, 8))
    similarity_matrix = comparison_df.pivot(
        index='General_Topic', 
        columns='Influential_Topic', 
        values='Similarity'
    )
    
    sns.heatmap(similarity_matrix, 
                annot=True, 
                fmt='.3f', 
                cmap='YlOrRd',
                vmin=0, 
                vmax=1,
                square=True,
                cbar_kws={'label': 'Jaccard Similarity'})
    
    plt.title('Topic Similarity Heatmap')
    plt.xlabel('Influential Authors Topics')
    plt.ylabel('General Topics')
    
    # Save the heatmap
    heatmap_path = os.path.join(images_dir, "topic_similarity_heatmap.png")
    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Find best matching topics
    best_matches = []
    used_influential_topics = set()
    
    for general_topic in range(1, 7):
        topic_similarities = comparison_df[comparison_df['General_Topic'] == general_topic]
        remaining_similarities = topic_similarities[~topic_similarities['Influential_Topic'].isin(used_influential_topics)]
        
        if not remaining_similarities.empty:
            best_match = remaining_similarities.loc[remaining_similarities['Similarity'].idxmax()]
            best_matches.append({
                'Influential_Topic': int(best_match['Influential_Topic']),
                'General_Topic': general_topic,
                'Similarity': best_match['Similarity']
            })
            used_influential_topics.add(best_match['Influential_Topic'])
    
    best_matches_df = pd.DataFrame(best_matches)
    
    # Create bar plot
    plt.figure(figsize=(12, 6))
    bars = plt.bar(best_matches_df['Influential_Topic'], 
                  best_matches_df['Similarity'],
                  color=[plt.cm.Set2(i) for i in best_matches_df['General_Topic'] / 7])
    
    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}',
                ha='center', va='bottom')
    
    # Customize plot
    plt.title('Topic Similarity Between Main and Influential Authors')
    plt.xlabel('Influential Authors\' Topic')
    plt.ylabel('Jaccard Similarity')
    plt.ylim(0, max(best_matches_df['Similarity']) * 1.1)  # Add 10% padding above highest bar
    
    # Add legend for matching main topics
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=plt.cm.Set2(i/7), 
                           label=f'Topic {i}') 
                      for i in best_matches_df['General_Topic']]
    plt.legend(handles=legend_elements, title='Matching Main Topic', 
              bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # Save the bar plot
    barplot_path = os.path.join(images_dir, "topic_similarity_comparison.png")
    plt.savefig(barplot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Print similarity statistics
    print("\nTopic Similarity Statistics:")
    print(f"Average similarity: {comparison_df['Similarity'].mean():.3f}")
    print(f"Maximum similarity: {comparison_df['Similarity'].max():.3f}")
    print(f"Minimum similarity: {comparison_df['Similarity'].min():.3f}")
    
    return comparison_df, best_matches_df

# Example usage:
if __name__ == "__main__":
    try:
        print("\nRunning topic similarity analysis...")
        comparison_df, best_matches_df = compare_topics(results_df, influential_results_df)
        print("\nAnalysis completed successfully!")
        print("\nBest topic matches:")
        print(best_matches_df.to_string(index=False))
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise
```

### Keyword Adoption
```{python Keyword Adoption}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

def preprocess_text(text):
    """Clean and tokenize text"""
    # Convert to lowercase and remove special characters
    text = str(text).lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]
    return tokens

def get_top_keywords(df, text_column, n_keywords=20):
    """Get top keywords from the dataset"""
    # Process all texts
    all_words = []
    for text in df[text_column]:
        tokens = preprocess_text(text)
        all_words.extend(tokens)
    
    # Count words
    word_counts = Counter(all_words)
    top_words = pd.DataFrame({
        'word': list(word_counts.keys()),
        'count': list(word_counts.values())
    }).sort_values('count', ascending=False)
    
    return top_words.head(n_keywords)['word'].tolist()

def calculate_keyword_prevalence(df, keywords, text_column='Full.Text'):
    """Calculate monthly keyword prevalence"""
    # Ensure date is datetime
    df['Date'] = pd.to_datetime(df['Date'])
    
    # Process texts and count keywords by month
    monthly_counts = []
    
    for idx, row in df.iterrows():
        tokens = preprocess_text(row[text_column])
        # Count keywords in this document
        keyword_counts = Counter(word for word in tokens if word in keywords)
        # Add date information
        for word, count in keyword_counts.items():
            monthly_counts.append({
                'Date': row['Date'],
                'word': word,
                'count': count
            })
    
    # Convert to DataFrame
    counts_df = pd.DataFrame(monthly_counts)
    
    # Group by month and word
    monthly_prevalence = counts_df.groupby([
        pd.Grouper(key='Date', freq='M'),
        'word'
    ])['count'].sum().reset_index()
    
    # Calculate prevalence
    total_counts = monthly_prevalence.groupby('Date')['count'].sum().reset_index()
    monthly_prevalence = monthly_prevalence.merge(total_counts, on='Date', suffixes=('', '_total'))
    monthly_prevalence['prevalence'] = monthly_prevalence['count'] / monthly_prevalence['count_total']
    
    return monthly_prevalence

def plot_keyword_adoption(influential_prevalence, overall_prevalence, save_path):
    """Create faceted plot of keyword adoption patterns"""
    # Combine the data
    influential_prevalence['Group'] = 'Influential Authors'
    overall_prevalence['Group'] = 'Overall Discussion'
    combined_data = pd.concat([influential_prevalence, overall_prevalence])
    
    # Get unique keywords
    keywords = combined_data['word'].unique()
    n_keywords = len(keywords)
    
    # Calculate grid dimensions
    n_cols = 4
    n_rows = int(np.ceil(n_keywords / n_cols))
    
    # Create figure
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2.5))
    fig.suptitle('Keyword Adoption Over Time: Influential Authors vs Overall Discussion')
    
    # Flatten axes array for easier iteration
    axes = axes.flatten()
    
    # Plot each keyword
    for idx, keyword in enumerate(sorted(keywords)):
        keyword_data = combined_data[combined_data['word'] == keyword]
        
        # Plot lines for both groups
        for group in ['Influential Authors', 'Overall Discussion']:
            group_data = keyword_data[keyword_data['Group'] == group]
            color = '#ff7f0e' if group == 'Influential Authors' else '#1f77b4'
            axes[idx].plot(group_data['Date'], group_data['prevalence'],
                         label=group, color=color)
        
        # Customize subplot
        axes[idx].set_title(keyword)
        axes[idx].tick_params(axis='x', rotation=45)
        axes[idx].grid(True, alpha=0.3)
    
    # Remove extra subplots
    for idx in range(len(keywords), len(axes)):
        fig.delaxes(axes[idx])
    
    # Add legend
    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, .96),
              ncol=2, frameon=False)
    
    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(top=.9)
    
    # Save plot
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

# Run the analysis
if __name__ == "__main__":
    try:
        print("\nStarting keyword adoption analysis...")
        
        # Get top keywords from influential authors
        top_keywords = get_top_keywords(filtered_comments, 'Full.Text')
        
        # Calculate keyword prevalence
        influential_prevalence = calculate_keyword_prevalence(
            top_authors_comments, top_keywords)
        overall_prevalence = calculate_keyword_prevalence(
            filtered_comments, top_keywords)
        
        # Create visualization
        output_path = os.path.join(images_dir, "keyword_adoption_over_time.png")
        plot_keyword_adoption(influential_prevalence, overall_prevalence, output_path)
        
        print("\nAnalysis completed successfully!")
        print(f"Visualization saved to: {output_path}")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise
```
### Influence Propagation
```{python}
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import os

# Identify influential authors' threads
influential_threads = filtered_comments[
    filtered_comments['Author'].isin(top_authors['Author'])
]['Thread.ID'].unique()

# Create edges based on who commented after influential authors
# Select comments in threads where influential authors participated
comments_in_influential_threads = filtered_comments[
    filtered_comments['Thread.ID'].isin(influential_threads)
]

# Sort comments by Thread ID and Date
comments_in_influential_threads = comments_in_influential_threads.sort_values(['Thread.ID', 'Date'])

# Create edges: for each thread, find who commented after influential authors
edges_list = []

# Group by Thread ID
for thread_id, group in comments_in_influential_threads.groupby('Thread.ID'):
    authors = group['Author'].tolist()
    dates = group['Date'].tolist()
    
    # Iterate over comments in the thread
    for i in range(1, len(authors)):
        current_author = authors[i]
        previous_author = authors[i - 1]
        
        # Check if the previous author is an influential author and current author is different
        if previous_author in top_authors['Author'].values and current_author != previous_author:
            edges_list.append({'from': previous_author, 'to': current_author})

# Convert edges to DataFrame
edges_df = pd.DataFrame(edges_list)

# Group edges and calculate weights
edges_df = edges_df.groupby(['from', 'to']).size().reset_index(name='weight')

# Filter for stronger connections (edges with weight above the 25th percentile)
threshold = edges_df['weight'].quantile(0.25)
edges_df = edges_df[edges_df['weight'] > threshold]

# Create directed graph
G = nx.from_pandas_edgelist(edges_df, 'from', 'to', ['weight'], create_using=nx.DiGraph())

# Calculate node sizes based on in-degree
node_sizes = dict(G.in_degree(weight='weight'))

# Identify top 10 influential nodes based on in-degree
top_nodes = sorted(node_sizes.items(), key=lambda x: x[1], reverse=True)[:10]
top_node_names = [node for node, degree in top_nodes]

# Prepare node colors: red for influential authors, light blue for others
node_colors = []
for node in G.nodes():
    if node in top_authors['Author'].values:
        node_colors.append('red')
    else:
        node_colors.append('lightblue')

# Prepare node sizes for plotting (adjust scaling as needed)
node_size_values = [node_sizes.get(node, 1) * 100 for node in G.nodes()]

# Create layout
pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

# Create plot
plt.figure(figsize=(20, 16))

# Draw edges with weights
edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
nx.draw_networkx_edges(
    G, pos,
    edge_color='gray',
    alpha=0.2,
    arrows=True,
    arrowsize=10,
    width=edge_weights,
    arrowstyle='-|>'
)

# Draw nodes
nx.draw_networkx_nodes(
    G, pos,
    node_color=node_colors,
    node_size=node_size_values
)

# Draw labels for top nodes
labels = {node: node if node in top_node_names else '' for node in G.nodes()}
nx.draw_networkx_labels(
    G, pos,
    labels=labels,
    font_size=10,
    font_weight='bold'
)

# Remove axes
plt.axis('off')

# Save the plot
output_file_path = os.path.join(
    output_directory, "Images and Tables", "Images", "plot_influence_propagation_network_improved.png"
)
os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
plt.savefig(output_file_path, dpi=300, bbox_inches='tight')
plt.close()

print(f"Influence propagation network plot saved to {output_file_path}")
```
"0","# Prepare the corpus for influential authors"
"0","influential_corpus <- Corpus(VectorSource(top_authors_comments$Full.Text))"
"0","influential_corpus <- tm_map(influential_corpus, content_transformer(tolower))"
"2","Warning in tm_map.SimpleCorpus(influential_corpus, content_transformer(tolower)) :"
"2","
 "
"2"," transformation drops documents
"
"0","influential_corpus <- tm_map(influential_corpus, removePunctuation)"
"2","Warning in tm_map.SimpleCorpus(influential_corpus, removePunctuation) :"
"2","
 "
"2"," transformation drops documents
"
"0","influential_corpus <- tm_map(influential_corpus, removeNumbers)"
"2","Warning in tm_map.SimpleCorpus(influential_corpus, removeNumbers) :"
"2","
 "
"2"," transformation drops documents
"
"0","influential_corpus <- tm_map(influential_corpus, removeWords, stopwords(""english""))"
"2","Warning in tm_map.SimpleCorpus(influential_corpus, removeWords, stopwords(""english"")) :"
"2","
 "
"2"," transformation drops documents
"
"0","influential_corpus <- tm_map(influential_corpus, stripWhitespace)"
"2","Warning in tm_map.SimpleCorpus(influential_corpus, stripWhitespace) :"
"2","
 "
"2"," transformation drops documents
"
"0","# Create DTM for influential authors"
"0","influential_dtm <- DocumentTermMatrix(influential_corpus)"
"0","influential_dtm <- removeSparseTerms(influential_dtm, 0.99)"
"0",""
"0","# LDA for influential authors"
"0","set.seed(123)"
"0","influential_lda <- LDA(influential_dtm, k = 6, control = list(seed = 123))"
"0",""
"0","# Extract top terms for each topic"
"0","influential_topics <- tidy(influential_lda, matrix = ""beta"") %>%"
"0","  group_by(topic) %>%"
"0","  top_n(10, beta) %>%"
"0","  ungroup() %>%"
"0","  arrange(topic, -beta)"
"0",""
"0","print(influential_topics)"

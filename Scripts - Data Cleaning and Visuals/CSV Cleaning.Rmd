---
title: "Usenet Project - CSV Cleaning"
author: "Emerson Johnston"
lastmodifeddate: "2024-08-07"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r Old Read and Merge, eval=FALSE, include=FALSE}
netmed_threads <- read.csv(file.path(threads_directory, "netmed_threads.csv")
netmotss_threads <- read.csv(file.path(threads_directory, "netmotss_threads.csv")
netnews_threads <- read.csv(file.path(threads_directory, "netnews_threads.csv")
netpolitics_threads <- read.csv(file.path(threads_directory, "netpolitics_threads.csv")
netreligion_threads <- read.csv(file.path(threads_directory, "netreligion_threads.csv")
netsingles_threads <- read.csv(file.path(threads_directory, "netsingles_threads.csv")

netmed_comments <- read.csv(file.path(comments_directory, "netmed_comments.csv")
netmotss_comments <- read.csv(file.path(comments_directory, "netmotss_comments.csv")
netnews_comments <- read.csv(file.path(comments_directory, "netnews_comments.csv")
netpolitics_comments <- read.csv(file.path(comments_directory, "netpolitics_comments.csv")
netreligion_comments <- read.csv(file.path(comments_directory, "netreligion_comments.csv")
netsingles_comments <- read.csv(file.path(comments_directory, "netsingles_comments.csv")

netmed_threads$newsgroup <- "netmed"
netmotss_threads$newsgroup <- "netmotss"
netnews_threads$newsgroup <- "netnews"
netpolitics_threads$newsgroup <- "netpolitics"
netreligion_threads$newsgroup <- "netreligion"
netsingles_threads$newsgroup <- "netsingles"

all_threads <- rbind(netmed_threads, netmotss_threads, netnews_threads, netpolitics_threads, netreligion_threads, netsingles_threads)

netmed_comments$newsgroup <- "netmed"
netmotss_comments$newsgroup <- "netmotss"
netnews_comments$newsgroup <- "netnews"
netpolitics_comments$newsgroup <- "netpolitics"
netreligion_comments$newsgroup <- "netreligion"
netsingles_comments$newsgroup <- "netsingles"

all_comments <- rbind(netmed_comments, netmotss_comments, netnews_comments, netpolitics_comments, netreligion_comments, netsingles_comments)

threads_directory <- "CSV Files/Threads"
comments_directory <- "CSV Files/Comments"

write.csv(all_threads, file.path(threads_directory, "combined_threads.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "combined_comments.csv"), row.names = FALSE)
```

# Maintainence
```{r Reset and Set Working Directory}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_AIDs_discourses_webpage')
```

```{r Load Libraries, Directories, and Datasets}
# Load Libraries
library(tidyverse)
library(readr)
library(webshot)
library(htmltools)
library(ggplot2)
library(dplyr)
library(syuzhet)
library(sjPlot)

# Load Directories
output_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/"
threads_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Threads"
comments_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Comments"

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "combined_threads.csv"))
all_comments <- read.csv(file.path(comments_directory, "combined_comments.csv"))
```

```{r Dataset Cleaning}
# Assign unique identifiers to newsgroups
newsgroup_ids <- c("netmed" = "NG01", "netmotss" = "NG02", "netnews" = "NG03",
                   "netpolitics" = "NG04", "netreligion" = "NG05", "netsingles" = "NG06")

# Update the all_threads dataset
all_threads <- all_threads %>%
  # Map newsgroup names to IDs and keep the original newsgroup column
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         # Generate Unique Thread ID
         Unique_ThreadID = paste(newsgroup_ID, ThreadID, sep = "_")) %>%
  # Reorder columns: Unique_ThreadID, newsgroup, newsgroup_ID, followed by the rest
  select(Unique_ThreadID, newsgroup, newsgroup_ID, everything()) %>%
  # Rename ThreadID to NG_Relative_ThreadID
  rename(NG_Relative_ThreadID = ThreadID)

# Update the all_comments dataset
all_comments <- all_comments %>%
  # Map newsgroup names to IDs and keep the original newsgroup column
  mutate(newsgroup_ID = factor(newsgroup, levels = names(newsgroup_ids), labels = newsgroup_ids),
         # Generate Unique Comment ID
         Unique_CommentID = paste(newsgroup_ID, Unique.Comment.ID, sep = "_"),
         # Retain old Unique.Comment.ID as NG_Relative_CommentID
         NG_Relative_CommentID = Unique.Comment.ID,
         # Retain old Thread.ID as NG_Relative_ThreadID
         NG_Relative_ThreadID = Thread.ID, 
         # Generate new newsgroup.thread.id
         Thread.ID = paste(newsgroup_ID, Thread.ID, sep = "_"),
         # Retain old Comment.ID as Thread_Relative_CommentID
         Thread_Relative_CommentID = Comment.ID) %>%
  # Reorder columns: Unique_CommentID, newsgroup, newsgroup_ID, followed by the rest
  select(Unique_CommentID, newsgroup, newsgroup_ID, Thread.ID, NG_Relative_CommentID, NG_Relative_ThreadID, Thread_Relative_CommentID, everything()) %>%
  # Drop the old Unique.Comment.ID and Comment.ID columns
  select(-Unique.Comment.ID, -Comment.ID)

# Function to clean the datetime strings
clean_datetime <- function(dt_str) {
  gsub("[^[:alnum:] [:punct:]]", "", dt_str)
}

# Clean the Date.and.Time column
all_comments$Date.and.Time <- sapply(all_comments$Date.and.Time, clean_datetime)

# Convert the cleaned Date.and.Time to POSIXct
all_comments$Date.and.Time <- as.POSIXct(all_comments$Date.and.Time, format = "%b %d, %Y, %I:%M:%S%p")

all_comments$Hour <- as.numeric(format(all_comments$Date.and.Time, "%H"))
all_comments$Date <- as.Date(all_comments$Date.and.Time)

all_threads$Date <- as.Date(all_threads$Date, format = "%m/%d/%y")

# Display the updated dataframes (optional)
head(all_threads)
head(all_comments)
```

```{r Run and Add Sentiment Scores} 
all_comments <- all_comments %>% mutate(SentimentScore = get_sentiment(Full.Text, method = "afinn"))
```

```{r Save Cleaned CSVs to File}
write.csv(all_threads, file.path(threads_directory, "combined_threads_cleaned.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "combined_comments_cleaned.csv"), row.names = FALSE)
```

Filtering to 1982-1986
```{r Filtering to 1982-1986}
all_comments <- all_comments %>%
  filter(Date < as.Date("1987-01-01")) %>% filter(Date > as.Date("1982-01-01"))

all_threads <- all_threads %>%
  filter(Date < as.Date("1987-01-01")) %>% filter(Date > as.Date("1982-01-01"))

write.csv(all_threads, file.path(threads_directory, "combined_threads_cleaned_82TO86.csv"), row.names = FALSE)
write.csv(all_comments, file.path(comments_directory, "combined_comments_cleaned_82TO86.csv"), row.names = FALSE)
```

# Table and Figures
```{r Descriptive Statistics}
# Calculate statistics using dplyr
summary_df <- all_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Threads = n_distinct(Thread.ID),
    Comments = n(),
    Authors = n_distinct(Author),
    Avg_Comments_Per_Thread = Comments / Threads,
    Avg_Sentiment_Score = mean(SentimentScore, na.rm = TRUE)
  )

# Add a totals row
totals <- summary_df %>%
  summarize(
    newsgroup = "Total",
    Threads = sum(Threads),
    Comments = sum(Comments),
    Authors = n_distinct(all_comments$Author),
    Avg_Comments_Per_Thread = sum(Comments) / sum(Threads),
    Avg_Sentiment_Score = mean(all_comments$SentimentScore, na.rm = TRUE)
  )

# Combine the summary with the totals row
summary_df <- bind_rows(summary_df, totals)

# Print the summary data frame with totals
tab_df(summary_df, file = paste0(output_directory, "images and tables/table_descriptive_statistics_full.html"))
```

# Additional Filtering
```{r Filter by Keyword and Add Relevancy Scores}
# List of refined keywords and phrases (broad keywords removed)
keywords <- c("aids", "htlv", "hiv", "acquired immune deficiency syndrome", "human immunodeficiency virus",
              "gay plague", "gay cancer", "kaposi's sarcoma", "pneumocystis pneumonia",
              "homosexual disease", "gay disease", "gay fear", "fear of gay", "fear of homosexual",
              "sexual orientation disease", "virus", "sex", "promiscuity", "patients")

# Function to calculate relevancy score
calculate_relevancy <- function(text) {
  # Convert text to lowercase
  text <- tolower(text)
  # Check for the presence of each keyword
  keyword_matches <- sapply(keywords, function(keyword) grepl(keyword, text))
  # Calculate relevancy score as the number of keywords present
  relevancy_score <- sum(keyword_matches)
  return(relevancy_score)
}

# Step 1: Identify threads with at least one comment containing a keyword
all_comments_aids <- all_comments %>%
  rowwise() %>%
  mutate(Contains_Keyword = any(sapply(keywords, function(keyword) grepl(keyword, tolower(Full.Text)))))

# Filter threads containing at least one keyword
all_threads_aids <- all_comments_aids %>%
  filter(Contains_Keyword) %>%
  distinct(Thread.ID)

# Step 2: Fetch all comments from threads with at least one keyword-containing comment
comments_in_keyword_threads <- all_comments %>%
  filter(Thread.ID %in% all_threads_aids$Thread.ID)

# Step 3: Calculate relevancy score for each comment and add it to the dataframe
comments_in_keyword_threads <- comments_in_keyword_threads %>%
  rowwise() %>%
  mutate(Relevancy = calculate_relevancy(Full.Text))

# Save the comments dataframe to CSV
write.csv(comments_in_keyword_threads, file.path(comments_directory, "aids_related_comments_82TO86.csv"), row.names = FALSE)

# Step 4: Calculate the average relevancy score for each thread
thread_relevancy_scores <- comments_in_keyword_threads %>%
  group_by(Thread.ID) %>%
  summarize(Thread_Relevancy = mean(Relevancy, na.rm = TRUE))

# Merge thread relevancy scores with the original all_threads dataframe and filter to keep only relevant threads
threads_summary_df <- all_threads %>%
  filter(Unique_ThreadID %in% all_threads_aids$Thread.ID) %>%
  left_join(thread_relevancy_scores, by = c("Unique_ThreadID" = "Thread.ID"))

# Save the threads dataframe to CSV
write.csv(threads_summary_df, file.path(threads_directory, "aids_related_threads_82TO86.csv"), row.names = FALSE)
```

```{r Keyword Descriptive Statistics}
# Step 5: Group by newsgroup and calculate required statistics, including thread relevancy
keywords_summary_df <- comments_in_keyword_threads %>%
  group_by(newsgroup) %>%
  summarize(
    Unique_Authors = n_distinct(Author),
    Comments_with_Keywords = sum(Relevancy > 0),
    Conversations_Mentioned_In = n_distinct(Thread.ID),
    Total_Comments_In_Threads = n(),
    Percent_Total_Comments_w_Keyword = (Comments_with_Keywords / Total_Comments_In_Threads) * 100,
    Average_Relevancy_Score = mean(Relevancy, na.rm = TRUE)
  )

# Merge thread relevancy averages back to the summary dataframe
threads_summary_df_grouped <- threads_summary_df %>%
  group_by(newsgroup) %>%
  summarize(Average_Thread_Relevancy = mean(Thread_Relevancy, na.rm = TRUE))

# Combine comment and thread summaries
keywords_summary_df <- keywords_summary_df %>%
  left_join(threads_summary_df_grouped, by = "newsgroup")

total_unique_authors <- n_distinct(comments_in_keyword_threads$Author)

# Add a totals row
totals <- keywords_summary_df %>%
  summarize(
    newsgroup = "Total",
    Comments_with_Keywords = sum(Comments_with_Keywords),
    Conversations_Mentioned_In = sum(Conversations_Mentioned_In),
    Total_Comments_In_Threads = sum(Total_Comments_In_Threads),
    Unique_Authors = total_unique_authors,
    Percent_Total_Comments_w_Keyword = (Comments_with_Keywords / Total_Comments_In_Threads) * 100,
    Average_Relevancy_Score = mean(Average_Relevancy_Score, na.rm = TRUE),
    Average_Thread_Relevancy = mean(Average_Thread_Relevancy, na.rm = TRUE)
  )

# Combine the summary with the totals row
keywords_summary_df <- bind_rows(keywords_summary_df, totals)

# Print the keyword summary data frame with totals
tab_df(keywords_summary_df, file = paste0(output_directory, "images and tables/table_descriptive_statistics_aids.html"))
```
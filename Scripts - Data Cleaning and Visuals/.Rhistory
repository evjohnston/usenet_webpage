show.rownames = FALSE,
show.ci = FALSE,
show.p = FALSE,
digits = 2,
file = output_file_path)
# Ensure necessary libraries are loaded
library(ggplot2)
library(dplyr)
# Convert Date columns to Date type
filtered_comments$Date <- as.Date(filtered_comments$Date)
usenet_comments_all$Date <- as.Date(usenet_comments_all$Date)
# Calculate monthly average sentiment scores for AIDS-related discussions
monthly_sentiment_aids <- filtered_comments %>%
group_by(Month = as.Date(format(Date, "%Y-%m-01"))) %>%
summarize(AverageSentiment = mean(SentimentScore, na.rm = TRUE))
# Calculate monthly average sentiment scores for the entire Usenet dataset
monthly_sentiment_usenet <- usenet_comments_all %>%
group_by(Month = as.Date(format(Date, "%Y-%m-01"))) %>%
summarize(AverageSentiment = mean(SentimentScore, na.rm = TRUE))
# Filter the all_comments dataset to only include posts by the top 15 influential authors
top_authors_comments <- filtered_comments %>%
filter(Author %in% top_influential_authors$Author)
# Calculate monthly average sentiment scores for the influential authors
monthly_sentiment_influential <- top_authors_comments %>%
group_by(Month = as.Date(format(Date, "%Y-%m-01"))) %>%
summarize(AverageSentiment = mean(SentimentScore, na.rm = TRUE))
# Combine the datasets for plotting
combined_monthly_sentiment <- bind_rows(
monthly_sentiment_usenet %>% mutate(Dataset = "Dataset One"),
monthly_sentiment_aids %>% mutate(Dataset = "Dataset Three"),
monthly_sentiment_influential %>% mutate(Dataset = "Influential Authors")
)
# Create the time series plot with a white background and no legend title
time_series_plot <- ggplot(combined_monthly_sentiment, aes(x = Month, y = AverageSentiment, color = Dataset)) +
geom_line(size = 1.2) +
labs(
title = "Time Series of Average Sentiment Scores Over Time",
x = "Month",
y = "Average Sentiment Score"
) +
theme_bw() +  # Use theme_bw() for a white background
theme(legend.position = "bottom") +
guides(color = guide_legend(title = NULL))  # Remove the legend title
# Add key event annotations
key_events <- data.frame(
date = as.Date(c("1982-05-11", "1983-09-30", "1984-04-23", "1984-10-01",
"1985-03-02", "1985-07-25", "1985-10-02", "1986-02-01",
"1986-08-14")),  # Add the key event dates
event = c("Term 'AIDS' Introduced", "CDC AIDS Guidelines", "HHS HIV/AIDS",
"First HIV Blood Test", "First HIV Test Approved", "Rock Hudson's Diagnosis",
"HIV Transmission Routes", "'HIV' Renamed", "AZT Approved"),  # Shorter descriptions
y = c(-25, -25, -25, -25, -25, -25, -25, -25, -25)  # Set y position for text labels
)
# Adding the annotations to the plot
time_series_plot <- time_series_plot +
geom_vline(data = key_events, aes(xintercept = date), linetype = "dashed", alpha = 0.5) +  # Vertical dashed lines
geom_text(data = key_events, aes(x = date, y = y, label = event),
size = 3, angle = 90, vjust = -0.5, hjust = 0, inherit.aes = FALSE)  # Event labels
# Print the plot to the R console
print(time_series_plot)
# Save the plot to a PNG file
output_file_path <- file.path(output_directory, "images and tables", "plot_time_series_sentiment_line_graph.png")
ggsave(output_file_path, plot = time_series_plot, width = 10, height = 6, dpi = 300)
# Prepare the corpus for influential authors
influential_corpus <- Corpus(VectorSource(top_authors_comments$Full.Text))
influential_corpus <- tm_map(influential_corpus, content_transformer(tolower))
influential_corpus <- tm_map(influential_corpus, removePunctuation)
influential_corpus <- tm_map(influential_corpus, removeNumbers)
influential_corpus <- tm_map(influential_corpus, removeWords, stopwords("english"))
influential_corpus <- tm_map(influential_corpus, stripWhitespace)
# Create DTM for influential authors
influential_dtm <- DocumentTermMatrix(influential_corpus)
influential_dtm <- removeSparseTerms(influential_dtm, 0.90)
# LDA for influential authors
set.seed(123)
influential_lda <- LDA(influential_dtm, k = 6, control = list(seed = 123))
# Load required libraries
library(tidyr)
library(dplyr)
library(sjPlot)
# Visualize the top terms for each topic (concise visualization)
influential_topics <- tidy(influential_lda, matrix = "beta") %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Round beta values to 3 decimal places
influential_topics <- influential_topics %>%
mutate(beta = round(beta, 3))
# Reshape the data frame to have each topic's terms and betas side by side
reshaped_df <- influential_topics %>%
group_by(topic) %>%
mutate(term_id = row_number()) %>%
ungroup() %>%
pivot_wider(
names_from = topic,
values_from = c(term, beta),
names_glue = "Topic_{topic}_{.value}"
)
reshaped_df <- reshaped_df %>%
select(term_id,
starts_with("Topic_1_term"), starts_with("Topic_1_beta"),
starts_with("Topic_2_term"), starts_with("Topic_2_beta"),
starts_with("Topic_3_term"), starts_with("Topic_3_beta"),
starts_with("Topic_4_term"), starts_with("Topic_4_beta"),
starts_with("Topic_5_term"), starts_with("Topic_5_beta"),
starts_with("Topic_6_term"), starts_with("Topic_6_beta"))
library(dplyr)
library(gt)
# Create the gt table
gt_table <- reshaped_df %>%
gt() %>%
tab_spanner(label = "Topic 1", columns = c("Topic_1_term", "Topic_1_beta")) %>%
tab_spanner(label = "Topic 2", columns = c("Topic_2_term", "Topic_2_beta")) %>%
tab_spanner(label = "Topic 3", columns = c("Topic_3_term", "Topic_3_beta")) %>%
tab_spanner(label = "Topic 4", columns = c("Topic_4_term", "Topic_4_beta")) %>%
tab_spanner(label = "Topic 5", columns = c("Topic_5_term", "Topic_5_beta")) %>%
tab_spanner(label = "Topic 6", columns = c("Topic_6_term", "Topic_6_beta")) %>%
cols_label(
term_id = "Term No.",
Topic_1_term = "term", Topic_1_beta = "beta",
Topic_2_term = "term", Topic_2_beta = "beta",
Topic_3_term = "term", Topic_3_beta = "beta",
Topic_4_term = "term", Topic_4_beta = "beta",
Topic_5_term = "term", Topic_5_beta = "beta",
Topic_6_term = "term", Topic_6_beta = "beta")
gt_table
# Save the gt table to an HTML file
output_file_path <- file.path(output_directory, "images and tables", "table_top_terms_and_betas_(influential_authors).html")
gtsave(gt_table, output_file_path)
# Extract top terms from the overall LDA model (assuming it's stored in 'lda_model')
overall_topics <- tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Function to calculate Jaccard similarity
jaccard_similarity <- function(set1, set2) {
intersection <- length(intersect(set1, set2))
union <- length(union(set1, set2))
return(intersection / union)
}
# Compare topics
topic_similarities <- data.frame(
Influential_Topic = numeric(),
Overall_Topic = numeric(),
Similarity = numeric()
)
for (i in 1:6) {
influential_terms <- influential_topics %>% filter(topic == i) %>% pull(term)
for (j in 1:6) {
overall_terms <- overall_topics %>% filter(topic == j) %>% pull(term)
similarity <- jaccard_similarity(influential_terms, overall_terms)
topic_similarities <- rbind(topic_similarities, data.frame(
Influential_Topic = i,
Overall_Topic = j,
Similarity = similarity
))
}
}
# Find the best matching topics
best_matches <- topic_similarities %>%
group_by(Influential_Topic) %>%
slice_max(order_by = Similarity, n = 1)
print(best_matches)
output_file_path <- file.path(output_directory, "images and tables", "table_topic_matches.html")
tab_df(best_matches, ,
file = output_file_path)
# Transpose the 'best_matches' dataframe
transposed_best_matches <- as.data.frame(t(best_matches))
# Display the transposed table
print(transposed_best_matches)
output_file_path_transposed <- file.path(output_directory, "images and tables", "table_topic_matches_transposed.html")
tab_df(transposed_best_matches,
file = output_file_path_transposed)
library(ggplot2)
# Create the bar chart and annotate with similarity values
similarity_plot <- ggplot(best_matches, aes(x = factor(Influential_Topic), y = Similarity, fill = factor(Overall_Topic))) +
geom_bar(stat = "identity") +
geom_text(aes(label = round(Similarity, 2)),
position = position_stack(vjust = 0.5), size = 3) + # Add similarity score annotations
labs(title = "Similarity Between Influential Authors' Topics and Overall Topics",
x = "Influential Authors' Topic",
y = "Jaccard Similarity",
fill = "Best Matching Overall Topic") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
similarity_plot
ggsave(file.path(output_directory, "images and tables", "plot_topic_influence_comparison_bar_graph.png"),
width = 10, height = 6, dpi = 300)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(scales)
library(topicmodels)
# Function to get topic distributions for a set of documents
get_topic_distribution <- function(lda_model, dtm) {
topic_dist <- posterior(lda_model)$topics
colnames(topic_dist) <- paste0("Topic_", 1:ncol(topic_dist))
cbind(data.frame(Document = rownames(dtm)), as.data.frame(topic_dist))
}
# Get topic distributions for all documents and influential authors' documents
all_topic_dist <- get_topic_distribution(lda_model, dtm)
influential_topic_dist <- get_topic_distribution(influential_lda, influential_dtm)
# Add date and author information
all_topic_dist$Date <- filtered_comments$Date[match(all_topic_dist$Document, rownames(dtm))]
all_topic_dist$Influential <- FALSE
influential_topic_dist$Date <- top_authors_comments$Date[match(influential_topic_dist$Document, rownames(influential_dtm))]
influential_topic_dist$Influential <- TRUE
# Combine datasets
combined_topic_dist <- rbind(all_topic_dist, influential_topic_dist)
# Calculate monthly topic prevalence
monthly_topic_prevalence <- combined_topic_dist %>%
mutate(Month = floor_date(Date, "month")) %>%
group_by(Month, Influential) %>%
summarise(across(starts_with("Topic_"), mean)) %>%
pivot_longer(cols = starts_with("Topic_"), names_to = "Topic", values_to = "Prevalence")
# Create a custom labeler for facet labels
facet_labels <- c(`FALSE` = "All Authors", `TRUE` = "Influential Authors")
# Create the line plot for topic prevalence over time with better scaling
line_plot <- ggplot(monthly_topic_prevalence, aes(x = Month, y = Prevalence, color = Topic, group = Topic)) +
geom_line(size = 1.5) +  # Increase line size for better visibility
facet_wrap(~Influential, scales = "free_y", ncol = 1, labeller = labeller(Influential = facet_labels)) +  # Use labeller to rename facets
scale_color_brewer(palette = "Dark2") +  # Use a palette with more distinct colors
labs(title = "Topic Prevalence Over Time: Influential Authors vs Overall Discussion",
x = "Date",
y = "Topic Prevalence",
color = "Topic") +
theme_bw() +
theme(legend.position = "bottom",
axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for better readability
scale_x_date(date_breaks = "6 months", date_labels = "%b %Y") +
scale_y_continuous(labels = scales::percent_format(), limits = c(0.155, 0.1825))  # Zoom in on relevant y-axis range
# Print the plot
print(line_plot)
# Save the plot with a higher DPI for better clarity
output_file_path <- file.path(output_directory, "images and tables", "plot_topic_prevalence_line_graph.png")
ggsave(output_file_path, plot = line_plot, width = 12, height = 8, dpi = 300)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Calculate the total number of comments per author across all newsgroups
total_comments_by_author <- filtered_comments %>%
group_by(Author) %>%
summarise(Total_Comments = n())
# Calculate the number of comments per author per newsgroup
top_authors_by_newsgroup <- filtered_comments %>%
group_by(Author, newsgroup) %>%
summarise(Number_of_Comments = n())
# Join the total comments data to the newsgroup data
top_authors_by_newsgroup <- top_authors_by_newsgroup %>%
left_join(total_comments_by_author, by = "Author")
# Select the top 15 most active authors by total number of comments
top_15_authors <- total_comments_by_author %>%
arrange(desc(Total_Comments)) %>%
slice(1:15)  # Ensure only the top 25 authors are selected
# Filter the main data to include only the top 25 authors
top_15_data <- top_authors_by_newsgroup %>%
filter(Author %in% top_15_authors$Author)
# Create the bar plot using ggplot2 and sort authors in descending order by total number of comments
bar_plot <- ggplot(top_15_data, aes(x = Number_of_Comments, y = reorder(Author, Total_Comments), fill = newsgroup)) +
geom_bar(stat = "identity") +
labs(title = "Top 15 Most Active Users in Dataset Three by Newsgroup",
x = "Number of Comments",
y = "Author",
fill = "Newsgroup") +
theme_bw() +
theme(legend.position = "right",
axis.text.x = element_text(angle = 45, hjust = 1))
# Print the plot
print(bar_plot)
# Save the plot to a file
output_file_path <- file.path(output_directory, "images and tables/plot_top_15_most_active_users_by_newsgroup.png")
ggsave(output_file_path, plot = bar_plot, width = 10, height = 6, dpi = 300)
library(tidytext)
library(tidyverse)
# Identify top keywords for influential authors
influential_keywords <- top_authors_comments %>%
unnest_tokens(word, Full.Text) %>%
anti_join(stop_words) %>%
count(Author, word, sort = TRUE) %>%
group_by(Author) %>%
top_n(5, n) %>%
ungroup()
# Get the top 20 keywords across all influential authors
top_keywords <- influential_keywords %>%
group_by(word) %>%
summarise(total = sum(n)) %>%
top_n(20, total) %>%
pull(word)
# Function to calculate keyword prevalence
calculate_keyword_prevalence <- function(df, keywords) {
df %>%
unnest_tokens(word, Full.Text) %>%
filter(word %in% keywords) %>%
count(Date = floor_date(Date, "month"), word) %>%
complete(Date, word, fill = list(n = 0)) %>%
group_by(Date) %>%
mutate(prevalence = n / sum(n)) %>%
ungroup()
}
# Calculate keyword prevalence for influential authors and overall discussions
influential_prevalence <- calculate_keyword_prevalence(top_authors_comments, top_keywords)
overall_prevalence <- calculate_keyword_prevalence(filtered_comments, top_keywords)
# Combine the data
combined_prevalence <- bind_rows(
influential_prevalence %>% mutate(group = "Influential Authors"),
overall_prevalence %>% mutate(group = "Overall Discussion")
)
# Create the plot
keyword_adoption_plot <- ggplot(combined_prevalence, aes(x = Date, y = prevalence, color = group)) +
geom_line() +
facet_wrap(~word, scales = "free_y", ncol = 4) +
labs(title = "Keyword Adoption Over Time: Influential Authors vs Overall Discussion",
x = "Date",
y = "Keyword Prevalence",
color = "Group") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom")
# Print the plot
print(keyword_adoption_plot)
# Save the plot
output_file_path <- file.path(output_directory, "images and tables", "plot_keyword_adoption_over_time.png")
ggsave(output_file_path, plot = keyword_adoption_plot, width = 16, height = 10, dpi = 300)
library(igraph)
library(ggraph)
library(tidygraph)
library(dplyr)
# Identify influential authors' threads
influential_threads <- filtered_comments %>%
filter(Author %in% top_influential_authors$Author) %>%
select(Thread.ID) %>%
distinct()
# Create edges based on who commented after influential authors
edges <- filtered_comments %>%
inner_join(influential_threads, by = "Thread.ID") %>%
arrange(Thread.ID, Date) %>%
group_by(Thread.ID) %>%
mutate(PreviousAuthor = lag(Author)) %>%
ungroup() %>%
filter(!is.na(PreviousAuthor),
Author != PreviousAuthor,
PreviousAuthor %in% top_influential_authors$Author) %>%
select(from = PreviousAuthor, to = Author) %>%
group_by(from, to) %>%
summarise(weight = n(), .groups = "drop") %>%
# Filter for stronger connections
filter(weight > quantile(weight, 0.25))
# Create graph
graph <- graph_from_data_frame(edges, directed = TRUE)
# Calculate node sizes based on in-degree
node_sizes <- degree(graph, mode = "in")
# Identify top 10 influential nodes
top_nodes <- names(sort(node_sizes, decreasing = TRUE)[1:10])
# Create layout
layout <- create_layout(graph, layout = "fr")
# Create plot
influence_plot <- ggraph(layout) +
geom_edge_link(aes(width = weight), alpha = 0.2, arrow = arrow(length = unit(2, 'mm')), end_cap = circle(3, 'mm')) +
geom_node_point(aes(size = node_sizes, color = name %in% top_influential_authors$Author)) +
geom_node_text(aes(label = ifelse(name %in% top_nodes, name, "")), repel = TRUE, size = 3) +
scale_edge_width(range = c(0.1, 2)) +
scale_size(range = c(1, 10)) +
scale_color_manual(values = c("TRUE" = "red", "FALSE" = "lightblue")) +
theme_minimal() +
theme(legend.position = "none") +
labs(title = "Influence Propagation Network",
subtitle = "Node size represents incoming connections, edge width represents frequency of interaction\nRed nodes are influential authors, labeled nodes are top 10 by incoming connections")
# Print the plot
print(influence_plot)
# Save the plot
output_file_path <- file.path(output_directory, "images and tables", "plot_influence_propagation_network_improved.png")
ggsave(output_file_path, plot = influence_plot, width = 20, height = 16, dpi = 300)
# Load necessary libraries
library(visNetwork)
library(igraph)
library(visNetwork)
library(igraph)
# Create an igraph object from the filtered co-occurrence matrix
graph_3d <- graph_from_adjacency_matrix(filtered_co_occurrence, weighted = TRUE, mode = "undirected", diag = FALSE)
# Simplify the graph to avoid self-loops and redundant edges
graph_3d <- simplify(graph_3d)
# Prepare colors based on topic assignment
topic_colors <- RColorBrewer::brewer.pal(6, "Set1")[term_topic_df$topic]
# Prepare data for visNetwork
nodes <- data.frame(
id = V(graph_3d)$name,
label = V(graph_3d)$name,  # Labels are always visible
group = term_topic_df$topic,  # Group nodes by topic
value = log1p(term_frequency[filtered_terms]) * 8,  # Increase node size scaling for visibility
title = paste("<p><b>Node:</b>", V(graph_3d)$name, "<br><b>Frequency:</b>", term_frequency[filtered_terms], "</p>")  # Custom hover popups
)
# Sort nodes alphabetically by label
nodes <- nodes[order(nodes$label), ]
edges <- data.frame(
from = as.character(ends(graph_3d, E(graph_3d))[, 1]),
to = as.character(ends(graph_3d, E(graph_3d))[, 2]),
value = E(graph_3d)$weight / max(E(graph_3d)$weight) * 1,  # Scale edge width for visibility
label = round(E(graph_3d)$weight, 2)  # Add edge labels
)
network_vis <- visNetwork(nodes, edges, width = "100%", height = "90vh") %>%
visNodes(
shape = "dot",
scaling = list(min = 5, max = 30),
font = list(size = 35),
color = list(
background = topic_colors,
border = "black",
highlight = "red"
),
title = nodes$title
) %>%
visEdges(
width = ~value,
color = list(color = 'gray', highlight = 'red'),
smooth = list(enabled = TRUE, type = "continuous", roundness = 1)
) %>%
visOptions(
highlightNearest = list(enabled = TRUE, hover = TRUE),
nodesIdSelection = list(enabled = TRUE, useLabels = TRUE, main = "Select by term"),  # Changed label for node selection
selectedBy = list(variable = "group", sort = TRUE, main = "Select by topic")  # Changed label for group selection
) %>%
visPhysics(
stabilization = FALSE,
solver = "barnesHut",
barnesHut = list(
gravitationalConstant = -30000,  # Increase negative value for stronger repulsion
springLength = 500,              # Increase spring length to spread nodes apart
springConstant = 0.01            # Decrease spring constant for more flexible edges
)
) %>%
visInteraction(
dragNodes = TRUE,
dragView = TRUE,
zoomView = TRUE,
navigationButtons = TRUE,  # Adds navigation buttons
keyboard = TRUE
) %>%
visLayout(
improvedLayout = TRUE
)
# Save the network graph to an HTML file with full width and height
network_graph_path <- file.path(output_directory, "images and tables/new web visualizations/interactive_plot_3d_cooccurrence_network.html")
visSave(network_vis, file = network_graph_path)
# Custom title and legend creation in HTML
title_html <- '
<div style="position: absolute; top: 40px; left: 50%; transform: translateX(-50%); font-size: 24px; font-weight: bold; font-family: Arial, sans-serif;">
Interactive Co-Occurrence Network of Top Terms
</div>
'
legend_html <-'
<div style="position: absolute; top: 40px; right: 40px; background-color: #f0f0f0; border: 1px solid #666; padding: 7px; text-padding: 7px; border-radius: 5px; font-family: Arial, sans-serif;">
<span style="color: #94df5c;">●</span> Topic 1
<span style="color: #ffff54;">●</span> Topic 2
<span style="color: #eb8584;">●</span> Topic 3<br>
<span style="color: #a787de;">●</span> Topic 4
<span style="color: #dd82ee;">●</span> Topic 5
<span style="color: #9fc0f7;">●</span> Topic 6
</div>
'
# Inject the custom title and legend into the HTML file
html_content <- readLines(network_graph_path)
html_content <- c(title_html, html_content, legend_html)
writeLines(html_content, network_graph_path)
library(igraph)
library(ggraph)
library(tidygraph)
library(dplyr)
# Identify influential authors' threads
influential_threads <- filtered_comments %>%
filter(Author %in% top_influential_authors$Author) %>%
select(Thread.ID) %>%
distinct()
# Create edges based on who commented after influential authors
edges <- filtered_comments %>%
inner_join(influential_threads, by = "Thread.ID") %>%
arrange(Thread.ID, Date) %>%
group_by(Thread.ID) %>%
mutate(PreviousAuthor = lag(Author)) %>%
ungroup() %>%
filter(!is.na(PreviousAuthor),
Author != PreviousAuthor,
PreviousAuthor %in% top_influential_authors$Author) %>%
select(from = PreviousAuthor, to = Author) %>%
group_by(from, to) %>%
summarise(weight = n(), .groups = "drop") %>%
# Filter for stronger connections
filter(weight > quantile(weight, 0.25))
# Create graph
graph <- graph_from_data_frame(edges, directed = TRUE)
# Calculate node sizes based on in-degree
node_sizes <- degree(graph, mode = "in")
# Identify top 10 influential nodes
top_nodes <- names(sort(node_sizes, decreasing = TRUE)[1:10])
# Create layout
layout <- create_layout(graph, layout = "fr")
# Create plot
influence_plot <- ggraph(layout) +
geom_edge_link(aes(width = weight), alpha = 0.2, arrow = arrow(length = unit(2, 'mm')), end_cap = circle(3, 'mm')) +
geom_node_point(aes(size = node_sizes, color = name %in% top_influential_authors$Author)) +
geom_node_text(aes(label = ifelse(name %in% top_nodes, name, "")), repel = TRUE, size = 3) +
scale_edge_width(range = c(0.1, 2)) +
scale_size(range = c(1, 10)) +
scale_color_manual(values = c("TRUE" = "red", "FALSE" = "lightblue")) +
theme_minimal() +
theme(legend.position = "none")
# Print the plot
print(influence_plot)
# Save the plot
output_file_path <- file.path(output_directory, "images and tables", "plot_influence_propagation_network_improved.png")
ggsave(output_file_path, plot = influence_plot, width = 20, height = 16, dpi = 300)
